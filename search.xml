<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>《E题1.0》</title>
    <url>/2022/10/06/HWB/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E9%97%AE%E9%A2%981">问题1</a></li>
<li><a href="#%E9%97%AE%E9%A2%982">问题2</a></li>
<li><a href="#%E9%97%AE%E9%A2%983">问题3</a></li>
<li><a href="#%E9%97%AE%E9%A2%984">问题4</a></li>
<li><a href="#%E9%97%AE%E9%A2%985">问题5</a></li>
<li><a href="#%E9%97%AE%E9%A2%986">问题6</a></li>
</ul>
<!-- tocstop -->

<span id="more"></span>

<h1><span id="问题1">问题1</span></h1><h4><span id="从机理分析的角度建立不同放牧策略放牧方式和放牧强度对锡林郭勒草原土壤物理性质主要是土壤湿度和植被生物量影响的数学模型">从机理分析的角度，建立不同放牧策略（放牧方式和放牧强度）对锡林郭勒草原土壤物理性质（主要是土壤湿度）和植被生物量影响的数学模型</span></h4><p>筛选出放牧策略（载畜量）、降水、地表蒸发量数据，套用下方公式</p>
<p>画出植被生物量和单位时间的曲线，可求出对应的单位时间的斜率，推出该时间S和w的关系；</p>
<p>土壤含水量同理，但要将α换成w的式子；</p>
<p>画出土壤含水量和单位时间的曲线，可求出对应的单位时间的斜率，推出该时间S和w的关系；</p>
<p>E(a)直接使用蒸发量代替</p>
<p><img src="/2022/10/06/HWB/image-20221006201807430.png" alt="image-20221006201807430"></p>
<h1><span id="问题2">问题2</span></h1><p>先时间序列自回归预测浅层因子，然后用这些浅层因子做自变量做多元回归；</p>
<p>其中可以使用前十年的单一月份数据，来预测后一年单一月份数据（建12个模型）</p>
<h1><span id="问题3">问题3</span></h1><h4><span id="从机理分析的角度建立不同放牧策略放牧方式和放牧强度对锡林郭勒草原土壤化学性质影响的数学模型-并请结合附件14中数据预测锡林郭勒草原监测样地12个放牧小区在不同放牧强度下2022年土壤同期有机碳-无机碳-全n-土壤cx2fn比等值并完成下表">从机理分析的角度，建立不同放牧策略（放牧方式和放牧强度）对锡林郭勒草原土壤化学性质影响的数学模型。并请结合附件14中数据预测锡林郭勒草原监测样地(12个放牧小区)在不同放牧强度下2022年土壤同期有机碳、无机碳、全N、土壤C&#x2F;N比等值,并完成下表。</span></h4><p>时间序列自回归</p>
<h1><span id="问题4">问题4</span></h1><h4><span id="利用沙漠化程度指数预测模型和附件提供数据包括自己收集的数据确定不同放牧强度下监测点的沙漠化程度指数值">利用沙漠化程度指数预测模型和附件提供数据（包括自己收集的数据）确定不同放牧强度下监测点的沙漠化程度指数值。</span></h4><p>影响因子：风速、降水、气温（气象因素附件8）；植被盖度（附件10或6或5）、地表水资源（附件9）、地下水位（附件9）；人口数量、牲畜数量、社会经济水平（附件2）等等。</p>
<p>PCA加权累加</p>
<p><img src="/2022/10/06/HWB/image-20221006191041118.png" alt="image-20221006191041118"></p>
<h4><span id="并请尝试给出定量的土壤板结化定义">并请尝试给出定量的土壤板结化定义</span></h4><p><img src="/2022/10/06/HWB/image-20221006192120031.png" alt="image-20221006192120031"></p>
<h4><span id="在建立合理的土壤板结化模型基础上结合问题3给出放牧策略模型使得沙漠化程度指数与板结化程度最小">在建立合理的土壤板结化模型基础上结合问题3，给出放牧策略模型，使得沙漠化程度指数与板结化程度最小。</span></h4><p>带入第一题公式和板结化公式，求解S</p>
<h1><span id="问题5">问题5</span></h1><h4><span id="锡林郭勒草原近10的年降水量包含降雪通常在300-mm-~1200-mm之间请在给定的降水量300mm600mm-900-mm-和1200mm情形下在保持草原可持续发展情况下对实验草场内附件14-15放牧羊的数量进行求解找到最大阈值-注这里计算结果可以不是正整数">锡林郭勒草原近10的年降水量（包含降雪）通常在300 mm ~1200 mm之间，请在给定的降水量（300mm，600mm、900 mm 和1200mm）情形下，在保持草原可持续发展情况下对实验草场内（附件14、15）放牧羊的数量进行求解，找到最大阈值。（注：这里计算结果可以不是正整数）</span></h4><p>筛选出满足降水条件的年份对应的数据。</p>
<p><img src="/2022/10/06/HWB/image-20221006193918802.png" alt="image-20221006193918802"></p>
<h1><span id="问题6">问题6</span></h1><h4><span id="在保持附件13的示范牧户放牧策略不变和问题4中得到的放牧方案两种情况下用图示或者动态演示方式分别预测示范区2023年9月土地状态比如土壤肥力变化-土壤湿度-植被覆盖等">在保持附件13的示范牧户放牧策略不变和问题4中得到的放牧方案两种情况下，用图示或者动态演示方式分别预测示范区2023年9月土地状态（比如土壤肥力变化、土壤湿度、植被覆盖等）</span></h4><p>善男先做</p>
]]></content>
      <categories>
        <category>建模</category>
      </categories>
      <tags>
        <tag>建模</tag>
      </tags>
  </entry>
  <entry>
    <title>《High-Fidelity 3D Digital Human Head Creation from RGB-D Selfies》</title>
    <url>/2022/10/17/review_11_Hifi3dFace/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#1-%E5%AE%9E%E7%8E%B0%E6%95%88%E6%9E%9C">1. 实现效果</a></li>
<li><a href="#2-%E5%85%A8%E9%83%A8%E7%AE%97%E6%B3%95%E6%80%BB%E8%A7%88">2. 全部算法总览</a><ul>
<li><a href="#21-%E6%95%B0%E6%8D%AE%E9%9B%86">2.1 数据集</a></li>
<li><a href="#22-%E7%9B%AE%E6%A0%87">2.2 目标</a></li>
<li><a href="#23-%E7%94%A8%E6%88%B7%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE">2.3 用户输入数据</a></li>
<li><a href="#24-%E5%A4%84%E7%90%86%E7%AE%A1%E7%BA%BF">2.4 处理管线</a></li>
</ul>
</li>
<li><a href="#3-%E5%85%B7%E4%BD%93%E7%AE%97%E6%B3%95">3. 具体算法</a><ul>
<li><a href="#31-%E5%B8%A7%E9%80%89%E6%8B%A9">3.1 帧选择</a><ul>
<li><a href="#311-%E7%B2%97%E7%AD%9B%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86">3.1.1 粗筛和预处理</a></li>
<li><a href="#312-%E5%B8%A7%E9%80%89%E6%8B%A9">3.1.2 帧选择</a></li>
</ul>
</li>
<li><a href="#32-%E5%87%A0%E4%BD%95%E7%94%9F%E6%88%90">3.2 几何生成</a><ul>
<li><a href="#321-%E5%88%9D%E5%A7%8B%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89">3.2.1 初始模型定义</a></li>
<li><a href="#322-%E4%BC%98%E5%8C%96%E6%A1%86%E6%9E%B6">3.2.2 优化框架</a></li>
<li><a href="#323-%E5%8F%AF%E5%BD%A2%E5%8F%98%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%A2%9E%E5%BC%BA%E6%89%A9%E5%85%853d%E6%95%B0%E6%8D%AE%E7%9A%84%E6%96%B9%E6%B3%95">3.2.3 可形变模型的增强（扩充3D数据的方法）</a></li>
</ul>
</li>
<li><a href="#33-%E8%B4%B4%E5%9B%BE%E7%94%9F%E6%88%90">3.3 贴图生成</a><ul>
<li><a href="#331-%E5%9F%BA%E4%BA%8E%E5%8C%BA%E5%9F%9F%E9%87%91%E5%AD%97%E5%A1%94%E7%9A%84%E5%8F%82%E6%95%B0%E8%A1%A8%E7%A4%BA%E6%9E%84%E9%80%A0%E8%AF%A6%E8%A7%A3regional-pyramid-bases">3.3.1 基于区域金字塔的参数表示构造详解（Regional Pyramid bases）</a></li>
<li><a href="#332-%E5%8C%BA%E5%9F%9F%E6%8B%9F%E5%90%88regional-fitting">3.3.2 区域拟合（Regional Fitting）</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Shi_Face-to-Parameter_Translation_for_Game_Character_Auto-Creation_ICCV_2019_paper.pdf">https://arxiv.org/abs/2010.05562</a></p>
<p>code: <a href="https://github.com/tencent-ailab/hifi3dface">tencent-ailab&#x2F;hifi3dface: Code and data for our paper “High-Fidelity 3D Digital Human Creation from RGB-D Selfies”. (github.com)</a></p>
<p>ref link:<a href="https://blog.csdn.net/jiafeier_555/article/details/125428388">https://blog.csdn.net/jiafeier_555/article/details/125428388</a></p>
<span id="more"></span>

<h1><span id="1-实现效果">1. 实现效果</span></h1><p>将用户的RGB-D自拍作为输入，自动生成高保真、可装配的头部模型，以及高分辨率的纹理图和法线图。</p>
<h1><span id="2-全部算法总览">2. 全部算法总览</span></h1><h2><span id="21-数据集">2.1 数据集</span></h2><p>包含各100名男女性的3D人脸，20481个顶点和40832个面片，每个模型对应2K分辨率的纹理图和法线图。</p>
<h2><span id="22-目标">2.2 目标</span></h2><p>用RGB-D自拍数据捕捉高保真用户的面部几何形状和反射率，该数据进一步用于创建和渲染全头、逼真的数字人类。<strong>对于几何建模，我们使用3DMM参数来表示一个面，因为它对低级别的输入数据更鲁棒，并且与基于变形的表示形式相比，网格质量更可控</strong>。对于纹理建模，我们合成了2K分辨率的反照率图和法线图，而不考虑输入的RGB-D分辨率。 </p>
<h2><span id="23-用户输入数据">2.3 用户输入数据</span></h2><p>需要通过带深度相机的手机设备采集RGB-D数据，腾讯其有个采集软件，拍摄界面将引导用户连续向左、向右、向上和向后中间旋转头部。整个采集过程不到10秒，总共采集了200-300帧RGB-D图像，分辨率为640×480。用于计算的面部区域被裁剪（并调整大小）为300×300。相机的固有参数直接从设备读取。 </p>
<h2><span id="24-处理管线">2.4 处理管线</span></h2><p>我们首先使用自动帧选择算法来选择覆盖用户所有侧面的几个高质量帧（第4节）。然后，使用所选帧中检测到的面部标志计算初始3DMM模型拟合（第5.1节）。从初始拟合开始，应用基于可差分渲染器的多视图RGB-D约束优化（第5.2节）来求解3DMM参数以及照明参数和姿势。根据估计的参数，合成高分辨率反照率法向图（第6节）。最后，可以创建和渲染高质量、逼真的全头像（第7节）。</p>
<h1><span id="3-具体算法">3. 具体算法</span></h1><h2><span id="31-帧选择">3.1 帧选择</span></h2><p>通常从用户获取200-300帧。为了提高效率和健壮性，我们开发了一个健壮的帧选择程序，以选择几个高质量的帧进行进一步处理，该程序同时考虑了视图覆盖和数据质量。如图2所示，该过程包括如下所述的两个阶段。 </p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221023165135058.png" alt="image-20221023165135058"></p>
<h3><span id="311-粗筛和预处理">3.1.1 粗筛和预处理</span></h3><p>首先用了个face landmark的检测（在300W-LP数据集上训练的MobileNet）来检测2D landmark；</p>
<p>然后，可以使用PnP算法利用模板3D面部模型上的2D地标和3D关键点之间的对应关系有效地计算每个帧的粗略头部姿势；</p>
<p>使用2D地标和粗略的头部姿势可以很容易地识别和筛选出具有极端无效姿势或闭眼张开嘴表情的帧。我们根据姿势将其余帧分类为组：前、左、右和上；</p>
<h3><span id="312-帧选择">3.1.2 帧选择</span></h3><p>对于每组，我们进一步根据两个标准选择一个帧：图像质量和刚性。为了测量帧的图像质量，我们计算拉普拉斯高斯（LoG）滤波器响应，并将方差用作运动模糊分数（分数越大的图像越清晰）。首先基于前面组中的运动模糊分数来选择前面帧。然后，我们利用深度数据计算其他组中的每个帧与正面之间的刚度。具体而言，使用深度数据将每个帧的检测到的2D地标从2D提升到3D。请注意，遮挡的界标会根据帧所属的组自动移除，例如，对于左侧组中的帧，面部右侧的界标将被移除。 </p>
<h2><span id="32-几何生成">3.2 几何生成</span></h2><h3><span id="321-初始模型定义">3.2.1 初始模型定义</span></h3><p>使用基于 PCA 的线性 3DMM 进行参数建模。 人脸模型的形状和反照率纹理表示为：</p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221025151737274.png" alt="image-20221025151737274"></p>
<p>其中 s¯ 是平均 3D 人脸形状模型的向量格式，𝑆 是形状身份基础，x𝑠ℎ𝑝 是要估计的相应身份参数向量，a¯ 是平均反照率图的向量格式，𝐴 是反照率图基础颜色，x𝑎𝑙𝑏是要估计的相关反照率参数向量。</p>
<p>使用岭回归 ，可以通过将形状模型投影到每个输入图像上来提取部分纹理图。 使用从每个视图的地标派生的预定义蒙版（见图 3），然后使用拉普拉斯金字塔混合将部分纹理贴图混合成完整的纹理贴图。（<strong>unwarp操作</strong>） 初始反照率参数可以通过另一个岭回归来获得，以拟合混合纹理图。</p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221025153015905.png" alt="image-20221025153015905"></p>
<h3><span id="322-优化框架">3.2.2 优化框架</span></h3><p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221025153218398.png" alt="image-20221025153218398"></p>
<p>优化框架： 要解决的参数包括：用户的 3DMM 参数 x𝑠ℎ𝑝 和 x𝑎𝑙𝑏，每个视图的照明参数 x𝑙𝑖𝑔ℎ𝑡 和姿势 x𝑝𝑜𝑠𝑒。 约束包括：关键点损失𝐿𝑙𝑎𝑛、RGB 照片损失𝐿𝑟𝑔𝑏、深度损失𝐿𝑑𝑒𝑝和身份感知损失𝐿𝑖𝑑。</p>
<p>优化目标：<img src="/2022/10/17/review_11_Hifi3dFace/image-20221025155915049.png" alt="image-20221025155915049"></p>
<p>对于每个用户的两个3DMM系数是不同的，而光照系数和姿态系数对每个用户都通用。</p>
<p>目标Loss：</p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221025160858227.png" alt="image-20221025160858227"></p>
<p>具体Loss：</p>
<p><strong>RGB Photo Loss</strong>：L2，输入的RGB图像和渲染的RGB图像做L2</p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221025160959267.png" alt="image-20221025160959267"></p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221025161500643.png" alt="image-20221025161500643"></p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221025161600812.png" alt="image-20221025161600812"></p>
<p>（下图是真实RGB，上图是mesh和贴图在原图上做渲染后的图像）</p>
<p><strong>Depth Loss</strong>：L2，ρ是个截断函数，用来限定L2的范围；</p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221025162833565.png" alt="image-20221025162833565"></p>
<p><strong>Identity Perceptual Loss</strong>：L2，过VGGFace的fc7后的特征做感知loss；</p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221025163251941.png" alt="image-20221025163251941"></p>
<p><strong>Landmark Loss</strong>：每个lankmark做L2（不同位置权重不一样）</p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221025163440791.png" alt="image-20221025163440791"></p>
<p>Regularization：为了确保重建的合理性，对形状和纹理参数应用正则化</p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221026111116994.png" alt="image-20221026111116994"></p>
<h3><span id="323-可形变模型的增强扩充3d数据的方法">3.2.3 可形变模型的增强（扩充3D数据的方法）</span></h3><p>优化中的约束十分丰富（loss很多），如果使用传统的线性3DMM的表达能力十分有限，为此提出了一种增强方法。</p>
<p>motivation：人脸大多不对称，会在对齐人脸模型时造成歧义， 原因是在两个模型的对齐过程中，它们之间的相对旋转和平移是通过最小化模型上某些参考点的误差来确定的。 不同的参考点可能导致不同的对齐结果。 由于人脸的不对称结构，没有完美的参考点。</p>
<p>这提醒我们可以扰乱两个对齐模型之间的相对姿势以获得“代替”的对齐。通过这种方式既可以获得额外的PCA样本，因为新的对齐方式引入了新的变形目标。此外，我们可以使用一组扰动操作，包括姿态扰动、镜像、区域替换等，来增强对齐的模型。 基于生成的大量数据，我们提出了一种随机迭代算法来构建一个 3DMM，将更多的容量压缩到基的更低维度。</p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221026151445577.png" alt="image-20221026151445577"></p>
<p><strong>数据生成和扰动具体增强步骤</strong>（从200个对齐后的面部模型开始）</p>
<p>从 200 个对齐的面部形状模型开始，我们的数据生成和扰动过程包括以下步骤： </p>
<p>1.使用扰动进行区域替换</p>
<p>我们首先用其他模型替换每个模型的鼻子区域，沿俯仰角旋转扰动（在±1度内均匀采样）。 嘴部区域也以相同的方式处理。对于眼睛区域，我们在没有扰动的情况下应用替换。通过在处理过程中最小化引入的视觉缺陷来根据经验设计不同的扰动。 <strong>此步骤中使用的面部区域如图 5 所示</strong></p>
<p>2.刚性变换扰动。 </p>
<p>然后，我们对每个人脸模型应用刚性变换扰动，其中统一采样范围设置为：沿yam&#x2F;pitch&#x2F;roll的 ±1 度用于旋转，沿三个轴中的每一个轴的 ±1% 用于平移 , ±1% 刻度。</p>
<p>3.镜像。 </p>
<p>最后，我们沿模型局部坐标系对所有生成的人脸模型应用镜像。 通过这种方式，我们总共得到了超过 100,000 个人脸模型。</p>
<p>最后效果表示，拥有大量数据生成和增强所训练出的模型不出意外有更强大的表达能力，拟合得更好。</p>
<p>迭代增强3DMM构造算法流程：</p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221026155015588.png" alt="image-20221026155015588"></p>
<h2><span id="33-贴图生成">3.3 贴图生成</span></h2><p><strong>这一部分参考博客里讲得很好，建议先看参考博客</strong></p>
<p>提出了一种合成高分辨率反照率和法线贴图的混合方法。</p>
<p>我们注意到基于超分辨率的方法无法产生高质量的眉毛细节。 另一方面，直接合成高分辨率纹理图可能会导致压倒性的细节，这也使得渲染不真实。 我们的方法在基于金字塔的参数表示的帮助下解决了这些问题。 图 9 显示了我们方法的流程。</p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221026163814008.png" alt="image-20221026163814008"></p>
<p>第一步提取纹理（extract）</p>
<p>第二步通过光照参数调整纹理亮度（delight）</p>
<p>第三步将3D人脸展开到2D贴图（unwrap），512*512</p>
<p>unwrap详情见代码，最终效果就是blender里面的展UV</p>
<p>第四步使用<strong>区域拟合方法</strong>获得2048*2048的反射率贴图和法线贴图（regional fitting）</p>
<p>第五步通过CNN（具体模型为<strong>pix2pix</strong>）对两个贴图进行细化（refinement）</p>
<h3><span id="331-基于区域金字塔的参数表示构造详解regional-pyramid-bases">3.3.1 基于区域金字塔的参数表示构造详解（Regional Pyramid bases）</span></h3><p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221026214551913.png" alt="image-20221026214551913"></p>
<p>1.先将unwrap后的贴图resize成512和2048大小的图像，最终要得到上图左边的素材；</p>
<p>2.使用Mask将脸部分为8个区域，具体规则如下表示</p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221027111816307.png" alt="image-20221027111816307"></p>
<p>3.每个样本有三种贴图（512-a，2048-a，2048-g），构成如下三元组</p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221027150644329.png" alt="image-20221027150644329"></p>
<p>4.concat到一维，计算PCA，得到主成分</p>
<p>5.将得到的主成分根据索引，scatter back回图像</p>
<p>scatter back的操作是tensorflow的scatter_nd()函数实现的。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scatter_nd(indices,updates,shape,name=None)</span><br></pre></td></tr></table></figure>

<p>根据indices将updates散布到新的（初始为零）张量。</p>
<h3><span id="332-区域拟合regional-fitting">3.3.2 区域拟合（Regional Fitting）</span></h3><p>由于构造了regional pyramid base，不同区域的不同类型皮肤&#x2F;头发细节可以通过高分辨率基分别保存，而低分辨率的拟合过程使算法专注于主要的面部结构，例如，形状眉毛和嘴唇。（<strong>多尺度</strong>）</p>
<p>区域拟合分为两部分：<strong>参数拟合和高分辨率贴图合成</strong></p>
<p><strong>参数拟合</strong></p>
<p><strong>参数拟合部分，就是将生成头模的时候生成的粗略化的Xalb，通过最小化loss，迭代训练得到较优的Xalb。</strong></p>
<p>参数拟合部分只使用了512的贴图</p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221027153638981.png" alt="image-20221027153638981"></p>
<p>变量解释：</p>
<p><strong>Loss 1</strong></p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221027155829303.png" alt="image-20221027155829303">为unwrap后的图像；</p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221027155908838.png" alt="image-20221027155908838">表示将生成头模过程中生成的粗略的颜色贴图参数，与region pyramid bases逐区域相乘拟合再叠加成一张总的颜色贴图；</p>
<p>然后计算这两张图像之间的L2，记作loss1；</p>
<p><strong>Loss 2</strong></p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221027160544519.png" alt="image-20221027160544519"></p>
<p>主要消除8个区域边界中的伪影。做法是：通过uv_mask将整个颜色贴图分割成边界mask和非边界mask，然后分别计算颜色贴图中的边界距离和非边界距离,将这两种距离相加。记做loss2；</p>
<p><strong>Loss 3</strong></p>
<p>对于<img src="/2022/10/17/review_11_Hifi3dFace/image-20221027160823904.png" alt="image-20221027160823904">这个颜色贴图参数做L2正则，记作loss3；</p>
<p>三个Loss相加 使其最小化，迭代更新颜色贴图参数Xalb，直到得到训练好的Xalb。</p>
<p><strong>高分辨率贴图生成</strong></p>
<p><img src="/2022/10/17/review_11_Hifi3dFace/image-20221027161429672.png" alt="image-20221027161429672"></p>
<p>将上部分得到的较优的Xalb，分别与区域金字塔生成的A2048（2048* 2048分辨率的颜色贴图基）逐区域相乘拟合再叠加成一张总的颜色贴图，这便是区域拟合部分得到的最终的颜色贴图。<br>将上部分得到的较优的Xalb，分别与区域金字塔生成的G2048（2048* 2048分辨率的法线贴图基）逐区域相乘拟合再叠加成一张总的法线贴图，这便是区域拟合部分得到的最终的法线贴图。</p>
<p><strong>细节生成</strong></p>
<p>基于pix2pix模型，进出模型的分辨率是相同的，但细节上会细化；</p>
<p>在训练过程中，首先使用面部区域替换和肤色转移将200幅高质量反照率&#x2F;法线图（来自构建3DMM的数据集）放大为4000幅图，作为训练两个网络的GT。</p>
<p>然后，对4000幅图执行区域拟合（Regional Fitting），以获得拟合的反照率&#x2F;法线地图，这些图在训练期间用作网络的输入。只使用整个UV贴图中的面部区域来计算训练损失。</p>
<p>类似于pix2pix，保持𝐿1损耗和GAN损耗。对于反照率细化，我们还应用总变化损失（total variation loss）来减少伪影并改善皮肤平滑度。对于法线图细化，还使用预测和GT图之间的逐像素余弦距离，以提高正常方向的精度。使用Adam优化器对网络进行75000次迭代。</p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>3DMM</tag>
      </tags>
  </entry>
  <entry>
    <title>《Face-to-Parameter Translation for Game Character Auto-Creation》</title>
    <url>/2022/07/24/review_10_F2P/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E6%96%B9%E6%B3%95%E7%AE%80%E8%BF%B0">方法简述</a><ul>
<li><a href="#%E8%B4%A1%E7%8C%AE">贡献</a></li>
</ul>
</li>
<li><a href="#%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3">方法详解</a><ul>
<li><a href="#1imitator">1.Imitator</a></li>
<li><a href="#2facial-similarity-measurement">2.Facial Similarity Measurement</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Shi_Face-to-Parameter_Translation_for_Game_Character_Auto-Creation_ICCV_2019_paper.pdf">Face-to-Parameter Translation for Game Character Auto-Creation (thecvf.com)</a></p>
<p>code: no code</p>
<p>reference link: <a href="https://zhuanlan.zhihu.com/p/105037127">【1】伏羲AI lab：脸部照片到脸部参数的游戏角色自动生成 论文笔记 - 知乎 (zhihu.com)</a></p>
<span id="more"></span>

<h1><span id="出发点">出发点</span></h1><p>RPG游戏中捏脸的标准工作流是从配置大量<strong>面部参数</strong>开始，然后，游戏引擎将这些用户指定的参数作为输入，并生成3D人脸。也可以说游戏人脸定制是一种3DMM的特例或是一种风格迁移问题。</p>
<p>但遗憾的是，上述方法不能直接应用于游戏环境中。原因有三。首先，这些方法并不是用于生成参数化的角色，而这对于大多数游戏引擎来说是必不可少的，因为它们通常会<strong>接受游戏角色的定制参数</strong>，而不是图像或3D网格。其次，这些方法对用户交互不友好，因为大多数用户很难直接编辑Mesh。最后，给定一组用户指定参数的游戏引擎的渲染过程是不可微分的，这进一步限制了深度学习方法在游戏环境中的适用性。</p>
<h1><span id="方法简述">方法简述</span></h1><p>针对上述问题，本文提出了一种根据玩家输入的人脸照片自动生成游戏角色的方法，如图所示。</p>
<p><img src="/2022/07/24/review_10_F2P/image-20220724165007254.png" alt="image-20220724165007254"></p>
<p>与以往3DMM方法不同，我们的方法通过<strong>预测一组具有明确物理意义的面部参数，为骨骼驱动的模型创建三维轮廓</strong>。在我们的方法中，每个参数控制每个面部组件的一个单独属性，包括位置、方向和比例。更重要的是，我们的方法在创建结果的基础上支持额外的用户交互，玩家可以根据自己的需求进一步改进他们的形象。由于游戏引擎的渲染过程是不可微分的，我们设计了一个生成网络作为“模仿者”来模仿游戏引擎的物理行为，从而可以在神经风格传递框架下实现本文提出的方法，并使用梯度下降法优化面部参数。</p>
<h2><span id="贡献">贡献</span></h2><p>1)我们提出了一种端到端的人脸参数转换和游戏角色自动生成方法。据我们所知，关于这一主题的研究工作很少。</p>
<p>2)由于游戏引擎的渲染过程是不可区分的，我们引入了一个模仿者，通过构建一个深度生成网络来模仿游戏引擎的行为。这样，梯度可以平滑地反向传播到输入，从而利用梯度下降法更新人脸参数。</p>
<p>3)为跨域人脸相似度测量设计了两个损失函数。提出的目标可以在多任务学习框架中联合优化。</p>
<h1><span id="方法详解">方法详解</span></h1><p><img src="/2022/07/24/review_10_F2P/image-20220724165938954.png" alt="image-20220724165938954"></p>
<h2><span id="1imitator">1.Imitator</span></h2><p><img src="/2022/07/24/review_10_F2P/image-20220724194516986.png" alt="image-20220724194516986"></p>
<p>采用类似DCGAN的结构，包含8个反卷积层，用来模拟游戏引擎从脸部参数到脸部图像的映射。然后将真实游戏引擎的渲染结果与模拟器生成的图像算L1 loss。训练用了20000对人脸图像，游戏引擎用的是逆水寒的游戏引擎。</p>
<p><img src="/2022/07/24/review_10_F2P/image-20220724195638704.png" alt="image-20220724195638704"></p>
<p>图4显示了我们的模仿者的“渲染”结果的三个示例。这些图像的面部参数是手动创建的。由于训练样本是根据面部参数的统一分布随机生成的，所以对于大多数人物可能看起来很奇怪(请参阅我们的补充资料)。尽管如此，我们仍然可以从图4中看到，生成的人脸图像和渲染的地面真实图像有很高的相似性，即使在一些纹理复杂的区域，如头发。这表明我们的模仿者不仅将训练数据拟合在一个低维的人脸流形中，而且还学会了解耦不同人脸参数之间的相关性。</p>
<h2><span id="2facial-similarity-measurement">2.Facial Similarity Measurement</span></h2><p>alignment后的真实人脸与Imitator生成的人脸通过预训练好的识别网络算embedding间的余弦相似度。</p>
<p><img src="/2022/07/24/review_10_F2P/image-20220725143108613.png" alt="image-20220725143108613"></p>
<p>又采用了一个预训练好的分割网络对两张图像进行分割，在著名的Helen face语义分割数据集上训练该模型。为了提高人脸语义特征的位置敏感性，进一步使用分割结果(分类概率图)作为特征图的像素权值，构建位置敏感的内容丢失函数。通俗点应该就是针对每个部分对feature map做掩码算L1。</p>
<p><img src="/2022/07/24/review_10_F2P/image-20220725144042722.png" alt="image-20220725144042722"></p>
<p><img src="/2022/07/24/review_10_F2P/image-20220725143221347.png" alt="image-20220725143221347"></p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>Game Character Create</tag>
      </tags>
  </entry>
  <entry>
    <title>《关于HIFI3DFACE的源码数据说明》</title>
    <url>/2022/11/02/review_11_add/</url>
    <content><![CDATA[<!-- toc -->



<!-- tocstop -->

<span id="more"></span>



]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>3DMM</tag>
      </tags>
  </entry>
  <entry>
    <title>《Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning》</title>
    <url>/2022/03/25/review_1_Sparse%20Local%20Patch%20Transformer%20for%20Robust%20Face%20Alignment%20and%20Landmarks/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a></li>
</ul>
<!-- tocstop -->

<p>paper: [<a href="https://arxiv.org/abs/2203.06541">2203.06541] Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning (arxiv.org)</a></p>
<p>code: <a href="https://github.com/Jiahao-UTS/SLPT-master">Jiahao-UTS&#x2F;SLPT-master (github.com)</a></p>
<span id="more"></span>

<h1><span id="出发点">出发点</span></h1><p>landmark之间的内在联系对于人脸对齐的性能有很大影响，本文重点考虑其内在联系。</p>
<p>之前的方法有heatmap regression，Coordinate regression，有着不同方面的劣势。</p>
<h1><span id="创新点">创新点</span></h1><p>提出了SLPT（<em>sparse local patch transformer</em>）来学习<em>query-query</em>和<em>representation-query</em>关系（自适应内在关系）；为了进一步提高SLPT的性能，提出了一种从粗到精的框架，使局部补丁进化为<strong>金字塔形补丁</strong>。</p>
<h1><span id="做法">做法</span></h1><p>本文的SLPT并非同DETR从完整的feature map中预测坐标，而是首先从局部patch中生成每个landmark的表示特征。</p>
<p>然后，使用一系列可学习的queries（称为<em>landmark queries</em>）来聚合表示。</p>
<p>基于Transformer的交叉注意机制，SPLT在每一层学习一个<strong>自适应邻接矩阵</strong>。最后，通过MLP独立预测每个landmark在其对应patch中的subpixel坐标。由于使用了稀疏的局部补丁，与其他ViT相比，输入token的数量显著减少。 </p>
<p>为了进一步提高性能，引入了从粗到精的框架，以与SLPT结合。下图为所提出的从粗到精的框架利用稀疏的局部面片实现鲁棒的人脸对齐。根据前一阶段的landmarks裁剪稀疏的局部补丁，并将其输入到同一SLPT中以预测面部landmarks。此外，patch大小随着阶段的增加而缩小，以使局部特征演变成金字塔形式。</p>
<p><img src="/2022/03/25/review_1_Sparse%20Local%20Patch%20Transformer%20for%20Robust%20Face%20Alignment%20and%20Landmarks/1.png" alt="1"></p>
<p>整体框架图：</p>
<p><img src="/2022/03/25/review_1_Sparse%20Local%20Patch%20Transformer%20for%20Robust%20Face%20Alignment%20and%20Landmarks/2.png" alt="2"></p>
<p>分为三部分：</p>
<p><strong>the patch embedding &amp; structure encoding</strong></p>
<p>不同于ViT，SLPT先根据landmark裁剪patch，再通过线性插值将patch大小调整为K*K，又使用了结构编码（可学习的参数）来补充表示。每种编码都与相邻地标（如左眼和右眼）的编码有很高的相似性。</p>
<p>Muti-head Cross-attention（在Vision Transformer基础上的改进）：通过landmark在CNN提取出的feature map上划取局部patch，将这些feature map上的patch排成一个patch embedding，将其视为landmark的表示；紧接着对其进行结构编码（Structure Encodeing）,以获取人脸中的相对位置和patch embedding做concat。输入 landmarks queries ，通过这些MLP，独立预测每个landmark的位置。</p>
<p><strong>inherent relation layers</strong></p>
<p>受Transformer启发，每一层由三个块组成，即多头自注意（MSA）块、多头交叉注意（MCA）块和多层感知器（MLP）块，并且在每个块之前应用一个layer norm（LN）。 </p>
<p><strong>prediction heads</strong></p>
<p>预测头由一个用于规范化输入的分层模板和一个用于预测结果的MLP层组成。</p>
<p>最右边的图像显示了不同样本的自适应固有关系。其将每个点连接到第一个内在关系层中交叉注意权重最高的点显示。</p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>Patch-based Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>《Pose-guided Feature Disentangling for Occluded Person Re-identification Based on Transformer》</title>
    <url>/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a><ul>
<li><a href="#visual-context-transformer-encoder">Visual Context Transformer Encoder</a></li>
<li><a href="#pose-guided-feature-aggregation">Pose-guided Feature Aggregation</a></li>
<li><a href="#part-view-based-transformer-decoder">Part View Based Transformer Decoder</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.pdf">https://arxiv.org/pdf/2112.02466v2.pdf</a></p>
<p>code: <a href="https://github.com/WangTaoAs/PFD_Net">WangTaoAs&#x2F;PFD_Net: This is Official implementation for “Pose-guided Feature Disentangling for Occluded Person Re-Identification Based on Transformer” in AAAI2022 (github.com)</a></p>
<span id="more"></span>

<h1><span id="出发点">出发点</span></h1><p>存在遮挡的行人重识别（<em>Occluded Person Re-identification</em>），由于遮挡的存在，各种噪声被引入，导致特征不匹配；遮挡可能具有与人体部位相似的特征，导致特征学习失败。</p>
<p>前人的方法有：使用姿势信息指导特征空间将全局特征划分为局部特征（缺点是需要严格的特征空间对齐）；使用基于图的方法建模拓扑信息（缺点是容易陷入上述的第二种问题）。</p>
<h1><span id="创新点">创新点</span></h1><p>本文探索了在没有空间对齐的情况下，将附加姿势信息与Transformer相结合的可能性。其使用姿势信息对语义成分（如人体的关节部位）进行分解，并对非遮挡的部位进行选择性匹配；设计了一种<em>Pose-guided Push Loss</em>。</p>
<h1><span id="做法">做法</span></h1><p>整体框架图：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329103047158.png" alt="image-20220329103047158"></p>
<h2><span id="visual-context-transformer-encoder">Visual Context Transformer Encoder</span></h2><p>首先需要对输入图像划分为固定大小的N块patch，步距大小定义为S，每块patch的尺度定义为P，patch个数N为：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329103958451.png" alt="image-20220329103958451"></p>
<p>当S等于P时，划分出来的patch就不重叠；当S&lt;P时，patch重叠，可以减少空间领域信息的丢失。</p>
<p>将这些patch通过线形层生成一个序列输入transformer encoder，concat一组可训练的<em>Position Encoding</em>，以及<em>Camera Information Embedding</em>（表示该图像所属的摄像头视角信息，标签给定的，相同视角图像有一样的值），最终的输入序列定义为：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329104849953.png" alt="image-20220329104849953"></p>
<p>最后通过Transformer Encoder输出分为两部分的特征，一部分为global feature，一部分为part feature。为进一步区分人体各个部位的特征，part feature又分为K组，每一组都与global feature做cancat送入shared transformer layer学习这些K组融合特征。</p>
<p><strong>Encoder Supervision Loss</strong></p>
<p>选用交叉熵损失作为identity loss以及triplet loss来作为这部分的loss：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329110809342.png" alt="image-20220329110809342"></p>
<h2><span id="pose-guided-feature-aggregation">Pose-guided Feature Aggregation</span></h2><p>被遮挡的人体图像的身体信息较少，而非身体部位的信息可能不明确。本文使用<em>pose estimator</em> 从图像中提取landmark信息。</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329112105052.png" alt="image-20220329112105052"></p>
<p><strong>Pose Estimation</strong></p>
<p>给定一张图像，估计器从中提取M个landmark，然后利用这些landmark生成一组heatmap <strong>H</strong>，每张heatmap都被下采样到（H&#x2F;4）*（W&#x2F;4），其中最大的response point对应一个joint point，设置了一个阈值γ来滤除高置信度和低置信度的landmark。滤除出的剩余landmark的heatmap并不是将其设为0，而是赋值0&#x2F;1，热图标签可以形式化为：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329112535277.png" alt="image-20220329112535277"></p>
<p>ci定义为第i个landmark的置信度分数。</p>
<p><strong>Pose-guided Feature Aggregation</strong></p>
<p>将之前的分组数设为K&#x3D;M，使其等于landmark的数量。将生成的一组heatmap <strong>H</strong>后接一层FC，使其尺寸与group part local feature（fgp）相同，得到<strong>H‘</strong>。将<strong>H‘</strong>与<strong>fgp</strong> mutiply element-wisely（向量对应元素相乘，将heatmap的注意力附加在fgp上）获得<strong>P</strong>，其目的是为了从fgp中找到对身体某个部位贡献最大的信息部分。</p>
<p>为此，本文开发了一种匹配和分布机制，将part local feature和pose-guided feature视为一组相似性度量问题，最终获取一个pose-guided feature集合<strong>S</strong>。</p>
<p>对于每个<strong>Pi</strong>,在fgp中找到最相似的特征，即<strong>找寻融合了heatmap注意力的序列和原始局部特征的最近距离的局部特征</strong>，以选出优质的局部特征，用余弦距离，形式化定义为：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329115119956.png" alt="image-20220329115119956"></p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329115132888.png" alt="image-20220329115132888"></p>
<h2><span id="part-view-based-transformer-decoder">Part View Based Transformer Decoder</span></h2><p>将heatmap和fen做点乘送入Decoder学习一系列learnable semantic views以学习有区别的身体部分。其实整个框架的大体思路为一张图片走两路，一路分patch进transformer encoder，一路特征点检测生成heatmap走transformer decoder，再将这两部分的输出进行match，可以得到view feature，取高置信度的view feature采样成与fgb，fgp相同尺寸算triplet loss，再将所有的view feature采样做<em>Pose Guided Push Loss</em>。</p>
<p><strong>Pose-View Matching Module</strong></p>
<p>此部分计算patch view和通过Pose-guided Feature Aggregation得到的Set之间的相似度，来获得最终的view feature，用余弦距离，形式化定义为：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329151559098.png" alt="image-20220329151559098"></p>
<p>之前的heatmap通过阈值打好了0&#x2F;1标签，最终的view feature即可通过heatmap标签分为两类。在上述距离置信度较高的view feature中取heatmap label为1的；在置信度较低的view feature中取heatmap label为0的。这样的操作会产生可变长度，需要固定长度补0操作。</p>
<p><strong>Decoder Supervision Loss</strong></p>
<p>提出的Pose-guided Push Loss：</p>
<p>余弦距离：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329152506162.png" alt="image-20220329152506162"></p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329152517889.png" alt="image-20220329152517889"></p>
<p>整体的loss定义：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329153030797.png" alt="image-20220329153030797"></p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>ReID</tag>
        <tag>Transformer</tag>
        <tag>Landmark</tag>
      </tags>
  </entry>
  <entry>
    <title>《DVG-Face:Dual Variational Generation for Heterogeneous Face Recognition》</title>
    <url>/2022/04/09/review_3_DVG-Face/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#%E4%B8%8E%E5%89%8D%E4%BD%9Cdvg%E7%9A%84%E4%B8%8D%E5%90%8C">与前作DVG的不同</a></li>
<li><a href="#%E5%89%8D%E5%A4%87%E7%9F%A5%E8%AF%86">前备知识</a></li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a><ul>
<li><a href="#dual-generation">Dual Generation</a><ul>
<li><a href="#training-with-paired-heterogeneous-data">Training with Paired Heterogeneous Data</a></li>
<li><a href="#training-with-unpaired-vis-data">Training with Unpaired VIS Data</a></li>
</ul>
</li>
<li><a href="#heterogeneous-face-recognition">Heterogeneous Face Recognition</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://arxiv.org/pdf/2009.09399.pdf">2009.09399.pdf (arxiv.org)</a></p>
<p>code: <a href="https://github.com/WangTaoAs/PFD_Net"><a href="https://github.com/BradyFU/DVG-Face">BradyFU&#x2F;DVG-Face: DVG-Face: Dual Variational Generation for Heterogeneous Face Recognition, TPAMI 2021 (github.com)</a></a></p>
<span id="more"></span>

<h1><span id="出发点">出发点</span></h1><p>为解决异构人脸识别（Heterogeneous Face Recognition）问题中成对异构数据匮乏的问题。</p>
<h1><span id="创新点">创新点</span></h1><p>将异构人脸识别视为一个双生成（dual generation）问题，<strong>从噪声中采样大规模的成对</strong>异构人脸数据；</p>
<p>将丰富的<strong>身份信息集成到联合分布</strong>中，以丰富生成数据的身份多样性。同时，对生成的成对图像施加一个保持成对身份的损失（<strong>pairwise identity preserving loss</strong>），以确保它们的身份一致性。这两个特性使得能够更好地利用生成的未标记数据来训练异构人脸识别网络；</p>
<p>通过将生成的成对图像视为正对，将从不同样本获取的图像视为负对，通过<strong>对比学习</strong>对异构人脸识别网络进行优化，以学习domain-invariant和区分性的embedding feature。</p>
<h1><span id="与前作dvg的不同">与前作DVG的不同</span></h1><p><strong>生成图像的身份更丰富：</strong></p>
<p>对于前作，生成器只能使用小规模的成对异构数据进行训练，从而限制生成图像的身份多样性。在当前版本中，重新设计了生成器的体系结构和训练方式，允许使用成对异构数据和<strong>大规模未配对VIS数据</strong>（单模态的非成对真实人脸数据）对其进行训练。后者的引入极大地丰富了生成图像的身份多样性。 </p>
<p><strong>生成的图像被更有效地利用：</strong></p>
<p>前作借助身份一致性属性，通过成对距离损失（pairwise distance loss）使用生成的成对数据对异构人脸识别网络进行训练。在此基础上，得益于上述身份多样性特性，当前版本进一步将从不同样本中获得的图像视为负对，形成了一种<strong>对比学习</strong>机制。 先前版本只能利用生成的图像来减少域差异，而当前版本则利用生成的图像来学习域不变和区分性嵌入特征（可学习）。 </p>
<p><strong>增加了更深入的分析和更多的实验：</strong></p>
<p>增加不同模态对图像的实验。</p>
<h1><span id="前备知识">前备知识</span></h1><p><strong>VAE</strong></p>
<p><a href="https://zhuanlan.zhihu.com/p/364917826">变分自编码器（VAE）原理 - 知乎 (zhihu.com)</a></p>
<h1><span id="做法">做法</span></h1><p><img src="/2022/04/09/review_3_DVG-Face/image-20220409140033446.png" alt="image-20220409140033446"></p>
<p>待解决问题：</p>
<p>（1）如何生成不同的配对异构数据</p>
<p>（2）如何有效利用这些生成的数据</p>
<h2><span id="dual-generation">Dual Generation</span></h2><p><strong>核心：结合域属性和身份特征</strong></p>
<p>通过一个双变分生成器实现。生成器包含两个特定域的encoder（图中的橙色和灰色Ev，En），一个decoder（浅蓝色G），一个预训练好的人脸识别网络（F）以及一个身份采样器（Fs）。</p>
<p>两个<em>Domain-specific attribute encoders</em>用于学习NIR和VIS数据的领域特定属性分布，人脸识别网络用于提取身份特征，身份采样器可以灵活地从噪声中采样丰富的身份表示（？）。成对异构数据的联合分布由身份表示和属性分布组成（具体的fusion文中并未指出，代码中是concat），decoder将联合分布映射到像素空间。</p>
<h3><span id="training-with-paired-heterogeneous-data">Training with Paired Heterogeneous Data</span></h3><p>输入成对同身份的异构图像，生成器学习潜在空间中的解耦联合分布。具体而言，采用在MS-Celeb-1M上预训练的人脸识别模型（本文采用的是LightCNN）作为特征提取器，由F提取出的特征被认为仅仅是identity related。考虑到F是从VIS模态预训练得到的，在另一个模态的表现不好，那么只需要提取VIS模态的身份特征作为两个模态共同的身份表示。</p>
<p>两个encoder提取出Domain-specific attribute分布，为确保其仅仅是属性相关的，在属性和身份表示之间施加了角度正交损失。最后解耦后的两种分布构成成对NIR-VIS数据的联合分布，然后被送到decoder作为输入。</p>
<p>该过程中涉及到了四个损失函数：包括角正交损失、分布学习损失、成对身份保持损失和对抗性损失。</p>
<p><strong>Angular Orthogonal Loss</strong></p>
<p>角正交损失施加在Zv和f，Zn和f之间，计算它们之间的余弦相似度，最小化他们的绝对值的和。</p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409153226995.png" alt="image-20220409153226995"></p>
<p><strong>Distribution Learning Loss</strong></p>
<p>分布学习损失启发自VAEs，首先用KL散度计算两个分布的差异，再结合L1正则化重构decoder的输入。</p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409162532051.png" alt="image-20220409162532051"></p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409162539110.png" alt="image-20220409162539110"></p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409162548220.png" alt="image-20220409162548220"></p>
<p><strong>Pairwise Identity Preserving Loss</strong></p>
<p>为了保留生成数据的身份，以前基于条件生成的方法通常采用身份保留损失。利用预训练好的人脸识别网络分别提取生成数据和真实目标数据的嵌入特征，然后迫使这两个特征尽可能接近。然而，由于既不存在类内约束，也不存在类间约束，因此很难保证生成的图像属于与目标一致的特定类。</p>
<p>本文关注生成的成对图像的身份一致性，而不是生成的图像属于谁。因此提出了一种成对的身份保持损失，以限制特征之间的距离</p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409172316619.png" alt="image-20220409172316619"></p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409172325881.png" alt="image-20220409172325881"></p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409172332593.png" alt="image-20220409172332593"></p>
<p><strong>Adversarial Loss</strong></p>
<p>引入对抗损失来提高生成图像的清晰度。</p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409184040111.png" alt="image-20220409184040111"></p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409184047202.png" alt="image-20220409184047202"></p>
<p><strong>Overall Loss</strong></p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409184118573.png" alt="image-20220409184118573"></p>
<h3><span id="training-with-unpaired-vis-data">Training with Unpaired VIS Data</span></h3><p>身份信息获取的一种简单的方法是使用预训练的人脸识别网络从大规模VIS数据中提取身份。然而，如果希望在测试阶段生成大规模的新配对数据，必须拥有相同数量的具有不同身份的VIS数据（如b图右下角）。</p>
<p>为避免此情况，引入了身份采样器（identity sampler）。具体实现为，首先采用识别网络提取MS-Celeb-1M数据集上的embedding特征，利用这些特征来训练VAE模型。训练后的VAE的decoder被用作身份采样器，它可以将标准高斯噪声中的点映射到身份表示。</p>
<p>由于这些采样的身份表示没有对应的ground true异构图像，本文建议以不成对的方式训练生成器。 </p>
<p><strong>整体流程：</strong></p>
<p>首先，通过身份采样器Fs对身份特征f~（经过识别网络提取）进行采样。</p>
<p>然后，将两种属性分布特征Zn和Zv以及f~分别concat输入decoder G。</p>
<p>最后，生成一对不属于异构数据库的新异构图像。 </p>
<p>其中的loss和train with paired的情况类似，只不过没有了对抗损失（具体可见train_generator.py代码中的loss部分）。</p>
<h2><span id="heterogeneous-face-recognition">Heterogeneous Face Recognition</span></h2><p>与训练好的LightCNN作为backbone，使用有限的数据对进行训练，再使用大规模生成后的数据进行训练对比。loss选择softmax loss。backbone在训练生成器时是权重更新的，在此处HRN中固定。</p>
<p>对于生成的数据，由于没有特定的类别标签，上述softmax loss不适用。引入<strong>对比学习</strong>机制来利用这些数据。</p>
<p>对比学习机制流程：</p>
<p>首先从生成数据中采样两对异构数据，基于生成的都是身份一致的，这两对都是正例，再将其做交叉，构造出了两对负例，要注意的是要保证交叉后的模态还是跨模态的。对比损失如下：</p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409191600670.png" alt="image-20220409191600670"></p>
<p>其中m是一个margin值。</p>
<p>整体的HFR网络损失为：</p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409191702568.png" alt="image-20220409191702568"></p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>HFR</tag>
        <tag>generation</tag>
      </tags>
  </entry>
  <entry>
    <title>《CariMe:Unpaired Caricature Generation with Multiple Exaggerations》</title>
    <url>/2022/04/13/review_4_CariMe/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E4%BB%BB%E5%8A%A1">任务</a></li>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a><ul>
<li><a href="#multi-exaggeration-warper">Multi-exaggeration Warper</a><ul>
<li><a href="#warp-reconstruction-loss">Warp Reconstruction Loss</a></li>
<li><a href="#photo-reconstruction-loss">Photo Reconstruction Loss</a></li>
<li><a href="#total-variation-loss">Total Variation Loss</a></li>
<li><a href="#wrapper-total-loss">Wrapper Total Loss</a></li>
</ul>
</li>
<li><a href="#styler">Styler</a><ul>
<li><a href="#adversarial-loss">Adversarial Loss</a></li>
<li><a href="#image-reconstruction-loss">Image Reconstruction Loss</a></li>
<li><a href="#cycle-consistency-loss">Cycle Consistency Loss</a></li>
<li><a href="#style-total-loss">Style Total Loss</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.pdf">https://ieeexplore.ieee.org/abstract/document/9454341/</a></p>
<p>code: <a href="https://github.com/edward3862/CariMe-pytorch">edward3862&#x2F;CariMe-pytorch: Unpaired Caricature Generation with Multiple Exaggerations (TMM 2021) (github.com)</a></p>
<span id="more"></span>

<h1><span id="任务">任务</span></h1><p>漫画生成、图像到图像的翻译、图像扭曲、风格转换</p>
<h1><span id="出发点">出发点</span></h1><p>与一般的image-to-image translation不同，由于各种空间变形的存在，自动绘制漫画是一项更具挑战性的任务。原先的漫画生成方式都是instance-level，本文拓展到distribution-level。</p>
<h1><span id="创新点">创新点</span></h1><p>CariMe可生成具有<strong>多重</strong>夸张和<strong>多种</strong>风格的漫画</p>
<p>提出了一种基于变形场（deformation fields）的非成对漫画生成方法，可以有效地学习真实照片到漫画的空间变换分布。还引入了一个辅助内容编码（auxiliary content code），以帮助产生有意义的、特定于照片的夸张。</p>
<h1><span id="做法">做法</span></h1><p>一个wrapper，一个styler</p>
<h2><span id="multi-exaggeration-warper">Multi-exaggeration Warper</span></h2><p>分别输入一张漫画脸和真实脸，首先计算所有漫画的平均landmark（代码中的main_cal_wrap_degree.py），<img src="/2022/04/13/review_4_CariMe/image-20220414090134868.png" alt="image-20220414090134868">从每个漫画中得到，表达了该张漫画所表示的特定的夸张模式，然后该特征经过encoder获得低维向量Zw（wrap code)，与此同时引入了对应的真实人脸过另一个encoder提取Zp（auxiliary photo-specific content code)，Zp与Zw都是满足标准正态分布的向量，在测试阶段Zw是从正态分布中随机采样获取，以获得不同的变形编码。down scale和up scale操作文中的解释是给变形场去噪。</p>
<p><img src="/2022/04/13/review_4_CariMe/image-20220414083337682.png" alt="image-20220414083337682"></p>
<h3><span id="warp-reconstruction-loss">Warp Reconstruction Loss</span></h3><p><img src="/2022/04/13/review_4_CariMe/image-20220414092823399.png" alt="image-20220414092823399"></p>
<p>L1正则，旨在拉近reconstruction后的变形场与原变形场的差异。</p>
<h3><span id="photo-reconstruction-loss">Photo Reconstruction Loss</span></h3><p><img src="/2022/04/13/review_4_CariMe/image-20220414093327113.png" alt="image-20220414093327113"></p>
<p>L1正则，旨在拉近reconstruction后的真实人脸与输入真实人脸的差异，来鼓励Zp保持对原真是人脸的内容和空间信息。</p>
<h3><span id="total-variation-loss">Total Variation Loss</span></h3><p><img src="/2022/04/13/review_4_CariMe/image-20220414094156859.png" alt="image-20220414094156859"></p>
<p>用来给生成的图像去噪。</p>
<p><a href="https://blog.csdn.net/qq_38406029/article/details/118996415">(54条消息) TV Loss详解_鬼道2022的博客-CSDN博客_tvloss公式</a></p>
<h3><span id="wrapper-total-loss">Wrapper Total Loss</span></h3><p><img src="/2022/04/13/review_4_CariMe/image-20220414094730445.png" alt="image-20220414094730445"></p>
<h2><span id="styler">Styler</span></h2><p>旨在做风格迁移，将图像分界为content representation和style code。</p>
<p>一个style encoder，一个context encoder，一个style decoder（AdaLIN:<a href="https://blog.csdn.net/weixin_43823140/article/details/107840916">(54条消息) 【飞桨】论文解读：U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance_目楽 Leo mu的博客-CSDN博客</a>），style输出一个正态分布的style code，context输出feature map。</p>
<p>将context输出的feature map分别进行instance normalization（styler中常用的一种像素normalization）和layer normalization，combine方式需要到代码中查看，送到AdaLIN中，其范式为：</p>
<p><img src="/2022/04/13/review_4_CariMe/image-20220414101725797.png" alt="image-20220414101725797"></p>
<p><img src="/2022/04/13/review_4_CariMe/image-20220414083358409.png"></p>
<h3><span id="adversarial-loss">Adversarial Loss</span></h3><p><img src="/2022/04/13/review_4_CariMe/image-20220414101755992.png" alt="image-20220414101755992"></p>
<p>AdaLIN是基于GAN的方法，所以有adversarial loss。</p>
<h3><span id="image-reconstruction-loss">Image Reconstruction Loss</span></h3><p><img src="/2022/04/13/review_4_CariMe/image-20220414101914719.png" alt="image-20220414101914719"></p>
<p>L1正则，旨在拉近reconstruction后的图像与原图像的距离。</p>
<h3><span id="cycle-consistency-loss">Cycle Consistency Loss</span></h3><p>对生成后的图像再次进入encoder，输出特征与之前encoder输出做L1正则，这里的cycle是循环，不是cycle loss！</p>
<p><img src="/2022/04/13/review_4_CariMe/image-20220414103236545.png" alt="image-20220414103236545"></p>
<h3><span id="style-total-loss">Style Total Loss</span></h3><p><img src="/2022/04/13/review_4_CariMe/image-20220414103409597.png" alt="image-20220414103409597"></p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>generation</tag>
        <tag>caricature</tag>
      </tags>
  </entry>
  <entry>
    <title>《Clothes-Changing Person Re-identification with RGB Modality Only》</title>
    <url>/2022/05/02/review_5_Simple-CCReID/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E4%BB%BB%E5%8A%A1">任务</a></li>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a><ul>
<li><a href="#clothes-based-adversarial-loss">Clothes-based Adversarial Loss</a><ul>
<li><a href="#training-clothes-classifier">Training clothes classifier</a></li>
<li><a href="#learning-clothes-irrelevant-features">Learning clothes-irrelevant features</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.pdf">https://arxiv.org/pdf/2204.06890v1.pdf</a></p>
<p>code: <a href="https://github.com/guxinqian/simple-ccreid">guxinqian&#x2F;Simple-CCReID: Pytorch implementation of ‘Clothes-Changing Person Re-identification with RGB Modality Only. In CVPR, 2022.’ (github.com)</a></p>
<span id="more"></span>

<h1><span id="任务">任务</span></h1><p>换装人员重识别</p>
<h1><span id="出发点">出发点</span></h1><p><strong>提取与衣服无关的特征</strong>，如脸型、发型、体形和步态。目前大多数工作主要集中在从多模态信息（如剪影和草图）中建模身体形状，但没有充分利用原始 RGB 图像中与衣服无关的信息。</p>
<h1><span id="创新点">创新点</span></h1><p>文章提出一种基于衣服的<strong>对抗</strong>性损失（CAL），通过惩罚 re-id 模型对衣服的预测能力（其实就是个分类器），从原始 RGB 图像中<strong>挖掘与衣服无关的特征</strong>。</p>
<h1><span id="做法">做法</span></h1><p>首先通过最小化Lc来优化衣服分类器。然后固定衣服分类器的参数，最小化Lid和Lca，迫使backbone学习与衣服无关的特征。（每张图片对应一个一个身份标签和服装标签，将衣服类定义为细粒度标识类——所有同一身份的样本都根据他们的衣服被划分为属于该身份的不同类别，不同的人即使穿着相同的衣服也不会共享相同的衣服标签）</p>
<p><img src="/2022/05/02/review_5_Simple-CCReID/image-20220504092335366.png" alt="image-20220504092335366"></p>
<h2><span id="clothes-based-adversarial-loss">Clothes-based Adversarial Loss</span></h2><h3><span id="training-clothes-classifier">Training clothes classifier</span></h3><p>Lc采用交叉熵损失</p>
<h3><span id="learning-clothes-irrelevant-features">Learning clothes-irrelevant features</span></h3><p>固定衣服分类器的参数，并强制主干学习与衣服无关的特征。为此，应该<strong>惩罚re-id模型对于衣服的预测能力</strong>。</p>
<p>然而，由于衣服类被定义为细粒度类，因此惩罚re-id模型对于所有衣服类别的预测能力，也会降低其预测身份的能力，这对re-id是有害的。</p>
<p>那么最终的目的是使经过训练的<strong>服装分类器无法区分相同身份和不同服装的样本</strong>。</p>
<p>所以，Lca是一个<em>multi-positive-class classification loss</em>，属于同一个身份的不同服装类互为正类。（把不同衣服的只要是属于同一个id身份的衣服类都视为衣服正类）</p>
<p><img src="/2022/05/02/review_5_Simple-CCReID/image-20220504101750986.png" alt="image-20220504101750986"></p>
<p>Si+（Si-)是一个有相同身份的衣服标签fi的集合，K是S中的衣服类别数量，q(c)是第c个衣服类别的交叉熵损失的权重，同一件衣服同一个身份的正例，和不同衣服同一个身份的正例有相同的权重1&#x2F;K。</p>
<p>同时，为了在不严重降低衣服一致性识别精度的情况下提高模型的换衣识别能力，等式（4）可替换为：</p>
<p><img src="/2022/05/02/review_5_Simple-CCReID/image-20220504105337953.png" alt="image-20220504105337953"></p>
<p><img src="/2022/05/02/review_5_Simple-CCReID/image-20220504105427890.png" alt="image-20220504105427890"></p>
<p>另外，穿着相同衣服的正例比穿着不同衣服的正例有更大的权重。</p>
<p>在优化CAL的同时，对身份分类器进行了优化。因此，第二步的优化过程是：</p>
<p><img src="/2022/05/02/review_5_Simple-CCReID/image-20220504105540307.png" alt="image-20220504105540307"></p>
<p>当仅使用Lid进行训练时，该模型在优化的早期阶段倾向于学习简单样本（具有相同的衣服），然后逐渐学习区分困难样本（具有相同的身份和不同的衣服）。类似课程学习方式，但尽管如此（6）中并未抛弃Lid。其原因是，在优化的早期阶段，只有最小化Lca并迫使模型区分硬样本，才可能导致局部最优。相反，在我们的实验中，在第一次降低学习率后，增加了Lca用于训练。</p>
<p>&#96;&#96;</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class ClothesBasedAdversarialLoss(nn.Module):</span><br><span class="line">    &quot;&quot;&quot; Clothes-based Adversarial Loss.</span><br><span class="line"></span><br><span class="line">    Reference:</span><br><span class="line">        Gu et al. Clothes-Changing Person Re-identification with RGB Modality Only. In CVPR, 2022.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        scale (float): scaling factor.</span><br><span class="line">        epsilon (float): a trade-off hyper-parameter.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, scale=16, epsilon=0.1):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.scale = scale</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line"></span><br><span class="line">    def forward(self, inputs, targets, positive_mask):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Args:</span><br><span class="line">            inputs: prediction matrix (before softmax) with shape (batch_size, num_classes)</span><br><span class="line">            targets: ground truth labels with shape (batch_size)</span><br><span class="line">            positive_mask: positive mask matrix with shape (batch_size, num_classes). The clothes classes with </span><br><span class="line">                the same identity as the anchor sample are defined as positive clothes classes and their mask </span><br><span class="line">                values are 1. The clothes classes with different identities from the anchor sample are defined </span><br><span class="line">                as negative clothes classes and their mask values in positive_mask are 0.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        inputs = self.scale * inputs</span><br><span class="line">        negtive_mask = 1 - positive_mask</span><br><span class="line">        identity_mask = torch.zeros(inputs.size()).scatter_(1, targets.unsqueeze(1).data.cpu(), 1).cuda()</span><br><span class="line"></span><br><span class="line">        exp_logits = torch.exp(inputs)</span><br><span class="line">        log_sum_exp_pos_and_all_neg = torch.log((exp_logits * negtive_mask).sum(1, keepdim=True) + exp_logits)</span><br><span class="line">        log_prob = inputs - log_sum_exp_pos_and_all_neg</span><br><span class="line"></span><br><span class="line">        mask = (1 - self.epsilon) * identity_mask + self.epsilon / positive_mask.sum(1, keepdim=True) * positive_mask</span><br><span class="line">        loss = (- mask * log_prob).sum(1).mean()</span><br><span class="line"></span><br><span class="line">        return loss</span><br></pre></td></tr></table></figure>





]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>ReID</tag>
        <tag>decouple</tag>
        <tag>adversarial</tag>
      </tags>
  </entry>
  <entry>
    <title>《IACycleGAN》</title>
    <url>/2022/06/08/review_6_IACycleGAN/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E4%BB%BB%E5%8A%A1">任务</a></li>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a><ul>
<li><a href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">生成模型</a><ul>
<li><a href="#%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93loss%E5%9B%BE%E7%A4%BA">生成网络整体loss图示</a></li>
</ul>
</li>
<li><a href="#%E8%AF%86%E5%88%AB%E7%BD%91%E7%BB%9C%E4%B8%8E%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BA%92%E7%9B%B8%E4%BC%98%E5%8C%96">识别网络与生成网络的互相优化</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E9%AA%8C">实验</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86">数据集</a><ul>
<li><a href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%96%BD%E7%BB%86%E8%8A%82">生成模型实施细节</a></li>
<li><a href="#%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%96%BD%E7%BB%86%E8%8A%82">识别模型实施细节</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.pdf"><a href="https://arxiv.org/pdf/2103.16019.pdf">Identity-Aware CycleGAN for Face Photo-Sketch Synthesis and Recognition (arxiv.org)</a></a></p>
<p>code: none</p>
<span id="more"></span>

<h1><span id="任务">任务</span></h1><p>真人素描生成与识别</p>
<h1><span id="出发点">出发点</span></h1><p>生成促进识别 识别促进生成；</p>
<p>大部分生成方法使得合成图像与原始图像在纹理上保持一致，会导致信息丢失；</p>
<p>大多数生成框架都只能学习两个域之间的关系，其鉴别器只关注照片和草图之间的差异，而不考虑任何特定的识别优化（身份信息）；</p>
<h1><span id="创新点">创新点</span></h1><p>在CycleGAN上加入了感知损失（perceptual loss）,能更好的关注面部的语义信息（眼睛、鼻子）;</p>
<p>使生成模型和识别模型相互优化，生成模型迭代生成更好的图像，Triplet Loss训识别模型；</p>
<h1><span id="做法">做法</span></h1><h2><span id="生成模型">生成模型</span></h2><p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608102426736.png" alt="image-20220608102426736"></p>
<p>分别给出两个domain的照片（这里输入的是paired数据，这里的paired应该身份paired），其训练目的是获得Gx，Gy两个生成器。</p>
<p>两个识别网络，其目的是使用pretrain好的vggface提取feature做Identity perception loss；</p>
<p>生成器用了《Perceptual losses for real-time style transfer and super-resolution》中的结构；</p>
<p>判别器用了PatchGAN的结构；</p>
<h3><span id="生成网络整体loss图示">生成网络整体loss图示</span></h3><p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608105545461.png" alt="image-20220608105545461"></p>
<h4><span id="对抗损失adversarial-loss">对抗损失（adversarial loss）</span></h4><p>输入图像与生成图像进入判别器进行计算，最小化下式</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608110633855.png" alt="image-20220608110633855"></p>
<h4><span id="循环一致性损失cycle-consistency-loss">循环一致性损失（cycle consistency loss）</span></h4><p>Gx生成的图像再进入Gy恢复原本domain与最初的输入x计算L1，<strong>此loss为pix级</strong>，最小化下式</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608110959225.png" alt="image-20220608110959225"></p>
<h4><span id="身份保持损失identity-perception-loss">身份保持损失（identity perception loss）</span></h4><p>仅使用对抗损失会导致伪影和训练不稳定，需要加更强大的监督；</p>
<p>通过一个pretrain的识别网络（文中选择vggface）分别提取两对（原图与合成图）的feature计算L2，<strong>此loss区别于pix级监督，是feature级</strong>，文中对于此处的解释是：绘制的草图会有夸张成分以扭曲面部纹理信息，夸大面部特征，完全基于pix重建图像效果不会好；CycleGAN的训练需要进行数据增强操作（resize，flip等）难以实施pix级的监督。</p>
<p>最小化下式</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608111456554.png" alt="image-20220608111456554"></p>
<h4><span id="身份映射损失identity-mapping-loss">身份映射损失（identity mapping loss）</span></h4><p>常规的pix级的约束，最小化下式</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608112212476.png" alt="image-20220608112212476"></p>
<h4><span id="整体loss">整体loss</span></h4><p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608112236780.png" alt="image-20220608112236780"></p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608112252105.png" alt="image-20220608112252105"></p>
<p>最小化生成器的loss最大化判别器的loss</p>
<p>其中超参数lamda分别设为10，30000000，5</p>
<h2><span id="识别网络与生成网络的互相优化">识别网络与生成网络的互相优化</span></h2><p>许多主做生成的工作都是fix相应的识别网络作为一个特征提取器来附加身份保持损失；</p>
<p>本文的做法估计是：</p>
<p>step1：先fix识别网络参数训练生成网络，获得一定量的生成的图片；</p>
<p>step2：使用生成的图片fine-tune识别网络（单走一个人脸识别模型的流程），用了triplet loss，更新识别网络参数，其中两个模态分别有两个识别网络，要分开训练；</p>
<p>step3：重复上述操作以获得更好的生成模型和识别模型。</p>
<p>（有问题的点：若第一次生成的质量得不到保证，那低质量的生成图像真的能提升识别模型的acc吗？<strong>互相优化的模型很依赖第一次生成的图像质量</strong>）</p>
<p>我认为的做法：先简单训练一个识别网络达到一个还行的acc，更新其backbone参数作为生成网络的特征提取器，然后再进行生成网络的训练。</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608135047478.png" alt="image-20220608135047478"></p>
<h1><span id="实验">实验</span></h1><p>CycleGAN生成—》一阶段vgg fine-tune—》IACycleGAN生成（加入fine-tune的vgg提取的特征做身份保持损失）—》二阶段vgg fine-tune</p>
<h2><span id="数据集">数据集</span></h2><p>CUFS和CUFSF</p>
<h3><span id="生成模型实施细节">生成模型实施细节</span></h3><p>对于生成网络的训练，都是从头开始训练，使用instance normalization来实现更好的稳定性和更低的噪声；</p>
<p>使用Adam优化器，horizontal filp prob&#x3D;0.5用于数据增强；</p>
<p>前100个epoch设置0.0002的学习率，并在后100个epoch线性下降至0；</p>
<p>在titian xp上训练了10小时；</p>
<p>为减小网络震荡，采用存储多个生成图像的图像缓冲区来更新鉴别器，而不是使用最后生成的图像。</p>
<p>生成使用了SSIM FSIM两个指标</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608152343197.png" alt="image-20220608152343197"></p>
<h3><span id="识别模型实施细节">识别模型实施细节</span></h3><p>文中用的vggface，caffe上跑的（估计是官方代码）</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608153331083.png" alt="image-20220608153331083"></p>
<p>在做检索任务时，先做风格模态的迁移，再计算相似度。</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608155946505.png" alt="image-20220608155946505"></p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608160958943.png" alt="image-20220608160958943"></p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>sketch</tag>
        <tag>synthesis</tag>
        <tag>GAN</tag>
        <tag>face recognition</tag>
      </tags>
  </entry>
  <entry>
    <title>《Learning to Warp for Style Transfer》</title>
    <url>/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E4%BB%BB%E5%8A%A1">任务</a></li>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C">相关工作</a><ul>
<li><a href="#texture-nst">Texture NST</a><ul>
<li><a href="#%E5%9C%A8%E7%BA%BF%E4%BC%98%E5%8C%96%E7%9A%84%E5%BD%A2%E5%BC%8F">在线优化的形式</a></li>
<li><a href="#%E7%A6%BB%E7%BA%BF%E4%BC%98%E5%8C%96%E7%9A%84%E5%BD%A2%E5%BC%8F">离线优化的形式</a></li>
<li><a href="#%E5%85%B6%E4%BB%96%E5%8F%98%E5%BC%8F">其他变式</a></li>
</ul>
</li>
<li><a href="#geometric-nst">Geometric NST</a></li>
</ul>
</li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a><ul>
<li><a href="#geometric-stylewarper">Geometric Style（Warper）</a><ul>
<li><a href="#feature-extraction">Feature Extraction</a></li>
<li><a href="#feature-correlation">Feature Correlation</a></li>
<li><a href="#warp-network-training-and-using">Warp Network: Training and Using</a></li>
</ul>
</li>
<li><a href="#texture-stylestyler">Texture Style（Styler）</a></li>
<li><a href="#%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82">实现细节</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.pdf"><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Learning_To_Warp_for_Style_Transfer_CVPR_2021_paper.pdf">Learning To Warp for Style Transfer (thecvf.com)</a></a></p>
<p>code: <a href="https://github.com/xch-liu/learning-warp-st">https://github.com/xch-liu/learning-warp-st</a></p>
<span id="more"></span>

<h1><span id="任务">任务</span></h1><p>NST（neural style transfer）</p>
<h1><span id="出发点">出发点</span></h1><p>大多数风格迁移只考虑了迁移纹理信息，而为关注在艺术层面的几何扭曲。</p>
<p>本文考虑使用深度神经网络进行图像样式化的问题，特别关注<strong>艺术扭曲</strong>。</p>
<h1><span id="创新点">创新点</span></h1><p>在CycleGAN上加入了感知损失（perceptual loss）,能更好的关注面部的语义信息（眼睛、鼻子）;</p>
<p>使生成模型和识别模型相互优化，生成模型迭代生成更好的图像，Triplet Loss训识别模型；</p>
<p>与其他扭曲迁移模型的区别：</p>
<p>不同于The face of art: Landmark detection and geometric style in portraits和WarpGAN，其<strong>不限于单个语义类别</strong>；</p>
<p>不同于Deformable style transfer依赖前向和后向优化，其专门设计了前馈网络，以<strong>输出给定内容和几何图像的扭曲字段</strong>；</p>
<p>比Deformable style transfer快；</p>
<p>与Geometric style transfer仅限于参数化扭曲字段不同，其生成的是<strong>非参数化扭曲</strong>；</p>
<p>与Geometric style transfer以外的其他NST算法不同，其<strong>支持使用两幅图像来指定样式</strong>，这为图像创建增加了其他NST算法所没有的多功能性。</p>
<h1><span id="相关工作">相关工作</span></h1><h2><span id="texture-nst">Texture NST</span></h2><p>纹理NST一直是NST的主要形式（默认NST都是纹理NST）</p>
<h3><span id="在线优化的形式">在线优化的形式</span></h3><p>通过迭代优化图像来传递样式。</p>
<h3><span id="离线优化的形式">离线优化的形式</span></h3><p>离线优化生成模型，并在测试阶段通过一次向前传递生成样式化图像。</p>
<p>训好的模型一般只能迁移特定样式，有些模型将多种风格融合到一个模型中，或者使用一个模型来传递任意的艺术风格。</p>
<h3><span id="其他变式">其他变式</span></h3><p>肖像画风格转移、视觉属性转移、语义风格转移、视频风格转移、3D风格转移和照片级真实感风格转移</p>
<h2><span id="geometric-nst">Geometric NST</span></h2><p>几何形变的艺术风格的迁移越来越重要。</p>
<p>一些方法仅限于特定的内容域，如面部《The face of art: Landmark detection and geometric style in portraits》和文本。这些方法产生了极好的结果。</p>
<p>《Deformable style transfer》和《Geometric Style Transfer》描述了在多个类上操作的更通用的方法，增加的灵活性在质量方面似乎成本不高。</p>
<p>DST速度慢 支持任意形变</p>
<p>GST速度快 不支持任意形变</p>
<p>本文方法快且支持任意形变</p>
<h1><span id="做法">做法</span></h1><p>输入要求：</p>
<p>1）一张要被迁移的content图像Ic</p>
<p>2）一张指导几何迁移的图像Ig</p>
<p>3）一张指导纹理迁移的图像It</p>
<p>Ig和It可以是同一张</p>
<p>total pipeline如下（一个warper 一个styler）</p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220609205913858.png" alt="image-20220609205913858"></p>
<h2><span id="geometric-stylewarper">Geometric Style（Warper）</span></h2><p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220609210756668.png" alt="image-20220609210756668"></p>
<p>其关键思想是训练一个能够推断<strong>二维扭曲场w</strong>的神经网络，并创建一个衡量特征相似性的四维标量函数<strong>M</strong>。</p>
<p>有三个主要组成部分：</p>
<p>1）特征提取：分别获取 Fg 和 Fc</p>
<p>2）特征融合：来衡量特征相似度 M(Fc,Fg)</p>
<p>3）训练warp网络输出函数 f 使得 w &#x3D; f(M)，<strong>一旦训完，网络 f 可以在新输入上使用，无需修改</strong>，（即得到了一个通用的Warp Field Estimation用来估计warp degree）本文所有输出都是单个warper生成的。</p>
<p>定义warp field的w是非参数化的，不对分布做假设，就是一些统计量，不用网络进行分布的约束。</p>
<h3><span id="feature-extraction">Feature Extraction</span></h3><p>使用VGG，提取pool4出来的特征，接一个L2 normalization。输出的 F 是 W * H 的特征图，本文中是16 * 16，文中说是平衡了计算效率和扭曲质量得出的结果。</p>
<h3><span id="feature-correlation">Feature Correlation</span></h3><p>此模块计算feature map在每个pixel上的特征关联分数，结果存储在四维标量函数 M 中,M∈R W×H×W×H，其中的每个元素的计算规则如下：</p>
<p>Fc 是content图像提取的feature；</p>
<p>Fg是geometric图像提取的feature；</p>
<p>i，j 对应Fc中的pix坐标</p>
<p>k，l 对应Fg中pix的坐标</p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220610101853238.png" alt="image-20220610101853238"></p>
<p>分母为遍历Fg上的每个pix与Fc( i , j ) 做内积求平方开根；</p>
<p>分子为Fc（i，j）与Fg（k，l）做内积。</p>
<p>个人理解：这个统计量表示了每个两个feature间的关联性特征量。</p>
<h3><span id="warp-network-training-and-using">Warp Network: Training and Using</span></h3><p>本文在技术上的贡献为其训练了一个 f 来输出非参数化的warp field（w）。</p>
<p>输入feature correlation（M），该步骤可以表示为 w &#x3D; f (M)，形式化定义这层mapping：</p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220610191607879.png" alt="image-20220610191607879"></p>
<p>W1 , H1是图像尺寸，最终warp module输出根据Ig的几何形变warp后的Ic。</p>
<p>原则上不需要进行训练，因为如下的优化问题对于任何的图像对的数据都足够解决：</p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220610192737703.png" alt="image-20220610192737703"></p>
<p>h是一个度量函数。</p>
<p>此基于优化的方式来自于DST，但与直接从经过训练的网络计算扭曲场相比，单实例优化速度较慢。（本文没采用基于优化的方式，而是单独训练了一个网络）</p>
<p>本文网络使用<strong>一组语义相关或具有几何相似部分的图像对</strong>进行训练。图像对涵盖了广泛的语义内容：人脸、动物等。为了提高模型在艺术领域的泛化能力，我们使用艺术增强来创建每个训练图像的纹理增强副本。<strong>经过训练后，变形网络可以应用于任何图像，无论其语义内容如何</strong>。</p>
<p><strong>其基本思想是局部移动内容图像中的像素，并重新计算新扭曲图像中的特征，直到loss收敛</strong></p>
<p>定义Fm来表示受像素m影响的</p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220610200411017.png" alt="image-20220610200411017"></p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220610200701085.png" alt="image-20220610200701085"></p>
<p><strong>这个最终的softmax归一化输出代表每个content图像的pixel在每个扭曲图像中的search window中的关联程度</strong></p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220610213521698.png" alt="image-20220610213521698"></p>
<p>个人最终理解：</p>
<p><strong>输入扭曲图像和内容图像（待扭曲图像），使用VGG提取feature map，warp field矩阵扭曲Fc矩阵，与Fg矩阵进行feature matching计算一个关联度四维统计量，输入warp field estimator（神经网络），最小化每个pixel和其扭曲图像相应search window的关联度统计量来使得内容图像尽可能扭曲成目标扭曲样式；神经网络更新参数，输出更好的warp field，循环优化网络，直至达到良好的扭曲效果。</strong></p>
<p>前半部分机器学习，后半部分深度学习，这样warp field可以直接通过网络inference出来，速度提升。</p>
<h2><span id="texture-stylestyler">Texture Style（Styler）</span></h2><p>最小化content loss 和 texture loss，二者依赖于用于目标检测而训练的模型，本文唯一不同在于采取多尺度策略，<strong>优先将纹理随着细节的增加转移到输出图像的不同区域</strong>。几十年来，这种策略一直用于规定性纹理合成，最近用于仅纹理的NST，有助于改善风格转换结果。在本文工作中，利用它来解决<strong>由于几何扭曲而产生的模糊和其他瑕疵</strong>。</p>
<p>此部分不做详解</p>
<h2><span id="实现细节">实现细节</span></h2><p>warper在PF-PASCAL和MS COCO数据集上训练，可训练网络结构不大，单卡训了2小时</p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220611151019063.png" alt="image-20220611151019063"></p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>style transfer</tag>
        <tag>warp</tag>
      </tags>
  </entry>
  <entry>
    <title>《Geometric and Textural Augmentation for Domain Gap Reduction》</title>
    <url>/2022/06/12/review_8_geom%20tex%20dg/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E4%BB%BB%E5%8A%A1">任务</a></li>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#idea%E6%A6%82%E8%A7%88">idea概览</a></li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a><ul>
<li><a href="#%E5%87%A0%E4%BD%95%E5%A2%9E%E5%BC%BA">几何增强</a></li>
<li><a href="#%E7%BA%B9%E7%90%86%E5%A2%9E%E5%BC%BA">纹理增强</a></li>
<li><a href="#%E7%BB%93%E5%90%88%E5%87%A0%E4%BD%95%E5%92%8C%E7%BA%B9%E7%90%86%E5%A2%9E%E5%BC%BA">结合几何和纹理增强</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E9%AA%8C">实验</a></li>
<li><a href="#%E7%BB%93%E8%AE%BA">结论</a></li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Geometric_and_Textural_Augmentation_for_Domain_Gap_Reduction_CVPR_2022_paper.pdf">Geometric and Textural Augmentation for Domain Gap Reduction (thecvf.com)</a></p>
<p>code: <a href="https://github.com/xch-liu/geom-tex-dg">xch-liu&#x2F;geom-tex-dg: Geometric and Textural Augmentation for Domain Gap Reduction (github.com)</a></p>
<span id="more"></span>

<h1><span id="任务">任务</span></h1><p>通过纹理和风格分布采样增强训练数据提升不同风格不同形变物体识别效果。</p>
<h1><span id="出发点">出发点</span></h1><p>对于艺术品这种具有风格形变的目标识别性能不好；</p>
<p>有工作人为这是个域泛化的问题，但已被证明的是：<strong>同一个类别但不同样式</strong>的数据往往比<strong>不同类别但相同样式</strong>的数据的差别更大，从而阻碍常规的DG方法；</p>
<p>此外，照片和艺术品的数量也不尽相同。因此，从几乎完全由照片组成的训练数据转移到包含艺术品的测试集是一个重大挑战。</p>
<p>最近的工作集中于通过style transfer应用于训练示例来提高模型鲁棒性，减轻过拟合；</p>
<p><strong>理由为：通过输入不同风格纹理的图像数据，从而使得识别网络被迫关注于物体本身语义特征的学习，如形状等</strong></p>
<p>但大多数仅关注于纹理的迁移，而忽略了几何形变的迁移；</p>
<h1><span id="创新点">创新点</span></h1><p>在几何和纹理风格方面弥补了domain的差距，而不仅仅是纹理；</p>
<p>本文的增强过程不同于现有技术，当前的文献通过使用一组（纹理）风格的样本将照片处理成不同风格艺术品来扩充数据集，相反，本文构建<strong>纹理和几何描述符的独立分布</strong>，并<strong>从中采样</strong>以增加训练数据。我们的实验表明，几何和纹理增强提高了几种常见跨域基准的分类泛化能力。</p>
<h1><span id="idea概览">idea概览</span></h1><p>本文假设纹理style和对象身份是独立的；（类似 content 和 style 的概念）</p>
<p>同一身份类别中的图像的扭曲场一般是相似的，而跨身份类别图像的扭曲场存在显著差异；</p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220612171213941.png" alt="image-20220612171213941"></p>
<p>手写数字和马的扭曲场明显是不一样的，</p>
<p>样式包括纹理和几何。纹理样式与对象类无关：知道图片是水彩或剪贴画并不能预测其对象类（身份类别）。不过，几何样式取决于对象类。使用<strong>learning to warp</strong>（箭头上方的颜色编码扭曲字段和下面相应的变形结果），可以扭曲显示相同对象类但纹理样式不同的源图像。可以看到，对象类中的扭曲场是相似的，但在对象类之间差异很大，马和文字的扭曲场（可视化表示为图中的热力图）明显不一致。</p>
<h1><span id="做法">做法</span></h1><p>增强通过处理训练输入x来扩展训练数据，以生成新的训练输入A（x）。分为两个步骤：几何增强扭曲图像；纹理增强变更纹理。</p>
<p>这两步通过独立采样两个分布来执行，<strong>分布构建是采用预先训练好的特征提取器</strong>。</p>
<p>图示训练集S包含3种风格3个类别</p>
<p><strong>本文目标为学习一个预测模型，能够很好地泛化到一个unseen style domain，也就是说，使用训练集S构建一个分类器，当图像的style是unseen的时候也能表现得很好。</strong></p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220612212437877.png" alt="image-20220612212437877"></p>
<p><strong>本文默认同一个身份类别的图像有着相近的扭曲场；</strong></p>
<p><strong>左：一个简化的训练集，包括三个风格（艺术、照片和素描）中的三个对象（狗、长颈鹿、吉他）。</strong></p>
<p><strong>中：输入同一身份不同风格的图像进行排列组合构成图像对，图中是三种就构建了三种排列组合；然后送入扭曲模块，生成warp field W*；</strong></p>
<p><em><em>扭曲分布NW</em> 就由每个身份类别的扭曲场 W</em> 构建，并从这些分布中采样扭曲原本的图像，以达到增强效果；**</p>
<p><strong>右：纹理分布的构造。将训练集输入纹理样式预测网络F以生成纹理表示集V。纹理分布NV是基于V构造的。为了更好地显示，扭曲字段采用颜色编码，NW∗和NV在低维空间中可视化。</strong></p>
<h2><span id="几何增强">几何增强</span></h2><p>几何增强是通过对训练数据进行随机形变来实现的。每个随机形变都要有以下的几个要求：</p>
<p>（1）形变速度要足够快，以便再训练期间在线执行；</p>
<p>（2）形变程度要可控，以避免过度形变；</p>
<p>（3）最重要的是，形变的类型应该足够丰富，来确保可变性并且足够合理，以避免无意义和误导性的形变（应符合特定的身份类别——比如马不能按照狗形变）</p>
<p>本文使用了《Learning to Warp》（见上个markdown）的warper来实现上述效果；</p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220616212625252.png" alt="image-20220616212625252"></p>
<p>具体操作：</p>
<p>例如一个类别 k 中有N张图像，那这<strong>N张图像排列组合成所有的图像对</strong>，<strong>每种图像对都对应了一个的扭曲场，每个类别的扭曲场又构成一组扭曲场</strong>；</p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220616212720503.png" alt="image-20220616212720503"></p>
<p>如何将这些扭曲场施加到训练图像上呢？</p>
<p>如果直接从Wk（一组扭曲场）中采样，那么扭曲的样式数量就由K的大小限制住了，为了使本文的方法能够支持尽可能广泛的几何样式，<strong>基于Wk构造了一个几何分布，并直接从中采样新的扭曲场</strong>。</p>
<p>使用多元正态分布对扭曲分布进行建模，计算均值和协方差矩阵：</p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220616212437320.png" alt="image-20220616212437320"></p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220616212447197.png" alt="image-20220616212447197"></p>
<p>Wk表示为一个二维的矩阵，每一列代表一个 “矢量化” 的扭曲场。</p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220616212739581.png" alt="image-20220616212739581"></p>
<p>为减少计算均值和协方差矩阵的计算量，对Wk做了一步下采样操作。</p>
<h2><span id="纹理增强">纹理增强</span></h2><p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220617102147930.png" alt="image-20220617102147930"></p>
<p>采用Ghiasi的《Exploring the structure of a real-time, arbitrary neural artistic stylization network》，模型代码位置Dassl\dassl\modeling\backbone\styleaugment\styleaug\ghiasi.py</p>
<p>具体细节：</p>
<p>模型具体的两个模块：</p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220617104416760.png" alt="image-20220617104416760"></p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220617104423963.png" alt="image-20220617104423963"></p>
<p>通过一个纹理样式预测网络，从一个style图像预测一个100维的向量v，（trained on PBN数据集）；</p>
<p>这些向量组成一个矩阵；</p>
<p>使用多元正态分布对扭曲分布进行建模，计算均值和协方差矩阵（<strong>同扭曲分布的构建</strong>），不同之处在于，由于纹理央视和对象类的假设是独立的，可以将类别和域标签融合在一起（不细分类别，所有训练数据一起构建）；</p>
<p>在做纹理增强时，在Nv分布中随机采样v，并通过样式预测网络传输v，再再content图像Ic上来应用这个采样出的纹理v得到Io；</p>
<p>为了控制风格增强的程度，做了个线性插值：</p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220617105954463.png" alt="image-20220617105954463"></p>
<p>除了增强纹理样式表示之外，上述方法的另一个好处是计算效率。通过对图像进行批处理来构造纹理分布（一个batch先构造好？），然后进行直接表示采样，可以大大减少训练过程中的时间开销。</p>
<h2><span id="结合几何和纹理增强">结合几何和纹理增强</span></h2><p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220617110427313.png" alt="image-20220617110427313"></p>
<p>在训练过程中使用几何和纹理分布。在训练过程中，每个图像都可能被从特定类的扭曲分布中采样的扭曲场变形，然后可能使用来自与类无关的纹理分布的样本重新纹理。下面的虚线框中显示了混合几何和纹理样式的一些潜在增强。</p>
<h1><span id="实验">实验</span></h1><p>在本节中，我们将评估我们的方法在几个基准上的性能，并将其与最新的最先进的方法进行比较。我们还进行了烧蚀研究，以验证我们的识别方法中每个成分的重要性，无论描述风格如何。在每一种情况下，被测试的假设都是，增强通过扩大视觉对象类（VOC）以包括看不见的示例来增强对象分类性能。因此，我们不对任何测试图像应用任何类型的增强：我们的假设是VOC足够宽，可以包含新图像。</p>
<p>具体数据见代码文件组织结构。</p>
<p>做了<strong>多源和单源</strong>的域泛化效果对比。</p>
<h1><span id="结论">结论</span></h1><p><strong>结论1：</strong></p>
<p>从实验效果可知纹理和几何变换都有效，纹理影响更大；</p>
<p>证明了：纹理偏移大于几何偏移。根据我们的实验结果和之前一些研究的发现，这种现象可能有一些原因：</p>
<p>这与CNN对非形状特征的敏感性有关。CNN对范围广泛的图像处理非常敏感，对人类判断几乎没有影响。</p>
<p>具有纹理偏好的CNN可能表示归纳纹理偏差，这使得模型很难在小数据区域中学习几何相关特征，也很难将其推广到不同的分布，而不是它们所训练的分布。</p>
<p><strong>结论2：</strong></p>
<p>我们的实验结果还表明，几何和纹理增强的效果因数据集而异。最大的原因之一是数据集之间的对象和样式差异。</p>
<p>一些对象类本身具有几何形状差异，例如PACS中的动物和数字DG中的手写数字。相比之下，Office Home中的静态对象具有较少的类内形状变化。这意味着它们对几何样式的依赖不同。</p>
<p>（几何扭曲越多的数据集，几何增强影响越大）</p>
<p><strong>局限：</strong></p>
<p>由于我们的几何和纹理分布是基于源图像的相应特征表示构建的，因此它们强烈<strong>依赖于图像质量</strong>。如果特征表示远远不够好，则扩充空间将是次优的。</p>
<p><strong>补充：</strong></p>
<p>此外，对于不同的任务，如场景级分类、多对象图像，我们的纹理增强是适用的，但几何增强不能直接使用，因为它可能会在不考虑场景内容的情况下引入扭曲。一种潜在的改进方法是增加场景中的单个对象，这反过来需要对象检测，这是一个与分类不同的研究领域。这超出了本文的范围，但却是一个很好的未来探索方向。</p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>style transfer</tag>
        <tag>argument</tag>
      </tags>
  </entry>
  <entry>
    <title>《MeInGame:Create a Game Character Face from a Single Portrait》</title>
    <url>/2022/06/29/review_9_MeInGame/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E4%BB%BB%E5%8A%A1">任务</a></li>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E8%B4%A1%E7%8C%AE">贡献</a></li>
<li><a href="#%E6%96%B9%E6%B3%95">方法</a><ul>
<li><a href="#3d%E4%BA%BA%E8%84%B8%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA%E6%96%B9%E6%B3%95">3D人脸数据集构建方法</a></li>
<li><a href="#%E4%BA%BA%E8%84%B8%E5%BD%A2%E7%8A%B6%E9%87%8D%E5%BB%BA-face-shape-reconstruction">人脸形状重建 （Face Shape Reconstruction）</a></li>
<li><a href="#%E5%BD%A2%E7%8A%B6%E8%BD%AC%E6%8D%A2shape-transfer">形状转换（Shape Transfer）</a></li>
<li><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">损失函数</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Geometric_and_Textural_Augmentation_for_Domain_Gap_Reduction_CVPR_2022_paper.pdf"><a href="https://arxiv.org/pdf/2102.02371.pdf">2102.02371.pdf (arxiv.org)</a></a></p>
<p>code: <a href="https://github.com/xch-liu/geom-tex-dg"><a href="https://github.com/FuxiCV/MeInGame">FuxiCV&#x2F;MeInGame: MeInGame: Create a Game Character Face from a Single Portrait, AAAI 2021 (github.com)</a></a></p>
<span id="more"></span>

<h1><span id="任务">任务</span></h1><p>从单张真实2D肖像生成武侠风格的3D可形变模型。</p>
<h1><span id="出发点">出发点</span></h1><p>近年来，人们提出了许多基于深度学习的三维人脸重建方法，但在游戏中应用较少。当前的游戏角色定制系统要么要求玩家手动调整相当多的面部属性以获得所需的面部，要么限制面部形状和纹理的自由度。</p>
<h1><span id="贡献">贡献</span></h1><p>我们提出了一种低成本的三维人脸数据集创建方法。我们创建的数据集在种族和性别上是平衡的，面部形状和纹理都是从原始图像中创建的。我们将在论文被接受后将其公开。 提出了一种低成本的面部纹理获取方法</p>
<p>我们提出了一种将重建的3DMM人脸形状转换为游戏网格的方法，可以直接在游戏环境中使用。该方法与网格连通性无关，在实际应用中计算效率较高。 </p>
<p>为了消除光照和遮挡的影响，我们在对抗性训练范式下训练神经网络，从野外人脸图像中的单个图像预测一个完整的漫反射贴图。 </p>
<h1><span id="方法">方法</span></h1><p><img src="/2022/06/29/review_9_MeInGame/image-20220629143637063.png" alt="image-20220629143637063"></p>
<p>Shape Reconstructor pretrain好，获取3DMM系数和姿势系数，通过3DMM网络构成3DMM mesh（只有正面3D人脸），Shape Transfer将3DMM mesh转换成Game mesh（整个3D人头）；</p>
<p>根据Game mesh将原图uv展开创建一个粗糙纹理贴图，通过encoder decoder进一步细化至细化纹理；</p>
<p>还引入了一个光照predictor，预测光照系数；</p>
<p>最后，将预测的形状、纹理和照明系数一起提供给可微分渲染器，并强制渲染输出与输入照片类似。为了进一步改进结果，引入了两个鉴别器。</p>
<h2><span id="3d人脸数据集构建方法">3D人脸数据集构建方法</span></h2><p>1）给定一张人脸图像，通过预训练好的人脸分割网络检测皮肤区域</p>
<p>2）计算输入面部皮肤的平均颜色，并将平均皮肤颜色传输到模板纹理贴图（由游戏开发人员提供，是标准3D模型模板对应的标准UV模板） </p>
<p>3）根据变形的game mesh，将输入人脸图像展开到UV空间</p>
<p>4）使用Poisson blending（图像融合操作）将展开的贴图与UV模板混合，移除头发和眼镜等非皮肤区域，并尽可能使用对称性修补遮挡区域</p>
<h2><span id="人脸形状重建-face-shape-reconstruction">人脸形状重建 （Face Shape Reconstruction）</span></h2><p>第一步从输入图像预测3DMM形状和姿势系数。采用了<a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/AMFG/Deng_Accurate_3D_Face_Reconstruction_With_Weakly-Supervised_Learning_From_Single_Image_CVPRW_2019_paper.pdf">Accurate 3D Face Reconstruction With Weakly-Supervised Learning: From Single Image to Image Set (thecvf.com)</a>的方法，其他3DMM的方法同样适用。</p>
<h2><span id="形状转换shape-transfer">形状转换（Shape Transfer）</span></h2><p>形状传递模块的目的是将重建的3DMM网格传递到游戏网格。我们设计了基于Radial Basis Function（RBF）插值的形状传递模块。</p>
<h2><span id="损失函数">损失函数</span></h2><p>我们设计了损失函数来最小化渲染人脸图像和输入人脸照片之间的距离，以及精细纹理贴图和地面真实纹理贴图之间的距离。在渲染循环中，我们设计了四种类型的损失函数，即像素损失、感知损失、皮肤正则化损失和对抗性损失，以从全局外观和局部细节来衡量面部相似性。 </p>
<p><strong>像素损失（Pixel Loss）</strong></p>
<p>渲染后的图像R和输入图像I，做pixel loss：</p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629153338519.png" alt="image-20220629153338519"></p>
<p>GT的UV贴图G和细化的纹理贴图F，做pixel loss：</p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629154446786.png" alt="image-20220629154446786"></p>
<p><strong>感知损失（Perceptual Loss）</strong></p>
<p>在感知层面减小重建图像的差异，做法遵循[Structure Guided Img Inpaint using Edge Prediction - 知乎 (zhihu.com)](<a href="https://zhuanlan.zhihu.com/p/147654092#:~:text=%E3%80%8AEdgeConnect%3A">https://zhuanlan.zhihu.com/p/147654092#:~:text=《EdgeConnect%3A</a> Structure Guided Image Inpainting using Edge Prediction》,结构信息 中的 边缘信息 来实现图像的修复（类似Free-form的素描信息） 适用于： rectangular masks、irregular masks。)</p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629160619803.png" alt="image-20220629160619803"></p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629160626723.png" alt="image-20220629160626723"></p>
<p><strong>皮肤正则化损失 （Skin Regularization Loss）</strong></p>
<p>为了在整个面部产生恒定的肤色并去除高光和阴影，我们进行了两次损失来调整面部皮肤，即“对称损失”和“标准偏差损失”。与之前将皮肤正则化直接应用于顶点颜色的工作不同，我们对高斯模糊纹理贴图施加惩罚。这是基于一个事实，即一些个性化的细节（例如痣）并不总是对称的，并且与肤色无关。我们将对称损耗定义如下： </p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629160919834.png" alt="image-20220629160919834"></p>
<p>我们将表皮标准偏差损失定义如下： </p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629161001203.png" alt="image-20220629161001203"></p>
<p><strong>对抗损失（Adversarial Loss）</strong></p>
<p>为了进一步提高重建的逼真度，我们还在训练期间使用对抗性损失。我们引入了两个鉴别器，一个用于渲染人脸，另一个用于生成的UV纹理贴图。我们训练鉴别器来判断生成的输出是真是假，同时，我们训练网络的其他部分来愚弄鉴别器。对抗训练的目标功能定义如下： </p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629161825452.png" alt="image-20220629161825452"></p>
<p><strong>总损失定义</strong></p>
<p>**<img src="/2022/06/29/review_9_MeInGame/image-20220629161847549.png" alt="image-20220629161847549">    **        </p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629161920596.png" alt="image-20220629161920596"></p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>3DMM</tag>
      </tags>
  </entry>
  <entry>
    <title>RTTO WEEK1</title>
    <url>/2022/09/19/rtto_1/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#week-1"><strong>WEEK 1</strong></a><ul>
<li><a href="#209%E9%95%BF%E5%BA%A6%E6%9C%80%E5%B0%8F%E7%9A%84%E5%AD%90%E6%95%B0%E7%BB%84m">209.长度最小的子数组（m）</a></li>
<li><a href="#904%E6%B0%B4%E6%9E%9C%E6%88%90%E7%AF%AEm">904.水果成篮（m）</a></li>
<li><a href="#76%E6%9C%80%E5%B0%8F%E8%A6%86%E7%9B%96%E5%AD%90%E4%B8%B2h">76.最小覆盖子串（h）</a></li>
<li><a href="#59%E8%9E%BA%E6%97%8B%E7%9F%A9%E9%98%B5m">59.螺旋矩阵（m）</a></li>
<li><a href="#203%E7%A7%BB%E9%99%A4%E9%93%BE%E8%A1%A8%E5%85%83%E7%B4%A0e">203.移除链表元素（e）</a></li>
<li><a href="#707%E8%AE%BE%E8%AE%A1%E9%93%BE%E8%A1%A8m">707.设计链表（m）</a></li>
<li><a href="#206%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8e">206.反转链表（e）</a></li>
<li><a href="#24%E4%B8%A4%E4%B8%A4%E4%BA%A4%E6%8D%A2%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%9A%84%E8%8A%82%E7%82%B9m">24.两两交换链表中的节点（m）</a></li>
<li><a href="#19%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E7%9A%84%E5%80%92%E6%95%B0%E7%AC%ACn%E5%90%84%E7%BB%93%E7%82%B9m">19.删除链表的倒数第N各结点（m）</a></li>
<li><a href="#%E9%9D%A2%E8%AF%95%E9%A2%98-0207-%E9%93%BE%E8%A1%A8%E7%9B%B8%E4%BA%A4e">面试题 02.07. 链表相交（e）</a></li>
<li><a href="#142%E7%8E%AF%E5%BD%A2%E9%93%BE%E8%A1%A8iim">142.环形链表Ⅱ（m）</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<span id="more"></span>

<h1><span id="week-1"><strong>WEEK 1</strong></span></h1><h2><span id="209长度最小的子数组m">209.长度最小的子数组（m）</span></h2><p>用滑动窗口，动态调整窗口的起始位置，不断比较子序列是否符合条件（sum&gt;&#x3D;s）,取当前子序列的长度，取相对较小的。</p>
<h2><span id="904水果成篮m">904.水果成篮（m）</span></h2><p>本题的目标是：得到一个最大子串，其中最多含有两种元素；</p>
<p>用一个hashmap表示fruits[i]类型的水果对应摘了几个；</p>
<p>用滑动窗口先遍历右边界，记录更新过程中的type值的变化；</p>
<p>当hashmap中相应类型水果数量为0时，则type++；</p>
<p>若type&gt;2，则更新左边界，把hashmap中fruits[left]水果的数量–，若该水果在hashmap中的数量为0，则type–；</p>
<p>判断每次窗口的长度取历次满足条件的窗口的最大值，即为最终结果；</p>
<h2><span id="76最小覆盖子串h">76.最小覆盖子串（h）</span></h2><p>用滑动窗口，两个hashmap比较各类字符串出现的频率？不会做</p>
<h2><span id="59螺旋矩阵m">59.螺旋矩阵（m）</span></h2><p>先确定遍历的圈数——n&#x2F;2；</p>
<p>上右下左分别进行左闭右开的遍历，进行四个for循环，改变i和j；</p>
<p>如果n是奇数最里面是一个数而不是一个圈，需要单独赋值count；</p>
<h2><span id="203移除链表元素e">203.移除链表元素（e）</span></h2><p>简单链表数据结构；</p>
<p>头节点被删除的情况：直接tmp保存原head，head &#x3D; head -&gt; next，然后删除tmp；</p>
<p>子节点被删除的情况：开辟一个cur节点，遍历链表元素，若cur -&gt;next 为空则停止遍历（尾节点），删除操作同上；</p>
<h2><span id="707设计链表m">707.设计链表（m）</span></h2><p>简单链表数据结构；</p>
<p>需要注意的点是选择index位置的元素的while终止条件；</p>
<p>判断_size异常输入的条件；</p>
<h2><span id="206反转链表e">206.反转链表（e）</span></h2><p>快慢指针；</p>
<p>一个在前一个在后，使用一个tmp来记录cur节点的后一个节点，以便循环推进，每次使得快指针指向的节点重新指向慢指针指向的节点；</p>
<h2><span id="24两两交换链表中的节点m">24.两两交换链表中的节点（m）</span></h2><p>画图！</p>
<p>三个步骤，要注意先暂存两个值，以及每一步骤都会改变当前链表结构，需要画图一步步看当前链表状态；</p>
<h2><span id="19删除链表的倒数第n各结点m">19.删除链表的倒数第N各结点（m）</span></h2><p>双指针；</p>
<p>让fast移动n步；</p>
<p>然后让fast和slow同时移动，直到fast指向链表末尾；</p>
<p>删掉slow所指向的节点；</p>
<h2><span id="面试题-0207-链表相交e">面试题 02.07. 链表相交（e）</span></h2><p>分别求两链表的长度，计算长度差，先把长链表遍历一段长度差，控制两个链表一样长，然后同时开始遍历两个链表，若遍历到了相同节点则找到了交点。</p>
<h2><span id="142环形链表iim">142.环形链表Ⅱ（m）</span></h2><p>分两步：判断是否有环；若有环判断环的出口；</p>
<p>是否有环：快慢指针，快指针一次移动两个，慢指针一次移动一个，若有环快慢指针会在不空的节点上相遇；</p>
<p>环的出口：从头结点出发一个指针，从相遇节点也出发一个指针，这两个指针每次只走一个节点， 那么当这两个指针相遇的时候就是环形入口的节点（这步有点绕）</p>
]]></content>
      <categories>
        <category>RTTO</category>
      </categories>
      <tags>
        <tag>RTTO</tag>
      </tags>
  </entry>
  <entry>
    <title>RTTO WEEK3</title>
    <url>/2022/09/30/rtto_3/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#week-3"><strong>WEEK 3</strong></a><ul>
<li><a href="#344%E5%8F%8D%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2e">344.反转字符串（e）</a></li>
<li><a href="#541%E5%8F%8D%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2iie">541.反转字符串Ⅱ（e）</a></li>
<li><a href="#%E5%89%91%E6%8C%87offer-05%E6%9B%BF%E6%8D%A2%E7%A9%BA%E6%A0%BCe">剑指Offer 05.替换空格（e）</a></li>
<li><a href="#151%E5%8F%8D%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E7%9A%84%E5%8D%95%E8%AF%8Dm">151.反转字符串中的单词（m）</a></li>
<li><a href="#%E5%89%91%E6%8C%87offer-58-ii%E5%B7%A6%E6%97%8B%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2e">剑指Offer 58 - Ⅱ.左旋转字符串（e）</a></li>
<li><a href="#28%E6%89%BE%E5%87%BA%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8C%B9%E9%85%8D%E9%A1%B9%E7%9A%84%E4%B8%8B%E6%A0%87m">28.找出字符串中第一个匹配项的下标（m）</a></li>
<li><a href="#459%E9%87%8D%E5%A4%8D%E7%9A%84%E5%AD%90%E5%AD%97%E7%AC%A6%E4%B8%B2e">459.重复的子字符串（e）</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<span id="more"></span>

<h1><span id="week-3"><strong>WEEK 3</strong></span></h1><h2><span id="344反转字符串e">344.反转字符串（e）</span></h2><p> 前一半后一半做个swap，没啥好说的。</p>
<h2><span id="541反转字符串iie">541.反转字符串Ⅱ（e）</span></h2><p>每次遍历2k的距离，如果有k个字符（size能容纳当前遍历的k个字符），则reverse前k个，否则，反转全部字符。</p>
<h2><span id="剑指offer-05替换空格e">剑指Offer 05.替换空格（e）</span></h2><p>简单遍历改字符需要进行字符串空间上的扩容；</p>
<p>1.首先遍历一遍原字符串计算出所含空格的个数；</p>
<p>2.resize原字符串，+2*count；</p>
<p>3.使用双指针法，j指向原字符串最后一位，i指向扩充后字符串最后一位，同时向前遍历新字符串；若碰到非空位置，则更新非空字符在新字符串中的位置；若碰到空位置，此时的i即对应新字符串中0的位置，前一位为2，前前位为%；</p>
<h2><span id="151反转字符串中的单词m">151.反转字符串中的单词（m）</span></h2><p>两个关键操作：字符串逆置reverse；去除多余空格，保证两个单词之间仅存在一个空格；</p>
<p>1.将原字符串去除多余空格；</p>
<p>2.将去重后的原字符串整个逆置；</p>
<p>3.遍历逆置后的字符串，碰到空格则将空格前的字符逆置；</p>
<h2><span id="剑指offer-58-ii左旋转字符串e">剑指Offer 58 - Ⅱ.左旋转字符串（e）</span></h2><p>默认不开辟额外空间；</p>
<p>reverse前n个；</p>
<p>reverse剩下的；</p>
<p>reverse全部的；</p>
<h2><span id="28找出字符串中第一个匹配项的下标m">28.找出字符串中第一个匹配项的下标（m）</span></h2><p>KMP算法</p>
<p><a href="https://programmercarl.com/0028.%E5%AE%9E%E7%8E%B0strStr.html">代码随想录 (programmercarl.com)</a></p>
<h2><span id="459重复的子字符串e">459.重复的子字符串（e）</span></h2><p>解法一：移动匹配</p>
<p>任何一个重复子字符串构成的字符串，其前半部分和后半部分都是相等的；</p>
<p>那么s+s中就一定存在一个新s；</p>
<p>掐头去尾（erase去首尾字母），这样才能找到拼接而成的新s，而不是原本头尾的老s；</p>
<p>若存在新s，则返回true，否则false；</p>
<p>解法二：KMP法</p>
<p>核心：最长相等前后缀不包含的子串就是最小重复子串</p>
]]></content>
      <categories>
        <category>RTTO</category>
      </categories>
      <tags>
        <tag>RTTO</tag>
      </tags>
  </entry>
  <entry>
    <title>RTTO WEEK4</title>
    <url>/2022/10/19/rtto_4/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#week-4"><strong>WEEK 4</strong></a><ul>
<li><a href="#27%E7%A7%BB%E9%99%A4%E5%85%83%E7%B4%A0e">27.移除元素（e）</a></li>
<li><a href="#344%E5%8F%8D%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2e">344.反转字符串（e）</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<span id="more"></span>

<h1><span id="week-4"><strong>WEEK 4</strong></span></h1><h2><span id="27移除元素e">27.移除元素（e）</span></h2><p>slow用于赋值符合条件的fast位置数据，fast碰到不与val相等的则赋值，碰到与val相等的则++</p>
<h2><span id="344反转字符串e">344.反转字符串（e）</span></h2><p>上周做过了</p>
]]></content>
      <categories>
        <category>RTTO</category>
      </categories>
      <tags>
        <tag>RTTO</tag>
      </tags>
  </entry>
  <entry>
    <title>RTTO WEEK2</title>
    <url>/2022/09/30/rtto_2/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#week-2"><strong>WEEK 2</strong></a><ul>
<li><a href="#242%E6%9C%89%E6%95%88%E7%9A%84%E5%AD%97%E6%AF%8D%E5%BC%82%E4%BD%8D%E8%AF%8De">242.有效的字母异位词（e）</a></li>
<li><a href="#383%E8%B5%8E%E9%87%91%E4%BF%A1e">383.赎金信（e）</a></li>
<li><a href="#49%E5%AD%97%E6%AF%8D%E5%BC%82%E4%BD%8D%E8%AF%8D%E5%88%86%E7%BB%84m">49.字母异位词分组（m）</a></li>
<li><a href="#438%E6%89%BE%E5%88%B0%E5%AD%97%E7%AC%A6%E4%B8%B2%E4%B8%AD%E6%89%80%E6%9C%89%E5%AD%97%E6%AF%8D%E5%BC%82%E4%BD%8D%E8%AF%8Dm">438.找到字符串中所有字母异位词（m）</a></li>
<li><a href="#349%E4%B8%A4%E4%B8%AA%E6%95%B0%E7%BB%84%E7%9A%84%E4%BA%A4%E9%9B%86e">349.两个数组的交集（e）</a></li>
<li><a href="#350%E4%B8%A4%E4%B8%AA%E6%95%B0%E7%BB%84%E7%9A%84%E4%BA%A4%E9%9B%86iie">350.两个数组的交集Ⅱ（e）</a></li>
<li><a href="#202%E5%BF%AB%E4%B9%90%E6%95%B0e">202.快乐数（e）</a></li>
<li><a href="#1%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8Ce">1.两数之和（e）</a></li>
<li><a href="#454%E5%9B%9B%E6%95%B0%E7%9B%B8%E5%8A%A0iim">454.四数相加Ⅱ（m）</a></li>
<li><a href="#15%E4%B8%89%E6%95%B0%E4%B9%8B%E5%92%8Cm">15.三数之和（m）</a></li>
<li><a href="#18%E5%9B%9B%E6%95%B0%E4%B9%8B%E5%92%8Cm">18.四数之和（m）</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<span id="more"></span>

<h1><span id="week-2"><strong>WEEK 2</strong></span></h1><h2><span id="242有效的字母异位词e">242.有效的字母异位词（e）</span></h2><p>开辟一个record数组size为26；</p>
<p>首先遍历s，把字母在相应位置进行累加计数；</p>
<p>其次遍历t，把字母在相应位置进行累减计数；</p>
<p>最后遍历record数组，若有元素不为0，则返回false；否则返回true；</p>
<h2><span id="383赎金信e">383.赎金信（e）</span></h2><p>同上，最后判断record数组中是否有-1；</p>
<h2><span id="49字母异位词分组m">49.字母异位词分组（m）</span></h2><p>该题目标为，给定一个字符串的list，需要把由相同字母构成的字符串自行分组。</p>
<p>首先遍历整个字符串列表，再将每个字符串自行排序；</p>
<p>将经过排序后的字符串放入一个哈希表，key为排序后的字符串，value为排序前的原字符串；</p>
<p>这样拥有相同字符的字符串会共享同一个key，只需取出相同key的vector；</p>
<p><code>vector&lt;vector&lt;string&gt;&gt;</code>二维string数组</p>
<p><code>unordered_map&lt;string,vector&lt;string&gt;&gt; map</code>一个无序hashmap，key是string，用于存放正序的素材字符串；value是string list，用于存放key字母构成的字符串</p>
<h2><span id="438找到字符串中所有字母异位词m">438.找到字符串中所有字母异位词（m）</span></h2><p>构造sCount和pCount两个哈希表（size&#x3D;26）用于存储每个字母分别在s和t中出现的次数；</p>
<p>保证pLen长度的窗口进行滑动，判断sCount和pCount是否相等；</p>
<p>两个string先遍历pLen的长度，更新sCount和pCount，若此时sCount &#x3D;&#x3D; pCount，则push_back(0)；</p>
<p>再从pLen+1个元素开始遍历s，减少滑动窗口第一个元素的出现次数，增加滑动窗口最后一个元素（新遍历的元素）的出现次数，若遍历过程中sCount &#x3D;&#x3D; pCount，则push_back(i+1)；</p>
<h2><span id="349两个数组的交集e">349.两个数组的交集（e）</span></h2><p>unordered_set 底层哈希表实现；</p>
<p><code>unordered_set&lt;int&gt; nums_set(nums1.begin(),nums1.end());</code>复制nums1</p>
<p>unordered_map常用操作详解：<a href="https://blog.csdn.net/weixin_45847364/article/details/121654719">(70条消息) unordered_set中end()与find()的使用_bulangman277的博客-CSDN博客</a></p>
<p>需要注意的是，使用了一个result_set用于最后结果的去重，最后再从set中取元素到vector中返回；</p>
<h2><span id="350两个数组的交集iie">350.两个数组的交集Ⅱ（e）</span></h2><p>数据范围合理的情况下可使用hash；</p>
<p>先构建两个hashmap来存放每种元素的出现次数；</p>
<p>最后检索两个构建好的hashmap，取同一元素出现次数的最小值m，添加m次该元素到最后的结果上；</p>
<h2><span id="202快乐数e">202.快乐数（e）</span></h2><p>由于该过程中求和的过程会重复出现，可单独封装一个getSum()方法；</p>
<p>使用哈希法，来判断这个sum是否重复出现，如果重复了就是return false， 否则一直找到sum为1为止；</p>
<p>设置一个sum_set来记录出现过的sum值；</p>
<p>每次getSum进行判断，如果这个sum曾经出现过，说明已经陷入了无限循环了，立刻return false；</p>
<h2><span id="1两数之和e">1.两数之和（e）</span></h2><p>使用unordered_map数据结构，key为nums[i]，value为i；</p>
<p>遍历一遍数组，寻找target-num[i]是否在map中；若有返回该map的索引和当前i，若没有，则在map中更新对应的key value；</p>
<h2><span id="454四数相加iim">454.四数相加Ⅱ（m）</span></h2><p>首先定义 一个unordered_map，key放a和b两数之和，value 放a和b两数之和出现的次数；</p>
<p>遍历大A和大B数组，统计两个数组元素之和，和出现的次数，放到map中；</p>
<p>定义int变量count，用来统计 a+b+c+d &#x3D; 0 出现的次数；</p>
<p>在遍历大C和大D数组，找到如果 0-(c+d) 在map中出现过的话，就用count把map中key对应的value也就是出现次数统计出来；</p>
<p>最后返回统计值 count 就可以了；</p>
<h2><span id="15三数之和m">15.三数之和（m）</span></h2><p>本题如果使用哈希会有许多去重的操作，不好写。</p>
<p>使用双指针法：</p>
<p>先将数组进行排序；</p>
<p>从头遍历一个数组选择第一个数i；left指针指向i后一位的数，right指针指向数组最后一位的数；</p>
<p>若i+left+right&gt;0，则需要将三数之和缩小，此时需要将right向左移动一位；</p>
<p>若i+left+right&lt;0，则需要将三数之和放大，此时需要将left向右移动一位；</p>
<p>直到三数之和等于0，然后对所有的组合进行去重；</p>
<p>最关键的是最后的去重！</p>
<p>i，left，right三种重复的情况</p>
<p>i重复的话判断相邻两位是否相等，若等于直接continue循环，进行i++；</p>
<p>left重复的话，比较left和left+1，直接left++；</p>
<p>right重复的话，比较right和right-1，直接right–；</p>
<h2><span id="18四数之和m">18.四数之和（m）</span></h2><p>核心思想和三数之和一样，都是双指针遍历；难的是去重和剪枝；</p>
]]></content>
      <categories>
        <category>RTTO</category>
      </categories>
      <tags>
        <tag>RTTO</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo建站相关</title>
    <url>/2022/03/25/tools_1_start/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E6%90%AD%E5%BB%BAhexo%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%80%9A%E8%BF%87git%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2%E5%9C%A8%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8">搭建Hexo服务器，通过Git自动部署在阿里云服务器</a><ul>
<li><a href="#%E6%90%AD%E5%BB%BA%E5%8F%8A%E9%83%A8%E7%BD%B2">搭建及部署</a></li>
<li><a href="#%E5%8D%9A%E6%96%87%E5%8F%91%E5%B8%83">博文发布</a></li>
</ul>
</li>
<li><a href="#%E4%B8%BB%E9%A2%98%E5%8F%8A%E5%85%B6%E7%BE%8E%E5%8C%96">主题及其美化</a><ul>
<li><a href="#butterfly">Butterfly</a></li>
<li><a href="#next">Next</a><ul>
<li><a href="#%E7%BE%8E%E5%8C%96">美化</a></li>
<li><a href="#%E6%96%87%E7%AB%A0%E5%90%AF%E7%94%A8tags%E5%92%8Ccategories">文章启用tags和categories</a></li>
<li><a href="#%E8%AE%BE%E7%BD%AE%E9%98%85%E8%AF%BB%E5%85%A8%E6%96%87">设置阅读全文</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98updating">相关问题（Updating…）</a><ul>
<li><a href="#%E6%97%A0%E6%B3%95%E6%AD%A3%E7%A1%AE%E6%B8%B2%E6%9F%93markdown%E6%96%87%E4%BB%B6%E9%97%AE%E9%A2%98">无法正确渲染markdown文件问题</a></li>
<li><a href="#%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98">插入图片问题</a></li>
<li><a href="#%E7%9B%AE%E5%BD%95%E7%94%9F%E6%88%90%E9%97%AE%E9%A2%98">目录生成问题</a></li>
<li><a href="#%E6%B7%BB%E5%8A%A0%E6%9C%AC%E5%9C%B0%E6%90%9C%E7%B4%A2%E5%8A%9F%E8%83%BD">添加本地搜索功能</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<span id="more"></span>

<h1><span id="搭建hexo服务器通过git自动部署在阿里云服务器">搭建Hexo服务器，通过Git自动部署在阿里云服务器</span></h1><h2><span id="搭建及部署">搭建及部署</span></h2><p>参考博文链接：<a href="https://mp.weixin.qq.com/s/JTTUYJTvtdT6X2fvLUBFZg">Win10下Hexo博客搭建教程，及阿里云服务器部署实战 (qq.com)</a></p>
<h2><span id="博文发布">博文发布</span></h2><p>在本地机器上部署Hexo相关MyHexoBlogs文件夹下，进入MyHexoBlogs&#x2F;myblogs&#x2F;source&#x2F;_posts目录；</p>
<p>在当前页进入git bash，输入hexo clean 清除缓存，hexo g 解析静态文件，hexo d 刷新部署新的资源。</p>
<p>hexo cl &amp;&amp; hexo g &amp;&amp; hexo d</p>
<h1><span id="主题及其美化">主题及其美化</span></h1><h2><span id="butterfly">Butterfly</span></h2><p>Butterfly主题部署参考博文链接：<a href="https://www.jianshu.com/p/50a565adaf15?ivk_sa=1024320u">hexo框架|butterfly主题配置 - 简书 (jianshu.com)</a></p>
<p>Butterfly主题官方文档：<a href="https://www.butterfly1.cn/">Hexo-Butterfly主题(🦋 A Hexo Theme: Butterfly-Official website) (butterfly1.cn)</a></p>
<p>Hexo默认美化项：<a href="https://zhuanlan.zhihu.com/p/369951111">Hexo个性化设置 - 知乎 (zhihu.com)</a></p>
<h2><span id="next">Next</span></h2><h3><span id="美化">美化</span></h3><p><a href="https://blog.csdn.net/qq_34003239/article/details/100883213">(54条消息) Next主题美化_蜗牛非牛的博客-CSDN博客_next主题美化</a></p>
<h3><span id="文章启用tags和categories">文章启用tags和categories</span></h3><p><a href="https://blog.csdn.net/Lancis/article/details/118788205">(54条消息) hexo next主题简单美化_Lancis的博客-CSDN博客_next主题美化</a></p>
<h3><span id="设置阅读全文">设置阅读全文</span></h3><p><a href="https://blog.csdn.net/CHENGXUYUAN09/article/details/103408380">(54条消息) next7.6版本关于设置阅读全文_LIYUANWAISPRING的博客-CSDN博客</a></p>
<h1><span id="相关问题updating">相关问题（Updating…）</span></h1><h2><span id="无法正确渲染markdown文件问题">无法正确渲染markdown文件问题</span></h2><p>hexo对于md文件的解析规则不是标准规则，有时候在复制论文标题时会有换行符无法正确解析；</p>
<p>重新手动输入检查标题格式解决问题</p>
<h2><span id="插入图片问题">插入图片问题</span></h2><p>选择采用hexo官网的解决方式：<a href="https://hexo.io/zh-cn/docs/asset-folders">资源文件夹 | Hexo</a></p>
<h2><span id="目录生成问题">目录生成问题</span></h2><p>安装插件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cnpm install hexo-toc --save</span><br></pre></td></tr></table></figure>

<p>hexo的配置文件中设置格式，添加配置代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">toc:</span><br><span class="line">  maxdepth: 3</span><br></pre></td></tr></table></figure>

<p>在md中使用时</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!-- toc --&gt;</span><br></pre></td></tr></table></figure>

<h2><span id="添加本地搜索功能">添加本地搜索功能</span></h2><p>与Next官方配置文件中的链接步骤不同的是，还需要修改</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">preload: true</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker学习笔记</title>
    <url>/2022/09/29/tech_1_docker/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#docker-hello-world">Docker Hello World</a><ul>
<li><a href="#%E8%BF%90%E8%A1%8C%E4%BA%A4%E4%BA%92%E5%BC%8F%E7%9A%84%E5%AE%B9%E5%99%A8">运行交互式的容器</a></li>
<li><a href="#%E5%90%AF%E5%8A%A8%E5%AE%B9%E5%99%A8%E5%90%8E%E5%8F%B0%E6%A8%A1%E5%BC%8F">启动容器（后台模式）</a></li>
</ul>
</li>
<li><a href="#docker%E5%AE%B9%E5%99%A8%E4%BD%BF%E7%94%A8">Docker容器使用</a><ul>
<li><a href="#docker%E5%AE%A2%E6%88%B7%E7%AB%AF">Docker客户端</a></li>
<li><a href="#%E5%AE%B9%E5%99%A8%E4%BD%BF%E7%94%A8">容器使用</a><ul>
<li><a href="#%E6%8D%A2%E9%BB%98%E8%AE%A4%E6%BA%90">换默认源</a></li>
<li><a href="#%E8%8E%B7%E5%8F%96%E9%95%9C%E5%83%8F">获取镜像</a></li>
<li><a href="#%E5%90%AF%E5%8A%A8%E5%AE%B9%E5%99%A8">启动容器</a></li>
<li><a href="#%E5%90%AF%E5%8A%A8%E5%B7%B2%E5%81%9C%E6%AD%A2%E8%BF%90%E8%A1%8C%E7%9A%84%E5%AE%B9%E5%99%A8">启动已停止运行的容器</a></li>
<li><a href="#%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C">后台运行</a></li>
<li><a href="#%E8%BF%9B%E5%85%A5%E5%AE%B9%E5%99%A8">进入容器</a></li>
<li><a href="#%E5%AF%BC%E5%87%BA%E5%92%8C%E5%AF%BC%E5%85%A5%E5%AE%B9%E5%99%A8">导出和导入容器</a></li>
<li><a href="#%E5%88%A0%E9%99%A4%E5%AE%B9%E5%99%A8">删除容器</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#docker%E9%95%9C%E5%83%8F%E4%BD%BF%E7%94%A8">Docker镜像使用</a><ul>
<li><a href="#%E5%88%97%E5%87%BA%E9%95%9C%E5%83%8F%E5%88%97%E8%A1%A8">列出镜像列表</a></li>
<li><a href="#%E8%8E%B7%E5%8F%96%E4%B8%80%E4%B8%AA%E6%96%B0%E7%9A%84%E9%95%9C%E5%83%8F">获取一个新的镜像</a></li>
<li><a href="#%E6%9F%A5%E6%89%BE%E9%95%9C%E5%83%8F">查找镜像</a></li>
<li><a href="#%E6%8B%96%E5%8F%96%E9%95%9C%E5%83%8F">拖取镜像</a></li>
<li><a href="#%E5%88%A0%E9%99%A4%E9%95%9C%E5%83%8F">删除镜像</a></li>
<li><a href="#%E5%88%9B%E5%BB%BA%E9%95%9C%E5%83%8F">创建镜像</a><ul>
<li><a href="#%E6%9B%B4%E6%96%B0%E9%95%9C%E5%83%8F">更新镜像</a></li>
<li><a href="#%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F">构建镜像</a></li>
<li><a href="#%E8%AE%BE%E7%BD%AE%E9%95%9C%E5%83%8F%E6%A0%87%E7%AD%BE">设置镜像标签</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#docker%E4%BB%93%E5%BA%93%E7%AE%A1%E7%90%86%E4%BB%A5docker-hub%E4%B8%BA%E4%BE%8B">Docker仓库管理（以Docker Hub为例）</a><ul>
<li><a href="#docker-hub">Docker Hub</a><ul>
<li><a href="#%E6%B3%A8%E5%86%8C">注册</a></li>
<li><a href="#%E7%99%BB%E5%BD%95%E5%92%8C%E9%80%80%E5%87%BA">登录和退出</a></li>
<li><a href="#%E9%80%80%E5%87%BA"><strong>退出</strong></a></li>
<li><a href="#%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F">拉取镜像</a></li>
<li><a href="#%E6%8E%A8%E9%80%81%E9%95%9C%E5%83%8F">推送镜像</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#dockerfile">Dockerfile</a></li>
</ul>
<!-- tocstop -->

<span id="more"></span>

<h1><span id="docker-hello-world">Docker Hello World</span></h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run ubuntu:20.03 /bin/echo &quot;Hello world&quot;</span><br></pre></td></tr></table></figure>

<p>Docker 以 ubuntu15.10 镜像创建一个新容器，然后在容器里执行 bin&#x2F;echo “Hello world”，然后输出结果。</p>
<p>（需要sudo权限）</p>
<h2><span id="运行交互式的容器">运行交互式的容器</span></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -i -t ubuntu:15.10 /bin/bash</span><br></pre></td></tr></table></figure>

<p>通过 docker 的两个参数 -i -t，让 docker 运行的容器实现**”对话”**的能力（进入容器的终端）</p>
<p><img src="/2022/09/29/tech_1_docker/image-20220929141727968.png" alt="image-20220929141727968"></p>
<p>如需退出容器：输入exit或者使用CTRL+D</p>
<h2><span id="启动容器后台模式">启动容器（后台模式）</span></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d ubuntu:20.03 /bin/sh -c &quot;while true; do echo hello world; sleep 1; done&quot;</span><br></pre></td></tr></table></figure>

<p><img src="/2022/09/29/tech_1_docker/image-20220929143428371.png" alt="image-20220929143428371"></p>
<p>会出现一个长字符串，该字符串为容器的id</p>
<p>查看当前容器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker ps</span><br></pre></td></tr></table></figure>

<p>查看容器内的标准输出</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker logs </span><br></pre></td></tr></table></figure>

<p><img src="/2022/09/29/tech_1_docker/image-20220929144110987.png" alt="image-20220929144110987"></p>
<p>停止容器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker stop</span><br></pre></td></tr></table></figure>

<p><img src="/2022/09/29/tech_1_docker/image-20220929144204323.png" alt="image-20220929144204323"></p>
<h1><span id="docker容器使用">Docker容器使用</span></h1><h2><span id="docker客户端">Docker客户端</span></h2><p>查看docker客户端所有命令选项</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker</span><br></pre></td></tr></table></figure>

<h2><span id="容器使用">容器使用</span></h2><h3><span id="换默认源">换默认源</span></h3><p><a href="https://huaweicloud.csdn.net/63311a8dd3efff3090b52222.html">解决docker拉取镜像慢的方法（亲测有效）_docker_阿正的梦工坊-DevPress官方社区 (csdn.net)</a></p>
<h3><span id="获取镜像">获取镜像</span></h3><p>如果我们本地没有 ubuntu 镜像，我们可以使用 docker pull 命令来载入 ubuntu 镜像</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker pull ubuntu</span><br></pre></td></tr></table></figure>

<h3><span id="启动容器">启动容器</span></h3><p>以命令行模式启动一个容器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -it ubuntu /bin/bash</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<ul>
<li><strong>-i</strong>: 交互式操作。</li>
<li><strong>-t</strong>: 终端。</li>
<li><strong>ubuntu</strong>: ubuntu 镜像。</li>
<li><strong>&#x2F;bin&#x2F;bash</strong>：放在镜像名后的是命令，这里希望有个交互式 Shell，因此用的是 &#x2F;bin&#x2F;bash。</li>
</ul>
<h3><span id="启动已停止运行的容器">启动已停止运行的容器</span></h3><p>查看所有的容器命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker ps -a</span><br></pre></td></tr></table></figure>

<p>启动一个容器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker start &lt;容器 ID&gt;</span><br></pre></td></tr></table></figure>

<h3><span id="后台运行">后台运行</span></h3><p>在大部分的场景下，我们希望 docker 的服务是在后台运行的，我们可以过 <strong>-d</strong> 指定容器的运行模式。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd --name ubuntu-test ubuntu /bin/bash</span><br></pre></td></tr></table></figure>

<p><strong>-d</strong>只是指定容器的运行模式，而不会默认进入容器</p>
<p><strong>–name</strong>给容器命名（此处为ubuntu-test）</p>
<p>停止的容器可以通过 docker restart 重启：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker restart &lt;容器 ID&gt;</span><br></pre></td></tr></table></figure>

<h3><span id="进入容器">进入容器</span></h3><p>在使用 <strong>-d</strong> 参数时，容器启动后会进入后台。此时想要进入容器，可以通过以下指令进入：</p>
<ul>
<li><strong>docker attach</strong></li>
<li><strong>docker exec</strong>：推荐大家使用 docker exec 命令，因为此命令会退出容器终端，但不会导致容器的停止。</li>
</ul>
<h4><span id="attach命令">attach命令</span></h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker attach &lt;容器 ID&gt;</span><br></pre></td></tr></table></figure>

<h4><span id="exec命令">exec命令</span></h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker exec -it &lt;容器 ID&gt; /bin/bash</span><br></pre></td></tr></table></figure>

<p>区别在于attach命令如果退出容器，容器会停止；exec不会；</p>
<p><img src="/2022/09/29/tech_1_docker/image-20220929155456079.png" alt="image-20220929155456079"></p>
<h3><span id="导出和导入容器">导出和导入容器</span></h3><p><strong>导出容器</strong></p>
<p>导出本地某个容器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker export &lt;容器 ID&gt; &gt; ubuntu.tar</span><br></pre></td></tr></table></figure>

<p><strong>导入容器快照</strong></p>
<p>使用 docker import 从容器快照文件中再导入为镜像，以下实例将快照文件 ubuntu.tar 导入到镜像 test&#x2F;ubuntu:v1:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat docker/ubuntu.tar | docker import - test/ubuntu:v1</span><br></pre></td></tr></table></figure>

<p>此外，也可以通过指定 URL 或者某个目录来导入，例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker import http://example.com/exampleimage.tgz example/imagerepo</span><br></pre></td></tr></table></figure>

<h3><span id="删除容器">删除容器</span></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker rm -f &lt;容器 ID&gt;</span><br></pre></td></tr></table></figure>

<h1><span id="docker镜像使用">Docker镜像使用</span></h1><p>当运行容器时，使用的镜像如果在本地中不存在，docker 就会自动从 docker 镜像仓库中下载，默认是从 Docker Hub 公共镜像源下载。</p>
<h2><span id="列出镜像列表">列出镜像列表</span></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker images</span><br></pre></td></tr></table></figure>

<p><img src="/2022/09/29/tech_1_docker/image-20220929162540821.png" alt="image-20220929162540821"></p>
<p>各个选项说明:</p>
<ul>
<li><strong>REPOSITORY：</strong>表示镜像的仓库源</li>
<li><strong>TAG：</strong>镜像的标签</li>
<li><strong>IMAGE ID：</strong>镜像ID</li>
<li><strong>CREATED：</strong>镜像创建时间</li>
<li><strong>SIZE：</strong>镜像大小</li>
</ul>
<p>如果你不指定一个镜像的版本标签，例如你只使用 ubuntu，docker 将默认使用 ubuntu:latest 镜像。</p>
<h2><span id="获取一个新的镜像">获取一个新的镜像</span></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker pull ubuntu:18.04</span><br></pre></td></tr></table></figure>

<h2><span id="查找镜像">查找镜像</span></h2><p>可以从 Docker Hub 网站来搜索镜像，Docker Hub 网址为： **<a href="https://hub.docker.com/**%EF%BC%9B%E6%88%91%E4%BB%AC%E4%B9%9F%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8">https://hub.docker.com/**；我们也可以使用</a> docker search 命令来搜索镜像。比如我们需要一个 httpd 的镜像来作为我们的 web 服务。我们可以通过 docker search 命令搜索 httpd 来寻找适合我们的镜像。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker search httpd</span><br></pre></td></tr></table></figure>

<p><img src="/2022/09/29/tech_1_docker/image-20220929162948690.png" alt="image-20220929162948690"></p>
<h2><span id="拖取镜像">拖取镜像</span></h2><p>我们决定使用上图中的 httpd 官方版本的镜像，使用命令 docker pull 来下载镜像。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker pull httpd</span><br></pre></td></tr></table></figure>

<h2><span id="删除镜像">删除镜像</span></h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker rmi hello-world</span><br></pre></td></tr></table></figure>

<p>首先需要先把相关依赖的容器删除，才能进而删除镜像</p>
<h2><span id="创建镜像">创建镜像</span></h2><p>当我们从 docker 镜像仓库中下载的镜像不能满足我们的需求时，我们可以通过以下两种方式对镜像进行更改。</p>
<ul>
<li>1、从已经创建的容器中更新镜像，并且提交这个镜像</li>
<li>2、使用 Dockerfile 指令来创建一个新的镜像</li>
</ul>
<h3><span id="更新镜像">更新镜像</span></h3><p>更新镜像之前，我们需要使用镜像来创建一个容器。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -t -i ubuntu:15.10 /bin/bash</span><br></pre></td></tr></table></figure>

<p>在容器内进行更新</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apt-get update</span><br></pre></td></tr></table></figure>

<p>退出容器后</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker commit -m=&quot;has update&quot; -a=&quot;runoob&quot; &lt;容器 ID&gt; runoob/ubuntu:v2</span><br></pre></td></tr></table></figure>

<p>各个参数说明：</p>
<ul>
<li><strong>-m:</strong> 提交的描述信息</li>
<li><strong>-a:</strong> 指定镜像作者</li>
<li><strong>runoob&#x2F;ubuntu:v2:</strong> 指定要创建的目标镜像名</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker images</span><br></pre></td></tr></table></figure>

<p><img src="/2022/09/29/tech_1_docker/image-20220929164503769.png" alt="image-20220929164503769"></p>
<h3><span id="构建镜像">构建镜像</span></h3><p>我们使用命令 <strong>docker build</strong> ， 从零开始来创建一个新的镜像。为此，我们需要创建一个 Dockerfile 文件，其中包含一组指令来告诉 Docker 如何构建我们的镜像。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat Dockerfile </span><br><span class="line"></span><br><span class="line">FROM    centos:6.7</span><br><span class="line">MAINTAINER      Fisher &quot;fisher@sudops.com&quot;</span><br><span class="line"></span><br><span class="line">RUN     /bin/echo &#x27;root:123456&#x27; |chpasswd</span><br><span class="line">RUN     useradd runoob</span><br><span class="line">RUN     /bin/echo &#x27;runoob:123456&#x27; |chpasswd</span><br><span class="line">RUN     /bin/echo -e &quot;LANG=\&quot;en_US.UTF-8\&quot;&quot; &gt;/etc/default/local</span><br><span class="line">EXPOSE  22</span><br><span class="line">EXPOSE  80</span><br><span class="line">CMD     /usr/sbin/sshd -D</span><br></pre></td></tr></table></figure>

<p>每一个指令都会在镜像上创建一个新的层，每一个指令的前缀都必须是大写的。</p>
<p>第一条FROM，指定使用哪个镜像源</p>
<p>RUN 指令告诉docker 在镜像内执行命令，安装了什么。。。</p>
<p>然后，我们使用 Dockerfile 文件，通过 docker build 命令来构建一个镜像。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker build -t runoob/centos:6.7 .</span><br></pre></td></tr></table></figure>

<p>参数说明：</p>
<ul>
<li><strong>-t</strong> ：指定要创建的目标镜像名</li>
<li><strong>.</strong> ：Dockerfile 文件所在目录，可以指定Dockerfile 的绝对路径</li>
</ul>
<p>我们可以使用新的镜像来创建容器</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -t -i runoob/centos:6.7  /bin/bash</span><br></pre></td></tr></table></figure>

<h3><span id="设置镜像标签">设置镜像标签</span></h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker tag 860c279d2fec runoob/centos:dev</span><br></pre></td></tr></table></figure>

<h1><span id="docker仓库管理以docker-hub为例">Docker仓库管理（以Docker Hub为例）</span></h1><h2><span id="docker-hub">Docker Hub</span></h2><h3><span id="注册">注册</span></h3><p>在 <a href="https://hub.docker.com/">https://hub.docker.com</a> 免费注册一个 Docker 账号。</p>
<h3><span id="登录和退出">登录和退出</span></h3><p>登录需要输入用户名和密码，登录成功后，我们就可以从 docker hub 上拉取自己账号下的全部镜像。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker login</span><br></pre></td></tr></table></figure>

<h3><span id="退出"><strong>退出</strong></span></h3><p>退出 docker hub 可以使用以下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker logout</span><br></pre></td></tr></table></figure>

<h3><span id="拉取镜像">拉取镜像</span></h3><p>通过docker search查找官方仓库的镜像，docker pull来拉取</p>
<h3><span id="推送镜像">推送镜像</span></h3><p>用户登录后，可以通过 docker push 命令将自己的镜像推送到 Docker Hub。</p>
<p>以下命令中的 username 请替换为你的 Docker 账号用户名。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker tag ubuntu:18.04 username/ubuntu:18.04</span><br></pre></td></tr></table></figure>

<h1><span id="dockerfile">Dockerfile</span></h1><ul>
<li>FROM</li>
</ul>
<p>构建镜像基于哪个镜像</p>
<ul>
<li>MAINTAINER</li>
</ul>
<p>镜像维护者姓名或邮箱地址</p>
<ul>
<li>RUN</li>
</ul>
<p>构建镜像时运行的指令</p>
<ul>
<li>CMD</li>
</ul>
<p>运行容器时执行的shell环境</p>
<ul>
<li>VOLUME</li>
</ul>
<p>指定容器挂载点到宿主机自动生成的目录或其他容器</p>
<ul>
<li>USER</li>
</ul>
<p>为RUN、CMD、和 ENTRYPOINT 执行命令指定运行用户</p>
<ul>
<li>WORKDIR</li>
</ul>
<p>为 RUN、CMD、ENTRYPOINT、COPY 和 ADD 设置工作目录，就是切换目录</p>
<ul>
<li>HEALTHCHECH</li>
</ul>
<p>健康检查</p>
<ul>
<li>ARG</li>
</ul>
<p>构建时指定的一些参数</p>
<ul>
<li>EXPOSE</li>
</ul>
<p>声明容器的服务端口（仅仅是声明）</p>
<ul>
<li>ENV</li>
</ul>
<p>设置容器环境变量</p>
<ul>
<li>ADD</li>
</ul>
<p>拷贝文件或目录到容器中，如果是URL或压缩包便会自动下载或自动解压</p>
<ul>
<li>COPY</li>
</ul>
<p>拷贝文件或目录到容器中，跟ADD类似，但不具备自动下载或解压的功能</p>
<ul>
<li>ENTRYPOINT</li>
</ul>
<p>运行容器时执行的shell命令</p>
]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
</search>
