<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>《Face-to-Parameter Translation for Game Character Auto-Creation》</title>
    <url>/2022/07/24/review_10_F2P/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E6%96%B9%E6%B3%95%E7%AE%80%E8%BF%B0">方法简述</a><ul>
<li><a href="#%E8%B4%A1%E7%8C%AE">贡献</a></li>
</ul>
</li>
<li><a href="#%E6%96%B9%E6%B3%95%E8%AF%A6%E8%A7%A3">方法详解</a><ul>
<li><a href="#1imitator">1.Imitator</a></li>
<li><a href="#2facial-similarity-measurement">2.Facial Similarity Measurement</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Shi_Face-to-Parameter_Translation_for_Game_Character_Auto-Creation_ICCV_2019_paper.pdf">Face-to-Parameter Translation for Game Character Auto-Creation (thecvf.com)</a></p>
<p>code: no code</p>
<p>reference link: <a href="https://zhuanlan.zhihu.com/p/105037127">【1】伏羲AI lab：脸部照片到脸部参数的游戏角色自动生成 论文笔记 - 知乎 (zhihu.com)</a></p>
<span id="more"></span>

<h1><span id="出发点">出发点</span></h1><p>RPG游戏中捏脸的标准工作流是从配置大量<strong>面部参数</strong>开始，然后，游戏引擎将这些用户指定的参数作为输入，并生成3D人脸。也可以说游戏人脸定制是一种3DMM的特例或是一种风格迁移问题。</p>
<p>但遗憾的是，上述方法不能直接应用于游戏环境中。原因有三。首先，这些方法并不是用于生成参数化的角色，而这对于大多数游戏引擎来说是必不可少的，因为它们通常会<strong>接受游戏角色的定制参数</strong>，而不是图像或3D网格。其次，这些方法对用户交互不友好，因为大多数用户很难直接编辑Mesh。最后，给定一组用户指定参数的游戏引擎的渲染过程是不可微分的，这进一步限制了深度学习方法在游戏环境中的适用性。</p>
<h1><span id="方法简述">方法简述</span></h1><p>针对上述问题，本文提出了一种根据玩家输入的人脸照片自动生成游戏角色的方法，如图所示。</p>
<p><img src="/2022/07/24/review_10_F2P/image-20220724165007254.png" alt="image-20220724165007254"></p>
<p>与以往3DMM方法不同，我们的方法通过<strong>预测一组具有明确物理意义的面部参数，为骨骼驱动的模型创建三维轮廓</strong>。在我们的方法中，每个参数控制每个面部组件的一个单独属性，包括位置、方向和比例。更重要的是，我们的方法在创建结果的基础上支持额外的用户交互，玩家可以根据自己的需求进一步改进他们的形象。由于游戏引擎的渲染过程是不可微分的，我们设计了一个生成网络作为“模仿者”来模仿游戏引擎的物理行为，从而可以在神经风格传递框架下实现本文提出的方法，并使用梯度下降法优化面部参数。</p>
<h2><span id="贡献">贡献</span></h2><p>1)我们提出了一种端到端的人脸参数转换和游戏角色自动生成方法。据我们所知，关于这一主题的研究工作很少。</p>
<p>2)由于游戏引擎的渲染过程是不可区分的，我们引入了一个模仿者，通过构建一个深度生成网络来模仿游戏引擎的行为。这样，梯度可以平滑地反向传播到输入，从而利用梯度下降法更新人脸参数。</p>
<p>3)为跨域人脸相似度测量设计了两个损失函数。提出的目标可以在多任务学习框架中联合优化。</p>
<h1><span id="方法详解">方法详解</span></h1><p><img src="/2022/07/24/review_10_F2P/image-20220724165938954.png" alt="image-20220724165938954"></p>
<h2><span id="1imitator">1.Imitator</span></h2><p><img src="/2022/07/24/review_10_F2P/image-20220724194516986.png" alt="image-20220724194516986"></p>
<p>采用类似DCGAN的结构，包含8个反卷积层，用来模拟游戏引擎从脸部参数到脸部图像的映射。然后将真实游戏引擎的渲染结果与模拟器生成的图像算L1 loss。训练用了20000对人脸图像，游戏引擎用的是逆水寒的游戏引擎。</p>
<p><img src="/2022/07/24/review_10_F2P/image-20220724195638704.png" alt="image-20220724195638704"></p>
<p>图4显示了我们的模仿者的“渲染”结果的三个示例。这些图像的面部参数是手动创建的。由于训练样本是根据面部参数的统一分布随机生成的，所以对于大多数人物可能看起来很奇怪(请参阅我们的补充资料)。尽管如此，我们仍然可以从图4中看到，生成的人脸图像和渲染的地面真实图像有很高的相似性，即使在一些纹理复杂的区域，如头发。这表明我们的模仿者不仅将训练数据拟合在一个低维的人脸流形中，而且还学会了解耦不同人脸参数之间的相关性。</p>
<h2><span id="2facial-similarity-measurement">2.Facial Similarity Measurement</span></h2><p>alignment后的真实人脸与Imitator生成的人脸通过预训练好的识别网络算embedding间的余弦相似度。</p>
<p><img src="/2022/07/24/review_10_F2P/image-20220725143108613.png" alt="image-20220725143108613"></p>
<p>又采用了一个预训练好的分割网络对两张图像进行分割，在著名的Helen face语义分割数据集上训练该模型。为了提高人脸语义特征的位置敏感性，进一步使用分割结果(分类概率图)作为特征图的像素权值，构建位置敏感的内容丢失函数。通俗点应该就是针对每个部分对feature map做掩码算L1。</p>
<p><img src="/2022/07/24/review_10_F2P/image-20220725144042722.png" alt="image-20220725144042722"></p>
<p><img src="/2022/07/24/review_10_F2P/image-20220725143221347.png" alt="image-20220725143221347"></p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>Game Character Create</tag>
      </tags>
  </entry>
  <entry>
    <title>《Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning》</title>
    <url>/2022/03/25/review_1_Sparse%20Local%20Patch%20Transformer%20for%20Robust%20Face%20Alignment%20and%20Landmarks/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a></li>
</ul>
<!-- tocstop -->

<p>paper: [<a href="https://arxiv.org/abs/2203.06541">2203.06541] Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning (arxiv.org)</a></p>
<p>code: <a href="https://github.com/Jiahao-UTS/SLPT-master">Jiahao-UTS&#x2F;SLPT-master (github.com)</a></p>
<span id="more"></span>

<h1><span id="出发点">出发点</span></h1><p>landmark之间的内在联系对于人脸对齐的性能有很大影响，本文重点考虑其内在联系。</p>
<p>之前的方法有heatmap regression，Coordinate regression，有着不同方面的劣势。</p>
<h1><span id="创新点">创新点</span></h1><p>提出了SLPT（<em>sparse local patch transformer</em>）来学习<em>query-query</em>和<em>representation-query</em>关系（自适应内在关系）；为了进一步提高SLPT的性能，提出了一种从粗到精的框架，使局部补丁进化为<strong>金字塔形补丁</strong>。</p>
<h1><span id="做法">做法</span></h1><p>本文的SLPT并非同DETR从完整的feature map中预测坐标，而是首先从局部patch中生成每个landmark的表示特征。</p>
<p>然后，使用一系列可学习的queries（称为<em>landmark queries</em>）来聚合表示。</p>
<p>基于Transformer的交叉注意机制，SPLT在每一层学习一个<strong>自适应邻接矩阵</strong>。最后，通过MLP独立预测每个landmark在其对应patch中的subpixel坐标。由于使用了稀疏的局部补丁，与其他ViT相比，输入token的数量显著减少。 </p>
<p>为了进一步提高性能，引入了从粗到精的框架，以与SLPT结合。下图为所提出的从粗到精的框架利用稀疏的局部面片实现鲁棒的人脸对齐。根据前一阶段的landmarks裁剪稀疏的局部补丁，并将其输入到同一SLPT中以预测面部landmarks。此外，patch大小随着阶段的增加而缩小，以使局部特征演变成金字塔形式。</p>
<p><img src="/2022/03/25/review_1_Sparse%20Local%20Patch%20Transformer%20for%20Robust%20Face%20Alignment%20and%20Landmarks/1.png" alt="1"></p>
<p>整体框架图：</p>
<p><img src="/2022/03/25/review_1_Sparse%20Local%20Patch%20Transformer%20for%20Robust%20Face%20Alignment%20and%20Landmarks/2.png" alt="2"></p>
<p>分为三部分：</p>
<p><strong>the patch embedding &amp; structure encoding</strong></p>
<p>不同于ViT，SLPT先根据landmark裁剪patch，再通过线性插值将patch大小调整为K*K，又使用了结构编码（可学习的参数）来补充表示。每种编码都与相邻地标（如左眼和右眼）的编码有很高的相似性。</p>
<p>Muti-head Cross-attention（在Vision Transformer基础上的改进）：通过landmark在CNN提取出的feature map上划取局部patch，将这些feature map上的patch排成一个patch embedding，将其视为landmark的表示；紧接着对其进行结构编码（Structure Encodeing）,以获取人脸中的相对位置和patch embedding做concat。输入 landmarks queries ，通过这些MLP，独立预测每个landmark的位置。</p>
<p><strong>inherent relation layers</strong></p>
<p>受Transformer启发，每一层由三个块组成，即多头自注意（MSA）块、多头交叉注意（MCA）块和多层感知器（MLP）块，并且在每个块之前应用一个layer norm（LN）。 </p>
<p><strong>prediction heads</strong></p>
<p>预测头由一个用于规范化输入的分层模板和一个用于预测结果的MLP层组成。</p>
<p>最右边的图像显示了不同样本的自适应固有关系。其将每个点连接到第一个内在关系层中交叉注意权重最高的点显示。</p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>Patch-based Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>《Pose-guided Feature Disentangling for Occluded Person Re-identification Based on Transformer》</title>
    <url>/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a><ul>
<li><a href="#visual-context-transformer-encoder">Visual Context Transformer Encoder</a></li>
<li><a href="#pose-guided-feature-aggregation">Pose-guided Feature Aggregation</a></li>
<li><a href="#part-view-based-transformer-decoder">Part View Based Transformer Decoder</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.pdf">https://arxiv.org/pdf/2112.02466v2.pdf</a></p>
<p>code: <a href="https://github.com/WangTaoAs/PFD_Net">WangTaoAs&#x2F;PFD_Net: This is Official implementation for “Pose-guided Feature Disentangling for Occluded Person Re-Identification Based on Transformer” in AAAI2022 (github.com)</a></p>
<span id="more"></span>

<h1><span id="出发点">出发点</span></h1><p>存在遮挡的行人重识别（<em>Occluded Person Re-identification</em>），由于遮挡的存在，各种噪声被引入，导致特征不匹配；遮挡可能具有与人体部位相似的特征，导致特征学习失败。</p>
<p>前人的方法有：使用姿势信息指导特征空间将全局特征划分为局部特征（缺点是需要严格的特征空间对齐）；使用基于图的方法建模拓扑信息（缺点是容易陷入上述的第二种问题）。</p>
<h1><span id="创新点">创新点</span></h1><p>本文探索了在没有空间对齐的情况下，将附加姿势信息与Transformer相结合的可能性。其使用姿势信息对语义成分（如人体的关节部位）进行分解，并对非遮挡的部位进行选择性匹配；设计了一种<em>Pose-guided Push Loss</em>。</p>
<h1><span id="做法">做法</span></h1><p>整体框架图：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329103047158.png" alt="image-20220329103047158"></p>
<h2><span id="visual-context-transformer-encoder">Visual Context Transformer Encoder</span></h2><p>首先需要对输入图像划分为固定大小的N块patch，步距大小定义为S，每块patch的尺度定义为P，patch个数N为：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329103958451.png" alt="image-20220329103958451"></p>
<p>当S等于P时，划分出来的patch就不重叠；当S&lt;P时，patch重叠，可以减少空间领域信息的丢失。</p>
<p>将这些patch通过线形层生成一个序列输入transformer encoder，concat一组可训练的<em>Position Encoding</em>，以及<em>Camera Information Embedding</em>（表示该图像所属的摄像头视角信息，标签给定的，相同视角图像有一样的值），最终的输入序列定义为：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329104849953.png" alt="image-20220329104849953"></p>
<p>最后通过Transformer Encoder输出分为两部分的特征，一部分为global feature，一部分为part feature。为进一步区分人体各个部位的特征，part feature又分为K组，每一组都与global feature做cancat送入shared transformer layer学习这些K组融合特征。</p>
<p><strong>Encoder Supervision Loss</strong></p>
<p>选用交叉熵损失作为identity loss以及triplet loss来作为这部分的loss：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329110809342.png" alt="image-20220329110809342"></p>
<h2><span id="pose-guided-feature-aggregation">Pose-guided Feature Aggregation</span></h2><p>被遮挡的人体图像的身体信息较少，而非身体部位的信息可能不明确。本文使用<em>pose estimator</em> 从图像中提取landmark信息。</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329112105052.png" alt="image-20220329112105052"></p>
<p><strong>Pose Estimation</strong></p>
<p>给定一张图像，估计器从中提取M个landmark，然后利用这些landmark生成一组heatmap <strong>H</strong>，每张heatmap都被下采样到（H&#x2F;4）*（W&#x2F;4），其中最大的response point对应一个joint point，设置了一个阈值γ来滤除高置信度和低置信度的landmark。滤除出的剩余landmark的heatmap并不是将其设为0，而是赋值0&#x2F;1，热图标签可以形式化为：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329112535277.png" alt="image-20220329112535277"></p>
<p>ci定义为第i个landmark的置信度分数。</p>
<p><strong>Pose-guided Feature Aggregation</strong></p>
<p>将之前的分组数设为K&#x3D;M，使其等于landmark的数量。将生成的一组heatmap <strong>H</strong>后接一层FC，使其尺寸与group part local feature（fgp）相同，得到<strong>H‘</strong>。将<strong>H‘</strong>与<strong>fgp</strong> mutiply element-wisely（向量对应元素相乘，将heatmap的注意力附加在fgp上）获得<strong>P</strong>，其目的是为了从fgp中找到对身体某个部位贡献最大的信息部分。</p>
<p>为此，本文开发了一种匹配和分布机制，将part local feature和pose-guided feature视为一组相似性度量问题，最终获取一个pose-guided feature集合<strong>S</strong>。</p>
<p>对于每个<strong>Pi</strong>,在fgp中找到最相似的特征，即<strong>找寻融合了heatmap注意力的序列和原始局部特征的最近距离的局部特征</strong>，以选出优质的局部特征，用余弦距离，形式化定义为：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329115119956.png" alt="image-20220329115119956"></p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329115132888.png" alt="image-20220329115132888"></p>
<h2><span id="part-view-based-transformer-decoder">Part View Based Transformer Decoder</span></h2><p>将heatmap和fen做点乘送入Decoder学习一系列learnable semantic views以学习有区别的身体部分。其实整个框架的大体思路为一张图片走两路，一路分patch进transformer encoder，一路特征点检测生成heatmap走transformer decoder，再将这两部分的输出进行match，可以得到view feature，取高置信度的view feature采样成与fgb，fgp相同尺寸算triplet loss，再将所有的view feature采样做<em>Pose Guided Push Loss</em>。</p>
<p><strong>Pose-View Matching Module</strong></p>
<p>此部分计算patch view和通过Pose-guided Feature Aggregation得到的Set之间的相似度，来获得最终的view feature，用余弦距离，形式化定义为：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329151559098.png" alt="image-20220329151559098"></p>
<p>之前的heatmap通过阈值打好了0&#x2F;1标签，最终的view feature即可通过heatmap标签分为两类。在上述距离置信度较高的view feature中取heatmap label为1的；在置信度较低的view feature中取heatmap label为0的。这样的操作会产生可变长度，需要固定长度补0操作。</p>
<p><strong>Decoder Supervision Loss</strong></p>
<p>提出的Pose-guided Push Loss：</p>
<p>余弦距离：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329152506162.png" alt="image-20220329152506162"></p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329152517889.png" alt="image-20220329152517889"></p>
<p>整体的loss定义：</p>
<p><img src="/2022/03/30/review_2_Pose-guided%20Feature%20Disentangling%20for%20Occluded%20Person%20Re-identification%20Based%20on%20Transformer/image-20220329153030797.png" alt="image-20220329153030797"></p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>ReID</tag>
        <tag>Transformer</tag>
        <tag>Landmark</tag>
      </tags>
  </entry>
  <entry>
    <title>《DVG-Face:Dual Variational Generation for Heterogeneous Face Recognition》</title>
    <url>/2022/04/09/review_3_DVG-Face/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#%E4%B8%8E%E5%89%8D%E4%BD%9Cdvg%E7%9A%84%E4%B8%8D%E5%90%8C">与前作DVG的不同</a></li>
<li><a href="#%E5%89%8D%E5%A4%87%E7%9F%A5%E8%AF%86">前备知识</a></li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a><ul>
<li><a href="#dual-generation">Dual Generation</a><ul>
<li><a href="#training-with-paired-heterogeneous-data">Training with Paired Heterogeneous Data</a></li>
<li><a href="#training-with-unpaired-vis-data">Training with Unpaired VIS Data</a></li>
</ul>
</li>
<li><a href="#heterogeneous-face-recognition">Heterogeneous Face Recognition</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://arxiv.org/pdf/2009.09399.pdf">2009.09399.pdf (arxiv.org)</a></p>
<p>code: <a href="https://github.com/WangTaoAs/PFD_Net"><a href="https://github.com/BradyFU/DVG-Face">BradyFU&#x2F;DVG-Face: DVG-Face: Dual Variational Generation for Heterogeneous Face Recognition, TPAMI 2021 (github.com)</a></a></p>
<span id="more"></span>

<h1><span id="出发点">出发点</span></h1><p>为解决异构人脸识别（Heterogeneous Face Recognition）问题中成对异构数据匮乏的问题。</p>
<h1><span id="创新点">创新点</span></h1><p>将异构人脸识别视为一个双生成（dual generation）问题，<strong>从噪声中采样大规模的成对</strong>异构人脸数据；</p>
<p>将丰富的<strong>身份信息集成到联合分布</strong>中，以丰富生成数据的身份多样性。同时，对生成的成对图像施加一个保持成对身份的损失（<strong>pairwise identity preserving loss</strong>），以确保它们的身份一致性。这两个特性使得能够更好地利用生成的未标记数据来训练异构人脸识别网络；</p>
<p>通过将生成的成对图像视为正对，将从不同样本获取的图像视为负对，通过<strong>对比学习</strong>对异构人脸识别网络进行优化，以学习domain-invariant和区分性的embedding feature。</p>
<h1><span id="与前作dvg的不同">与前作DVG的不同</span></h1><p><strong>生成图像的身份更丰富：</strong></p>
<p>对于前作，生成器只能使用小规模的成对异构数据进行训练，从而限制生成图像的身份多样性。在当前版本中，重新设计了生成器的体系结构和训练方式，允许使用成对异构数据和<strong>大规模未配对VIS数据</strong>（单模态的非成对真实人脸数据）对其进行训练。后者的引入极大地丰富了生成图像的身份多样性。 </p>
<p><strong>生成的图像被更有效地利用：</strong></p>
<p>前作借助身份一致性属性，通过成对距离损失（pairwise distance loss）使用生成的成对数据对异构人脸识别网络进行训练。在此基础上，得益于上述身份多样性特性，当前版本进一步将从不同样本中获得的图像视为负对，形成了一种<strong>对比学习</strong>机制。 先前版本只能利用生成的图像来减少域差异，而当前版本则利用生成的图像来学习域不变和区分性嵌入特征（可学习）。 </p>
<p><strong>增加了更深入的分析和更多的实验：</strong></p>
<p>增加不同模态对图像的实验。</p>
<h1><span id="前备知识">前备知识</span></h1><p><strong>VAE</strong></p>
<p><a href="https://zhuanlan.zhihu.com/p/364917826">变分自编码器（VAE）原理 - 知乎 (zhihu.com)</a></p>
<h1><span id="做法">做法</span></h1><p><img src="/2022/04/09/review_3_DVG-Face/image-20220409140033446.png" alt="image-20220409140033446"></p>
<p>待解决问题：</p>
<p>（1）如何生成不同的配对异构数据</p>
<p>（2）如何有效利用这些生成的数据</p>
<h2><span id="dual-generation">Dual Generation</span></h2><p><strong>核心：结合域属性和身份特征</strong></p>
<p>通过一个双变分生成器实现。生成器包含两个特定域的encoder（图中的橙色和灰色Ev，En），一个decoder（浅蓝色G），一个预训练好的人脸识别网络（F）以及一个身份采样器（Fs）。</p>
<p>两个<em>Domain-specific attribute encoders</em>用于学习NIR和VIS数据的领域特定属性分布，人脸识别网络用于提取身份特征，身份采样器可以灵活地从噪声中采样丰富的身份表示（？）。成对异构数据的联合分布由身份表示和属性分布组成（具体的fusion文中并未指出，代码中是concat），decoder将联合分布映射到像素空间。</p>
<h3><span id="training-with-paired-heterogeneous-data">Training with Paired Heterogeneous Data</span></h3><p>输入成对同身份的异构图像，生成器学习潜在空间中的解耦联合分布。具体而言，采用在MS-Celeb-1M上预训练的人脸识别模型（本文采用的是LightCNN）作为特征提取器，由F提取出的特征被认为仅仅是identity related。考虑到F是从VIS模态预训练得到的，在另一个模态的表现不好，那么只需要提取VIS模态的身份特征作为两个模态共同的身份表示。</p>
<p>两个encoder提取出Domain-specific attribute分布，为确保其仅仅是属性相关的，在属性和身份表示之间施加了角度正交损失。最后解耦后的两种分布构成成对NIR-VIS数据的联合分布，然后被送到decoder作为输入。</p>
<p>该过程中涉及到了四个损失函数：包括角正交损失、分布学习损失、成对身份保持损失和对抗性损失。</p>
<p><strong>Angular Orthogonal Loss</strong></p>
<p>角正交损失施加在Zv和f，Zn和f之间，计算它们之间的余弦相似度，最小化他们的绝对值的和。</p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409153226995.png" alt="image-20220409153226995"></p>
<p><strong>Distribution Learning Loss</strong></p>
<p>分布学习损失启发自VAEs，首先用KL散度计算两个分布的差异，再结合L1正则化重构decoder的输入。</p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409162532051.png" alt="image-20220409162532051"></p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409162539110.png" alt="image-20220409162539110"></p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409162548220.png" alt="image-20220409162548220"></p>
<p><strong>Pairwise Identity Preserving Loss</strong></p>
<p>为了保留生成数据的身份，以前基于条件生成的方法通常采用身份保留损失。利用预训练好的人脸识别网络分别提取生成数据和真实目标数据的嵌入特征，然后迫使这两个特征尽可能接近。然而，由于既不存在类内约束，也不存在类间约束，因此很难保证生成的图像属于与目标一致的特定类。</p>
<p>本文关注生成的成对图像的身份一致性，而不是生成的图像属于谁。因此提出了一种成对的身份保持损失，以限制特征之间的距离</p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409172316619.png" alt="image-20220409172316619"></p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409172325881.png" alt="image-20220409172325881"></p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409172332593.png" alt="image-20220409172332593"></p>
<p><strong>Adversarial Loss</strong></p>
<p>引入对抗损失来提高生成图像的清晰度。</p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409184040111.png" alt="image-20220409184040111"></p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409184047202.png" alt="image-20220409184047202"></p>
<p><strong>Overall Loss</strong></p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409184118573.png" alt="image-20220409184118573"></p>
<h3><span id="training-with-unpaired-vis-data">Training with Unpaired VIS Data</span></h3><p>身份信息获取的一种简单的方法是使用预训练的人脸识别网络从大规模VIS数据中提取身份。然而，如果希望在测试阶段生成大规模的新配对数据，必须拥有相同数量的具有不同身份的VIS数据（如b图右下角）。</p>
<p>为避免此情况，引入了身份采样器（identity sampler）。具体实现为，首先采用识别网络提取MS-Celeb-1M数据集上的embedding特征，利用这些特征来训练VAE模型。训练后的VAE的decoder被用作身份采样器，它可以将标准高斯噪声中的点映射到身份表示。</p>
<p>由于这些采样的身份表示没有对应的ground true异构图像，本文建议以不成对的方式训练生成器。 </p>
<p><strong>整体流程：</strong></p>
<p>首先，通过身份采样器Fs对身份特征f~（经过识别网络提取）进行采样。</p>
<p>然后，将两种属性分布特征Zn和Zv以及f~分别concat输入decoder G。</p>
<p>最后，生成一对不属于异构数据库的新异构图像。 </p>
<p>其中的loss和train with paired的情况类似，只不过没有了对抗损失（具体可见train_generator.py代码中的loss部分）。</p>
<h2><span id="heterogeneous-face-recognition">Heterogeneous Face Recognition</span></h2><p>与训练好的LightCNN作为backbone，使用有限的数据对进行训练，再使用大规模生成后的数据进行训练对比。loss选择softmax loss。backbone在训练生成器时是权重更新的，在此处HRN中固定。</p>
<p>对于生成的数据，由于没有特定的类别标签，上述softmax loss不适用。引入<strong>对比学习</strong>机制来利用这些数据。</p>
<p>对比学习机制流程：</p>
<p>首先从生成数据中采样两对异构数据，基于生成的都是身份一致的，这两对都是正例，再将其做交叉，构造出了两对负例，要注意的是要保证交叉后的模态还是跨模态的。对比损失如下：</p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409191600670.png" alt="image-20220409191600670"></p>
<p>其中m是一个margin值。</p>
<p>整体的HFR网络损失为：</p>
<p><img src="/2022/04/09/review_3_DVG-Face/image-20220409191702568.png" alt="image-20220409191702568"></p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>HFR</tag>
        <tag>generation</tag>
      </tags>
  </entry>
  <entry>
    <title>《CariMe:Unpaired Caricature Generation with Multiple Exaggerations》</title>
    <url>/2022/04/13/review_4_CariMe/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E4%BB%BB%E5%8A%A1">任务</a></li>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a><ul>
<li><a href="#multi-exaggeration-warper">Multi-exaggeration Warper</a><ul>
<li><a href="#warp-reconstruction-loss">Warp Reconstruction Loss</a></li>
<li><a href="#photo-reconstruction-loss">Photo Reconstruction Loss</a></li>
<li><a href="#total-variation-loss">Total Variation Loss</a></li>
<li><a href="#wrapper-total-loss">Wrapper Total Loss</a></li>
</ul>
</li>
<li><a href="#styler">Styler</a><ul>
<li><a href="#adversarial-loss">Adversarial Loss</a></li>
<li><a href="#image-reconstruction-loss">Image Reconstruction Loss</a></li>
<li><a href="#cycle-consistency-loss">Cycle Consistency Loss</a></li>
<li><a href="#style-total-loss">Style Total Loss</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.pdf">https://ieeexplore.ieee.org/abstract/document/9454341/</a></p>
<p>code: <a href="https://github.com/edward3862/CariMe-pytorch">edward3862&#x2F;CariMe-pytorch: Unpaired Caricature Generation with Multiple Exaggerations (TMM 2021) (github.com)</a></p>
<span id="more"></span>

<h1><span id="任务">任务</span></h1><p>漫画生成、图像到图像的翻译、图像扭曲、风格转换</p>
<h1><span id="出发点">出发点</span></h1><p>与一般的image-to-image translation不同，由于各种空间变形的存在，自动绘制漫画是一项更具挑战性的任务。原先的漫画生成方式都是instance-level，本文拓展到distribution-level。</p>
<h1><span id="创新点">创新点</span></h1><p>CariMe可生成具有<strong>多重</strong>夸张和<strong>多种</strong>风格的漫画</p>
<p>提出了一种基于变形场（deformation fields）的非成对漫画生成方法，可以有效地学习真实照片到漫画的空间变换分布。还引入了一个辅助内容编码（auxiliary content code），以帮助产生有意义的、特定于照片的夸张。</p>
<h1><span id="做法">做法</span></h1><p>一个wrapper，一个styler</p>
<h2><span id="multi-exaggeration-warper">Multi-exaggeration Warper</span></h2><p>分别输入一张漫画脸和真实脸，首先计算所有漫画的平均landmark（代码中的main_cal_wrap_degree.py），<img src="/2022/04/13/review_4_CariMe/image-20220414090134868.png" alt="image-20220414090134868">从每个漫画中得到，表达了该张漫画所表示的特定的夸张模式，然后该特征经过encoder获得低维向量Zw（wrap code)，与此同时引入了对应的真实人脸过另一个encoder提取Zp（auxiliary photo-specific content code)，Zp与Zw都是满足标准正态分布的向量，在测试阶段Zw是从正态分布中随机采样获取，以获得不同的变形编码。down scale和up scale操作文中的解释是给变形场去噪。</p>
<p><img src="/2022/04/13/review_4_CariMe/image-20220414083337682.png" alt="image-20220414083337682"></p>
<h3><span id="warp-reconstruction-loss">Warp Reconstruction Loss</span></h3><p><img src="/2022/04/13/review_4_CariMe/image-20220414092823399.png" alt="image-20220414092823399"></p>
<p>L1正则，旨在拉近reconstruction后的变形场与原变形场的差异。</p>
<h3><span id="photo-reconstruction-loss">Photo Reconstruction Loss</span></h3><p><img src="/2022/04/13/review_4_CariMe/image-20220414093327113.png" alt="image-20220414093327113"></p>
<p>L1正则，旨在拉近reconstruction后的真实人脸与输入真实人脸的差异，来鼓励Zp保持对原真是人脸的内容和空间信息。</p>
<h3><span id="total-variation-loss">Total Variation Loss</span></h3><p><img src="/2022/04/13/review_4_CariMe/image-20220414094156859.png" alt="image-20220414094156859"></p>
<p>用来给生成的图像去噪。</p>
<p><a href="https://blog.csdn.net/qq_38406029/article/details/118996415">(54条消息) TV Loss详解_鬼道2022的博客-CSDN博客_tvloss公式</a></p>
<h3><span id="wrapper-total-loss">Wrapper Total Loss</span></h3><p><img src="/2022/04/13/review_4_CariMe/image-20220414094730445.png" alt="image-20220414094730445"></p>
<h2><span id="styler">Styler</span></h2><p>旨在做风格迁移，将图像分界为content representation和style code。</p>
<p>一个style encoder，一个context encoder，一个style decoder（AdaLIN:<a href="https://blog.csdn.net/weixin_43823140/article/details/107840916">(54条消息) 【飞桨】论文解读：U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance_目楽 Leo mu的博客-CSDN博客</a>），style输出一个正态分布的style code，context输出feature map。</p>
<p>将context输出的feature map分别进行instance normalization（styler中常用的一种像素normalization）和layer normalization，combine方式需要到代码中查看，送到AdaLIN中，其范式为：</p>
<p><img src="/2022/04/13/review_4_CariMe/image-20220414101725797.png" alt="image-20220414101725797"></p>
<p><img src="/2022/04/13/review_4_CariMe/image-20220414083358409.png"></p>
<h3><span id="adversarial-loss">Adversarial Loss</span></h3><p><img src="/2022/04/13/review_4_CariMe/image-20220414101755992.png" alt="image-20220414101755992"></p>
<p>AdaLIN是基于GAN的方法，所以有adversarial loss。</p>
<h3><span id="image-reconstruction-loss">Image Reconstruction Loss</span></h3><p><img src="/2022/04/13/review_4_CariMe/image-20220414101914719.png" alt="image-20220414101914719"></p>
<p>L1正则，旨在拉近reconstruction后的图像与原图像的距离。</p>
<h3><span id="cycle-consistency-loss">Cycle Consistency Loss</span></h3><p>对生成后的图像再次进入encoder，输出特征与之前encoder输出做L1正则，这里的cycle是循环，不是cycle loss！</p>
<p><img src="/2022/04/13/review_4_CariMe/image-20220414103236545.png" alt="image-20220414103236545"></p>
<h3><span id="style-total-loss">Style Total Loss</span></h3><p><img src="/2022/04/13/review_4_CariMe/image-20220414103409597.png" alt="image-20220414103409597"></p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>generation</tag>
        <tag>caricature</tag>
      </tags>
  </entry>
  <entry>
    <title>《Clothes-Changing Person Re-identification with RGB Modality Only》</title>
    <url>/2022/05/02/review_5_Simple-CCReID/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E4%BB%BB%E5%8A%A1">任务</a></li>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a><ul>
<li><a href="#clothes-based-adversarial-loss">Clothes-based Adversarial Loss</a><ul>
<li><a href="#training-clothes-classifier">Training clothes classifier</a></li>
<li><a href="#learning-clothes-irrelevant-features">Learning clothes-irrelevant features</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.pdf">https://arxiv.org/pdf/2204.06890v1.pdf</a></p>
<p>code: <a href="https://github.com/guxinqian/simple-ccreid">guxinqian&#x2F;Simple-CCReID: Pytorch implementation of ‘Clothes-Changing Person Re-identification with RGB Modality Only. In CVPR, 2022.’ (github.com)</a></p>
<span id="more"></span>

<h1><span id="任务">任务</span></h1><p>换装人员重识别</p>
<h1><span id="出发点">出发点</span></h1><p><strong>提取与衣服无关的特征</strong>，如脸型、发型、体形和步态。目前大多数工作主要集中在从多模态信息（如剪影和草图）中建模身体形状，但没有充分利用原始 RGB 图像中与衣服无关的信息。</p>
<h1><span id="创新点">创新点</span></h1><p>文章提出一种基于衣服的<strong>对抗</strong>性损失（CAL），通过惩罚 re-id 模型对衣服的预测能力（其实就是个分类器），从原始 RGB 图像中<strong>挖掘与衣服无关的特征</strong>。</p>
<h1><span id="做法">做法</span></h1><p>首先通过最小化Lc来优化衣服分类器。然后固定衣服分类器的参数，最小化Lid和Lca，迫使backbone学习与衣服无关的特征。（每张图片对应一个一个身份标签和服装标签，将衣服类定义为细粒度标识类——所有同一身份的样本都根据他们的衣服被划分为属于该身份的不同类别，不同的人即使穿着相同的衣服也不会共享相同的衣服标签）</p>
<p><img src="/2022/05/02/review_5_Simple-CCReID/image-20220504092335366.png" alt="image-20220504092335366"></p>
<h2><span id="clothes-based-adversarial-loss">Clothes-based Adversarial Loss</span></h2><h3><span id="training-clothes-classifier">Training clothes classifier</span></h3><p>Lc采用交叉熵损失</p>
<h3><span id="learning-clothes-irrelevant-features">Learning clothes-irrelevant features</span></h3><p>固定衣服分类器的参数，并强制主干学习与衣服无关的特征。为此，应该<strong>惩罚re-id模型对于衣服的预测能力</strong>。</p>
<p>然而，由于衣服类被定义为细粒度类，因此惩罚re-id模型对于所有衣服类别的预测能力，也会降低其预测身份的能力，这对re-id是有害的。</p>
<p>那么最终的目的是使经过训练的<strong>服装分类器无法区分相同身份和不同服装的样本</strong>。</p>
<p>所以，Lca是一个<em>multi-positive-class classification loss</em>，属于同一个身份的不同服装类互为正类。（把不同衣服的只要是属于同一个id身份的衣服类都视为衣服正类）</p>
<p><img src="/2022/05/02/review_5_Simple-CCReID/image-20220504101750986.png" alt="image-20220504101750986"></p>
<p>Si+（Si-)是一个有相同身份的衣服标签fi的集合，K是S中的衣服类别数量，q(c)是第c个衣服类别的交叉熵损失的权重，同一件衣服同一个身份的正例，和不同衣服同一个身份的正例有相同的权重1&#x2F;K。</p>
<p>同时，为了在不严重降低衣服一致性识别精度的情况下提高模型的换衣识别能力，等式（4）可替换为：</p>
<p><img src="/2022/05/02/review_5_Simple-CCReID/image-20220504105337953.png" alt="image-20220504105337953"></p>
<p><img src="/2022/05/02/review_5_Simple-CCReID/image-20220504105427890.png" alt="image-20220504105427890"></p>
<p>另外，穿着相同衣服的正例比穿着不同衣服的正例有更大的权重。</p>
<p>在优化CAL的同时，对身份分类器进行了优化。因此，第二步的优化过程是：</p>
<p><img src="/2022/05/02/review_5_Simple-CCReID/image-20220504105540307.png" alt="image-20220504105540307"></p>
<p>当仅使用Lid进行训练时，该模型在优化的早期阶段倾向于学习简单样本（具有相同的衣服），然后逐渐学习区分困难样本（具有相同的身份和不同的衣服）。类似课程学习方式，但尽管如此（6）中并未抛弃Lid。其原因是，在优化的早期阶段，只有最小化Lca并迫使模型区分硬样本，才可能导致局部最优。相反，在我们的实验中，在第一次降低学习率后，增加了Lca用于训练。</p>
<p>&#96;&#96;</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class ClothesBasedAdversarialLoss(nn.Module):</span><br><span class="line">    &quot;&quot;&quot; Clothes-based Adversarial Loss.</span><br><span class="line"></span><br><span class="line">    Reference:</span><br><span class="line">        Gu et al. Clothes-Changing Person Re-identification with RGB Modality Only. In CVPR, 2022.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        scale (float): scaling factor.</span><br><span class="line">        epsilon (float): a trade-off hyper-parameter.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, scale=16, epsilon=0.1):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.scale = scale</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line"></span><br><span class="line">    def forward(self, inputs, targets, positive_mask):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Args:</span><br><span class="line">            inputs: prediction matrix (before softmax) with shape (batch_size, num_classes)</span><br><span class="line">            targets: ground truth labels with shape (batch_size)</span><br><span class="line">            positive_mask: positive mask matrix with shape (batch_size, num_classes). The clothes classes with </span><br><span class="line">                the same identity as the anchor sample are defined as positive clothes classes and their mask </span><br><span class="line">                values are 1. The clothes classes with different identities from the anchor sample are defined </span><br><span class="line">                as negative clothes classes and their mask values in positive_mask are 0.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        inputs = self.scale * inputs</span><br><span class="line">        negtive_mask = 1 - positive_mask</span><br><span class="line">        identity_mask = torch.zeros(inputs.size()).scatter_(1, targets.unsqueeze(1).data.cpu(), 1).cuda()</span><br><span class="line"></span><br><span class="line">        exp_logits = torch.exp(inputs)</span><br><span class="line">        log_sum_exp_pos_and_all_neg = torch.log((exp_logits * negtive_mask).sum(1, keepdim=True) + exp_logits)</span><br><span class="line">        log_prob = inputs - log_sum_exp_pos_and_all_neg</span><br><span class="line"></span><br><span class="line">        mask = (1 - self.epsilon) * identity_mask + self.epsilon / positive_mask.sum(1, keepdim=True) * positive_mask</span><br><span class="line">        loss = (- mask * log_prob).sum(1).mean()</span><br><span class="line"></span><br><span class="line">        return loss</span><br></pre></td></tr></table></figure>





]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>ReID</tag>
        <tag>decouple</tag>
        <tag>adversarial</tag>
      </tags>
  </entry>
  <entry>
    <title>《IACycleGAN》</title>
    <url>/2022/06/08/review_6_IACycleGAN/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E4%BB%BB%E5%8A%A1">任务</a></li>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a><ul>
<li><a href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">生成模型</a><ul>
<li><a href="#%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C%E6%95%B4%E4%BD%93loss%E5%9B%BE%E7%A4%BA">生成网络整体loss图示</a></li>
</ul>
</li>
<li><a href="#%E8%AF%86%E5%88%AB%E7%BD%91%E7%BB%9C%E4%B8%8E%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BA%92%E7%9B%B8%E4%BC%98%E5%8C%96">识别网络与生成网络的互相优化</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E9%AA%8C">实验</a><ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E9%9B%86">数据集</a><ul>
<li><a href="#%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%96%BD%E7%BB%86%E8%8A%82">生成模型实施细节</a></li>
<li><a href="#%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%96%BD%E7%BB%86%E8%8A%82">识别模型实施细节</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.pdf"><a href="https://arxiv.org/pdf/2103.16019.pdf">Identity-Aware CycleGAN for Face Photo-Sketch Synthesis and Recognition (arxiv.org)</a></a></p>
<p>code: none</p>
<span id="more"></span>

<h1><span id="任务">任务</span></h1><p>真人素描生成与识别</p>
<h1><span id="出发点">出发点</span></h1><p>生成促进识别 识别促进生成；</p>
<p>大部分生成方法使得合成图像与原始图像在纹理上保持一致，会导致信息丢失；</p>
<p>大多数生成框架都只能学习两个域之间的关系，其鉴别器只关注照片和草图之间的差异，而不考虑任何特定的识别优化（身份信息）；</p>
<h1><span id="创新点">创新点</span></h1><p>在CycleGAN上加入了感知损失（perceptual loss）,能更好的关注面部的语义信息（眼睛、鼻子）;</p>
<p>使生成模型和识别模型相互优化，生成模型迭代生成更好的图像，Triplet Loss训识别模型；</p>
<h1><span id="做法">做法</span></h1><h2><span id="生成模型">生成模型</span></h2><p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608102426736.png" alt="image-20220608102426736"></p>
<p>分别给出两个domain的照片（这里输入的是paired数据，这里的paired应该身份paired），其训练目的是获得Gx，Gy两个生成器。</p>
<p>两个识别网络，其目的是使用pretrain好的vggface提取feature做Identity perception loss；</p>
<p>生成器用了《Perceptual losses for real-time style transfer and super-resolution》中的结构；</p>
<p>判别器用了PatchGAN的结构；</p>
<h3><span id="生成网络整体loss图示">生成网络整体loss图示</span></h3><p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608105545461.png" alt="image-20220608105545461"></p>
<h4><span id="对抗损失adversarial-loss">对抗损失（adversarial loss）</span></h4><p>输入图像与生成图像进入判别器进行计算，最小化下式</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608110633855.png" alt="image-20220608110633855"></p>
<h4><span id="循环一致性损失cycle-consistency-loss">循环一致性损失（cycle consistency loss）</span></h4><p>Gx生成的图像再进入Gy恢复原本domain与最初的输入x计算L1，<strong>此loss为pix级</strong>，最小化下式</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608110959225.png" alt="image-20220608110959225"></p>
<h4><span id="身份保持损失identity-perception-loss">身份保持损失（identity perception loss）</span></h4><p>仅使用对抗损失会导致伪影和训练不稳定，需要加更强大的监督；</p>
<p>通过一个pretrain的识别网络（文中选择vggface）分别提取两对（原图与合成图）的feature计算L2，<strong>此loss区别于pix级监督，是feature级</strong>，文中对于此处的解释是：绘制的草图会有夸张成分以扭曲面部纹理信息，夸大面部特征，完全基于pix重建图像效果不会好；CycleGAN的训练需要进行数据增强操作（resize，flip等）难以实施pix级的监督。</p>
<p>最小化下式</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608111456554.png" alt="image-20220608111456554"></p>
<h4><span id="身份映射损失identity-mapping-loss">身份映射损失（identity mapping loss）</span></h4><p>常规的pix级的约束，最小化下式</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608112212476.png" alt="image-20220608112212476"></p>
<h4><span id="整体loss">整体loss</span></h4><p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608112236780.png" alt="image-20220608112236780"></p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608112252105.png" alt="image-20220608112252105"></p>
<p>最小化生成器的loss最大化判别器的loss</p>
<p>其中超参数lamda分别设为10，30000000，5</p>
<h2><span id="识别网络与生成网络的互相优化">识别网络与生成网络的互相优化</span></h2><p>许多主做生成的工作都是fix相应的识别网络作为一个特征提取器来附加身份保持损失；</p>
<p>本文的做法估计是：</p>
<p>step1：先fix识别网络参数训练生成网络，获得一定量的生成的图片；</p>
<p>step2：使用生成的图片fine-tune识别网络（单走一个人脸识别模型的流程），用了triplet loss，更新识别网络参数，其中两个模态分别有两个识别网络，要分开训练；</p>
<p>step3：重复上述操作以获得更好的生成模型和识别模型。</p>
<p>（有问题的点：若第一次生成的质量得不到保证，那低质量的生成图像真的能提升识别模型的acc吗？<strong>互相优化的模型很依赖第一次生成的图像质量</strong>）</p>
<p>我认为的做法：先简单训练一个识别网络达到一个还行的acc，更新其backbone参数作为生成网络的特征提取器，然后再进行生成网络的训练。</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608135047478.png" alt="image-20220608135047478"></p>
<h1><span id="实验">实验</span></h1><p>CycleGAN生成—》一阶段vgg fine-tune—》IACycleGAN生成（加入fine-tune的vgg提取的特征做身份保持损失）—》二阶段vgg fine-tune</p>
<h2><span id="数据集">数据集</span></h2><p>CUFS和CUFSF</p>
<h3><span id="生成模型实施细节">生成模型实施细节</span></h3><p>对于生成网络的训练，都是从头开始训练，使用instance normalization来实现更好的稳定性和更低的噪声；</p>
<p>使用Adam优化器，horizontal filp prob&#x3D;0.5用于数据增强；</p>
<p>前100个epoch设置0.0002的学习率，并在后100个epoch线性下降至0；</p>
<p>在titian xp上训练了10小时；</p>
<p>为减小网络震荡，采用存储多个生成图像的图像缓冲区来更新鉴别器，而不是使用最后生成的图像。</p>
<p>生成使用了SSIM FSIM两个指标</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608152343197.png" alt="image-20220608152343197"></p>
<h3><span id="识别模型实施细节">识别模型实施细节</span></h3><p>文中用的vggface，caffe上跑的（估计是官方代码）</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608153331083.png" alt="image-20220608153331083"></p>
<p>在做检索任务时，先做风格模态的迁移，再计算相似度。</p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608155946505.png" alt="image-20220608155946505"></p>
<p><img src="/2022/06/08/review_6_IACycleGAN/image-20220608160958943.png" alt="image-20220608160958943"></p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>sketch</tag>
        <tag>synthesis</tag>
        <tag>GAN</tag>
        <tag>face recognition</tag>
      </tags>
  </entry>
  <entry>
    <title>《Learning to Warp for Style Transfer》</title>
    <url>/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E4%BB%BB%E5%8A%A1">任务</a></li>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C">相关工作</a><ul>
<li><a href="#texture-nst">Texture NST</a><ul>
<li><a href="#%E5%9C%A8%E7%BA%BF%E4%BC%98%E5%8C%96%E7%9A%84%E5%BD%A2%E5%BC%8F">在线优化的形式</a></li>
<li><a href="#%E7%A6%BB%E7%BA%BF%E4%BC%98%E5%8C%96%E7%9A%84%E5%BD%A2%E5%BC%8F">离线优化的形式</a></li>
<li><a href="#%E5%85%B6%E4%BB%96%E5%8F%98%E5%BC%8F">其他变式</a></li>
</ul>
</li>
<li><a href="#geometric-nst">Geometric NST</a></li>
</ul>
</li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a><ul>
<li><a href="#geometric-stylewarper">Geometric Style（Warper）</a><ul>
<li><a href="#feature-extraction">Feature Extraction</a></li>
<li><a href="#feature-correlation">Feature Correlation</a></li>
<li><a href="#warp-network-training-and-using">Warp Network: Training and Using</a></li>
</ul>
</li>
<li><a href="#texture-stylestyler">Texture Style（Styler）</a></li>
<li><a href="#%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82">实现细节</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Jing_Cross-Modal_Center_Loss_for_3D_Cross-Modal_Retrieval_CVPR_2021_paper.pdf"><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Liu_Learning_To_Warp_for_Style_Transfer_CVPR_2021_paper.pdf">Learning To Warp for Style Transfer (thecvf.com)</a></a></p>
<p>code: <a href="https://github.com/xch-liu/learning-warp-st">https://github.com/xch-liu/learning-warp-st</a></p>
<span id="more"></span>

<h1><span id="任务">任务</span></h1><p>NST（neural style transfer）</p>
<h1><span id="出发点">出发点</span></h1><p>大多数风格迁移只考虑了迁移纹理信息，而为关注在艺术层面的几何扭曲。</p>
<p>本文考虑使用深度神经网络进行图像样式化的问题，特别关注<strong>艺术扭曲</strong>。</p>
<h1><span id="创新点">创新点</span></h1><p>在CycleGAN上加入了感知损失（perceptual loss）,能更好的关注面部的语义信息（眼睛、鼻子）;</p>
<p>使生成模型和识别模型相互优化，生成模型迭代生成更好的图像，Triplet Loss训识别模型；</p>
<p>与其他扭曲迁移模型的区别：</p>
<p>不同于The face of art: Landmark detection and geometric style in portraits和WarpGAN，其<strong>不限于单个语义类别</strong>；</p>
<p>不同于Deformable style transfer依赖前向和后向优化，其专门设计了前馈网络，以<strong>输出给定内容和几何图像的扭曲字段</strong>；</p>
<p>比Deformable style transfer快；</p>
<p>与Geometric style transfer仅限于参数化扭曲字段不同，其生成的是<strong>非参数化扭曲</strong>；</p>
<p>与Geometric style transfer以外的其他NST算法不同，其<strong>支持使用两幅图像来指定样式</strong>，这为图像创建增加了其他NST算法所没有的多功能性。</p>
<h1><span id="相关工作">相关工作</span></h1><h2><span id="texture-nst">Texture NST</span></h2><p>纹理NST一直是NST的主要形式（默认NST都是纹理NST）</p>
<h3><span id="在线优化的形式">在线优化的形式</span></h3><p>通过迭代优化图像来传递样式。</p>
<h3><span id="离线优化的形式">离线优化的形式</span></h3><p>离线优化生成模型，并在测试阶段通过一次向前传递生成样式化图像。</p>
<p>训好的模型一般只能迁移特定样式，有些模型将多种风格融合到一个模型中，或者使用一个模型来传递任意的艺术风格。</p>
<h3><span id="其他变式">其他变式</span></h3><p>肖像画风格转移、视觉属性转移、语义风格转移、视频风格转移、3D风格转移和照片级真实感风格转移</p>
<h2><span id="geometric-nst">Geometric NST</span></h2><p>几何形变的艺术风格的迁移越来越重要。</p>
<p>一些方法仅限于特定的内容域，如面部《The face of art: Landmark detection and geometric style in portraits》和文本。这些方法产生了极好的结果。</p>
<p>《Deformable style transfer》和《Geometric Style Transfer》描述了在多个类上操作的更通用的方法，增加的灵活性在质量方面似乎成本不高。</p>
<p>DST速度慢 支持任意形变</p>
<p>GST速度快 不支持任意形变</p>
<p>本文方法快且支持任意形变</p>
<h1><span id="做法">做法</span></h1><p>输入要求：</p>
<p>1）一张要被迁移的content图像Ic</p>
<p>2）一张指导几何迁移的图像Ig</p>
<p>3）一张指导纹理迁移的图像It</p>
<p>Ig和It可以是同一张</p>
<p>total pipeline如下（一个warper 一个styler）</p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220609205913858.png" alt="image-20220609205913858"></p>
<h2><span id="geometric-stylewarper">Geometric Style（Warper）</span></h2><p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220609210756668.png" alt="image-20220609210756668"></p>
<p>其关键思想是训练一个能够推断<strong>二维扭曲场w</strong>的神经网络，并创建一个衡量特征相似性的四维标量函数<strong>M</strong>。</p>
<p>有三个主要组成部分：</p>
<p>1）特征提取：分别获取 Fg 和 Fc</p>
<p>2）特征融合：来衡量特征相似度 M(Fc,Fg)</p>
<p>3）训练warp网络输出函数 f 使得 w &#x3D; f(M)，<strong>一旦训完，网络 f 可以在新输入上使用，无需修改</strong>，（即得到了一个通用的Warp Field Estimation用来估计warp degree）本文所有输出都是单个warper生成的。</p>
<p>定义warp field的w是非参数化的，不对分布做假设，就是一些统计量，不用网络进行分布的约束。</p>
<h3><span id="feature-extraction">Feature Extraction</span></h3><p>使用VGG，提取pool4出来的特征，接一个L2 normalization。输出的 F 是 W * H 的特征图，本文中是16 * 16，文中说是平衡了计算效率和扭曲质量得出的结果。</p>
<h3><span id="feature-correlation">Feature Correlation</span></h3><p>此模块计算feature map在每个pixel上的特征关联分数，结果存储在四维标量函数 M 中,M∈R W×H×W×H，其中的每个元素的计算规则如下：</p>
<p>Fc 是content图像提取的feature；</p>
<p>Fg是geometric图像提取的feature；</p>
<p>i，j 对应Fc中的pix坐标</p>
<p>k，l 对应Fg中pix的坐标</p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220610101853238.png" alt="image-20220610101853238"></p>
<p>分母为遍历Fg上的每个pix与Fc( i , j ) 做内积求平方开根；</p>
<p>分子为Fc（i，j）与Fg（k，l）做内积。</p>
<p>个人理解：这个统计量表示了每个两个feature间的关联性特征量。</p>
<h3><span id="warp-network-training-and-using">Warp Network: Training and Using</span></h3><p>本文在技术上的贡献为其训练了一个 f 来输出非参数化的warp field（w）。</p>
<p>输入feature correlation（M），该步骤可以表示为 w &#x3D; f (M)，形式化定义这层mapping：</p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220610191607879.png" alt="image-20220610191607879"></p>
<p>W1 , H1是图像尺寸，最终warp module输出根据Ig的几何形变warp后的Ic。</p>
<p>原则上不需要进行训练，因为如下的优化问题对于任何的图像对的数据都足够解决：</p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220610192737703.png" alt="image-20220610192737703"></p>
<p>h是一个度量函数。</p>
<p>此基于优化的方式来自于DST，但与直接从经过训练的网络计算扭曲场相比，单实例优化速度较慢。（本文没采用基于优化的方式，而是单独训练了一个网络）</p>
<p>本文网络使用<strong>一组语义相关或具有几何相似部分的图像对</strong>进行训练。图像对涵盖了广泛的语义内容：人脸、动物等。为了提高模型在艺术领域的泛化能力，我们使用艺术增强来创建每个训练图像的纹理增强副本。<strong>经过训练后，变形网络可以应用于任何图像，无论其语义内容如何</strong>。</p>
<p><strong>其基本思想是局部移动内容图像中的像素，并重新计算新扭曲图像中的特征，直到loss收敛</strong></p>
<p>定义Fm来表示受像素m影响的</p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220610200411017.png" alt="image-20220610200411017"></p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220610200701085.png" alt="image-20220610200701085"></p>
<p><strong>这个最终的softmax归一化输出代表每个content图像的pixel在每个扭曲图像中的search window中的关联程度</strong></p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220610213521698.png" alt="image-20220610213521698"></p>
<p>个人最终理解：</p>
<p><strong>输入扭曲图像和内容图像（待扭曲图像），使用VGG提取feature map，warp field矩阵扭曲Fc矩阵，与Fg矩阵进行feature matching计算一个关联度四维统计量，输入warp field estimator（神经网络），最小化每个pixel和其扭曲图像相应search window的关联度统计量来使得内容图像尽可能扭曲成目标扭曲样式；神经网络更新参数，输出更好的warp field，循环优化网络，直至达到良好的扭曲效果。</strong></p>
<p>前半部分机器学习，后半部分深度学习，这样warp field可以直接通过网络inference出来，速度提升。</p>
<h2><span id="texture-stylestyler">Texture Style（Styler）</span></h2><p>最小化content loss 和 texture loss，二者依赖于用于目标检测而训练的模型，本文唯一不同在于采取多尺度策略，<strong>优先将纹理随着细节的增加转移到输出图像的不同区域</strong>。几十年来，这种策略一直用于规定性纹理合成，最近用于仅纹理的NST，有助于改善风格转换结果。在本文工作中，利用它来解决<strong>由于几何扭曲而产生的模糊和其他瑕疵</strong>。</p>
<p>此部分不做详解</p>
<h2><span id="实现细节">实现细节</span></h2><p>warper在PF-PASCAL和MS COCO数据集上训练，可训练网络结构不大，单卡训了2小时</p>
<p><img src="/2022/06/09/review_7_Learing%20to%20Warp%20for%20Style%20Transfer/image-20220611151019063.png" alt="image-20220611151019063"></p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>style transfer</tag>
        <tag>warp</tag>
      </tags>
  </entry>
  <entry>
    <title>《Geometric and Textural Augmentation for Domain Gap Reduction》</title>
    <url>/2022/06/12/review_8_geom%20tex%20dg/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E4%BB%BB%E5%8A%A1">任务</a></li>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E5%88%9B%E6%96%B0%E7%82%B9">创新点</a></li>
<li><a href="#idea%E6%A6%82%E8%A7%88">idea概览</a></li>
<li><a href="#%E5%81%9A%E6%B3%95">做法</a><ul>
<li><a href="#%E5%87%A0%E4%BD%95%E5%A2%9E%E5%BC%BA">几何增强</a></li>
<li><a href="#%E7%BA%B9%E7%90%86%E5%A2%9E%E5%BC%BA">纹理增强</a></li>
<li><a href="#%E7%BB%93%E5%90%88%E5%87%A0%E4%BD%95%E5%92%8C%E7%BA%B9%E7%90%86%E5%A2%9E%E5%BC%BA">结合几何和纹理增强</a></li>
</ul>
</li>
<li><a href="#%E5%AE%9E%E9%AA%8C">实验</a></li>
<li><a href="#%E7%BB%93%E8%AE%BA">结论</a></li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Geometric_and_Textural_Augmentation_for_Domain_Gap_Reduction_CVPR_2022_paper.pdf">Geometric and Textural Augmentation for Domain Gap Reduction (thecvf.com)</a></p>
<p>code: <a href="https://github.com/xch-liu/geom-tex-dg">xch-liu&#x2F;geom-tex-dg: Geometric and Textural Augmentation for Domain Gap Reduction (github.com)</a></p>
<span id="more"></span>

<h1><span id="任务">任务</span></h1><p>通过纹理和风格分布采样增强训练数据提升不同风格不同形变物体识别效果。</p>
<h1><span id="出发点">出发点</span></h1><p>对于艺术品这种具有风格形变的目标识别性能不好；</p>
<p>有工作人为这是个域泛化的问题，但已被证明的是：<strong>同一个类别但不同样式</strong>的数据往往比<strong>不同类别但相同样式</strong>的数据的差别更大，从而阻碍常规的DG方法；</p>
<p>此外，照片和艺术品的数量也不尽相同。因此，从几乎完全由照片组成的训练数据转移到包含艺术品的测试集是一个重大挑战。</p>
<p>最近的工作集中于通过style transfer应用于训练示例来提高模型鲁棒性，减轻过拟合；</p>
<p><strong>理由为：通过输入不同风格纹理的图像数据，从而使得识别网络被迫关注于物体本身语义特征的学习，如形状等</strong></p>
<p>但大多数仅关注于纹理的迁移，而忽略了几何形变的迁移；</p>
<h1><span id="创新点">创新点</span></h1><p>在几何和纹理风格方面弥补了domain的差距，而不仅仅是纹理；</p>
<p>本文的增强过程不同于现有技术，当前的文献通过使用一组（纹理）风格的样本将照片处理成不同风格艺术品来扩充数据集，相反，本文构建<strong>纹理和几何描述符的独立分布</strong>，并<strong>从中采样</strong>以增加训练数据。我们的实验表明，几何和纹理增强提高了几种常见跨域基准的分类泛化能力。</p>
<h1><span id="idea概览">idea概览</span></h1><p>本文假设纹理style和对象身份是独立的；（类似 content 和 style 的概念）</p>
<p>同一身份类别中的图像的扭曲场一般是相似的，而跨身份类别图像的扭曲场存在显著差异；</p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220612171213941.png" alt="image-20220612171213941"></p>
<p>手写数字和马的扭曲场明显是不一样的，</p>
<p>样式包括纹理和几何。纹理样式与对象类无关：知道图片是水彩或剪贴画并不能预测其对象类（身份类别）。不过，几何样式取决于对象类。使用<strong>learning to warp</strong>（箭头上方的颜色编码扭曲字段和下面相应的变形结果），可以扭曲显示相同对象类但纹理样式不同的源图像。可以看到，对象类中的扭曲场是相似的，但在对象类之间差异很大，马和文字的扭曲场（可视化表示为图中的热力图）明显不一致。</p>
<h1><span id="做法">做法</span></h1><p>增强通过处理训练输入x来扩展训练数据，以生成新的训练输入A（x）。分为两个步骤：几何增强扭曲图像；纹理增强变更纹理。</p>
<p>这两步通过独立采样两个分布来执行，<strong>分布构建是采用预先训练好的特征提取器</strong>。</p>
<p>图示训练集S包含3种风格3个类别</p>
<p><strong>本文目标为学习一个预测模型，能够很好地泛化到一个unseen style domain，也就是说，使用训练集S构建一个分类器，当图像的style是unseen的时候也能表现得很好。</strong></p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220612212437877.png" alt="image-20220612212437877"></p>
<p><strong>本文默认同一个身份类别的图像有着相近的扭曲场；</strong></p>
<p><strong>左：一个简化的训练集，包括三个风格（艺术、照片和素描）中的三个对象（狗、长颈鹿、吉他）。</strong></p>
<p><strong>中：输入同一身份不同风格的图像进行排列组合构成图像对，图中是三种就构建了三种排列组合；然后送入扭曲模块，生成warp field W*；</strong></p>
<p><em><em>扭曲分布NW</em> 就由每个身份类别的扭曲场 W</em> 构建，并从这些分布中采样扭曲原本的图像，以达到增强效果；**</p>
<p><strong>右：纹理分布的构造。将训练集输入纹理样式预测网络F以生成纹理表示集V。纹理分布NV是基于V构造的。为了更好地显示，扭曲字段采用颜色编码，NW∗和NV在低维空间中可视化。</strong></p>
<h2><span id="几何增强">几何增强</span></h2><p>几何增强是通过对训练数据进行随机形变来实现的。每个随机形变都要有以下的几个要求：</p>
<p>（1）形变速度要足够快，以便再训练期间在线执行；</p>
<p>（2）形变程度要可控，以避免过度形变；</p>
<p>（3）最重要的是，形变的类型应该足够丰富，来确保可变性并且足够合理，以避免无意义和误导性的形变（应符合特定的身份类别——比如马不能按照狗形变）</p>
<p>本文使用了《Learning to Warp》（见上个markdown）的warper来实现上述效果；</p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220616212625252.png" alt="image-20220616212625252"></p>
<p>具体操作：</p>
<p>例如一个类别 k 中有N张图像，那这<strong>N张图像排列组合成所有的图像对</strong>，<strong>每种图像对都对应了一个的扭曲场，每个类别的扭曲场又构成一组扭曲场</strong>；</p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220616212720503.png" alt="image-20220616212720503"></p>
<p>如何将这些扭曲场施加到训练图像上呢？</p>
<p>如果直接从Wk（一组扭曲场）中采样，那么扭曲的样式数量就由K的大小限制住了，为了使本文的方法能够支持尽可能广泛的几何样式，<strong>基于Wk构造了一个几何分布，并直接从中采样新的扭曲场</strong>。</p>
<p>使用多元正态分布对扭曲分布进行建模，计算均值和协方差矩阵：</p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220616212437320.png" alt="image-20220616212437320"></p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220616212447197.png" alt="image-20220616212447197"></p>
<p>Wk表示为一个二维的矩阵，每一列代表一个 “矢量化” 的扭曲场。</p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220616212739581.png" alt="image-20220616212739581"></p>
<p>为减少计算均值和协方差矩阵的计算量，对Wk做了一步下采样操作。</p>
<h2><span id="纹理增强">纹理增强</span></h2><p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220617102147930.png" alt="image-20220617102147930"></p>
<p>采用Ghiasi的《Exploring the structure of a real-time, arbitrary neural artistic stylization network》，模型代码位置Dassl\dassl\modeling\backbone\styleaugment\styleaug\ghiasi.py</p>
<p>具体细节：</p>
<p>模型具体的两个模块：</p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220617104416760.png" alt="image-20220617104416760"></p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220617104423963.png" alt="image-20220617104423963"></p>
<p>通过一个纹理样式预测网络，从一个style图像预测一个100维的向量v，（trained on PBN数据集）；</p>
<p>这些向量组成一个矩阵；</p>
<p>使用多元正态分布对扭曲分布进行建模，计算均值和协方差矩阵（<strong>同扭曲分布的构建</strong>），不同之处在于，由于纹理央视和对象类的假设是独立的，可以将类别和域标签融合在一起（不细分类别，所有训练数据一起构建）；</p>
<p>在做纹理增强时，在Nv分布中随机采样v，并通过样式预测网络传输v，再再content图像Ic上来应用这个采样出的纹理v得到Io；</p>
<p>为了控制风格增强的程度，做了个线性插值：</p>
<p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220617105954463.png" alt="image-20220617105954463"></p>
<p>除了增强纹理样式表示之外，上述方法的另一个好处是计算效率。通过对图像进行批处理来构造纹理分布（一个batch先构造好？），然后进行直接表示采样，可以大大减少训练过程中的时间开销。</p>
<h2><span id="结合几何和纹理增强">结合几何和纹理增强</span></h2><p><img src="/2022/06/12/review_8_geom%20tex%20dg/image-20220617110427313.png" alt="image-20220617110427313"></p>
<p>在训练过程中使用几何和纹理分布。在训练过程中，每个图像都可能被从特定类的扭曲分布中采样的扭曲场变形，然后可能使用来自与类无关的纹理分布的样本重新纹理。下面的虚线框中显示了混合几何和纹理样式的一些潜在增强。</p>
<h1><span id="实验">实验</span></h1><p>在本节中，我们将评估我们的方法在几个基准上的性能，并将其与最新的最先进的方法进行比较。我们还进行了烧蚀研究，以验证我们的识别方法中每个成分的重要性，无论描述风格如何。在每一种情况下，被测试的假设都是，增强通过扩大视觉对象类（VOC）以包括看不见的示例来增强对象分类性能。因此，我们不对任何测试图像应用任何类型的增强：我们的假设是VOC足够宽，可以包含新图像。</p>
<p>具体数据见代码文件组织结构。</p>
<p>做了<strong>多源和单源</strong>的域泛化效果对比。</p>
<h1><span id="结论">结论</span></h1><p><strong>结论1：</strong></p>
<p>从实验效果可知纹理和几何变换都有效，纹理影响更大；</p>
<p>证明了：纹理偏移大于几何偏移。根据我们的实验结果和之前一些研究的发现，这种现象可能有一些原因：</p>
<p>这与CNN对非形状特征的敏感性有关。CNN对范围广泛的图像处理非常敏感，对人类判断几乎没有影响。</p>
<p>具有纹理偏好的CNN可能表示归纳纹理偏差，这使得模型很难在小数据区域中学习几何相关特征，也很难将其推广到不同的分布，而不是它们所训练的分布。</p>
<p><strong>结论2：</strong></p>
<p>我们的实验结果还表明，几何和纹理增强的效果因数据集而异。最大的原因之一是数据集之间的对象和样式差异。</p>
<p>一些对象类本身具有几何形状差异，例如PACS中的动物和数字DG中的手写数字。相比之下，Office Home中的静态对象具有较少的类内形状变化。这意味着它们对几何样式的依赖不同。</p>
<p>（几何扭曲越多的数据集，几何增强影响越大）</p>
<p><strong>局限：</strong></p>
<p>由于我们的几何和纹理分布是基于源图像的相应特征表示构建的，因此它们强烈<strong>依赖于图像质量</strong>。如果特征表示远远不够好，则扩充空间将是次优的。</p>
<p><strong>补充：</strong></p>
<p>此外，对于不同的任务，如场景级分类、多对象图像，我们的纹理增强是适用的，但几何增强不能直接使用，因为它可能会在不考虑场景内容的情况下引入扭曲。一种潜在的改进方法是增加场景中的单个对象，这反过来需要对象检测，这是一个与分类不同的研究领域。这超出了本文的范围，但却是一个很好的未来探索方向。</p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>style transfer</tag>
        <tag>argument</tag>
      </tags>
  </entry>
  <entry>
    <title>《MeInGame:Create a Game Character Face from a Single Portrait》</title>
    <url>/2022/06/29/review_9_MeInGame/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E4%BB%BB%E5%8A%A1">任务</a></li>
<li><a href="#%E5%87%BA%E5%8F%91%E7%82%B9">出发点</a></li>
<li><a href="#%E8%B4%A1%E7%8C%AE">贡献</a></li>
<li><a href="#%E6%96%B9%E6%B3%95">方法</a><ul>
<li><a href="#3d%E4%BA%BA%E8%84%B8%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA%E6%96%B9%E6%B3%95">3D人脸数据集构建方法</a></li>
<li><a href="#%E4%BA%BA%E8%84%B8%E5%BD%A2%E7%8A%B6%E9%87%8D%E5%BB%BA-face-shape-reconstruction">人脸形状重建 （Face Shape Reconstruction）</a></li>
<li><a href="#%E5%BD%A2%E7%8A%B6%E8%BD%AC%E6%8D%A2shape-transfer">形状转换（Shape Transfer）</a></li>
<li><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">损失函数</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>paper: <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Geometric_and_Textural_Augmentation_for_Domain_Gap_Reduction_CVPR_2022_paper.pdf"><a href="https://arxiv.org/pdf/2102.02371.pdf">2102.02371.pdf (arxiv.org)</a></a></p>
<p>code: <a href="https://github.com/xch-liu/geom-tex-dg"><a href="https://github.com/FuxiCV/MeInGame">FuxiCV&#x2F;MeInGame: MeInGame: Create a Game Character Face from a Single Portrait, AAAI 2021 (github.com)</a></a></p>
<span id="more"></span>

<h1><span id="任务">任务</span></h1><p>从单张真实2D肖像生成武侠风格的3D可形变模型。</p>
<h1><span id="出发点">出发点</span></h1><p>近年来，人们提出了许多基于深度学习的三维人脸重建方法，但在游戏中应用较少。当前的游戏角色定制系统要么要求玩家手动调整相当多的面部属性以获得所需的面部，要么限制面部形状和纹理的自由度。</p>
<h1><span id="贡献">贡献</span></h1><p>我们提出了一种低成本的三维人脸数据集创建方法。我们创建的数据集在种族和性别上是平衡的，面部形状和纹理都是从原始图像中创建的。我们将在论文被接受后将其公开。 提出了一种低成本的面部纹理获取方法</p>
<p>我们提出了一种将重建的3DMM人脸形状转换为游戏网格的方法，可以直接在游戏环境中使用。该方法与网格连通性无关，在实际应用中计算效率较高。 </p>
<p>为了消除光照和遮挡的影响，我们在对抗性训练范式下训练神经网络，从野外人脸图像中的单个图像预测一个完整的漫反射贴图。 </p>
<h1><span id="方法">方法</span></h1><p><img src="/2022/06/29/review_9_MeInGame/image-20220629143637063.png" alt="image-20220629143637063"></p>
<p>Shape Reconstructor pretrain好，获取3DMM系数和姿势系数，通过3DMM网络构成3DMM mesh（只有正面3D人脸），Shape Transfer将3DMM mesh转换成Game mesh（整个3D人头）；</p>
<p>根据Game mesh将原图uv展开创建一个粗糙纹理贴图，通过encoder decoder进一步细化至细化纹理；</p>
<p>还引入了一个光照predictor，预测光照系数；</p>
<p>最后，将预测的形状、纹理和照明系数一起提供给可微分渲染器，并强制渲染输出与输入照片类似。为了进一步改进结果，引入了两个鉴别器。</p>
<h2><span id="3d人脸数据集构建方法">3D人脸数据集构建方法</span></h2><p>1）给定一张人脸图像，通过预训练好的人脸分割网络检测皮肤区域</p>
<p>2）计算输入面部皮肤的平均颜色，并将平均皮肤颜色传输到模板纹理贴图（由游戏开发人员提供，是标准3D模型模板对应的标准UV模板） </p>
<p>3）根据变形的game mesh，将输入人脸图像展开到UV空间</p>
<p>4）使用Poisson blending（图像融合操作）将展开的贴图与UV模板混合，移除头发和眼镜等非皮肤区域，并尽可能使用对称性修补遮挡区域</p>
<h2><span id="人脸形状重建-face-shape-reconstruction">人脸形状重建 （Face Shape Reconstruction）</span></h2><p>第一步从输入图像预测3DMM形状和姿势系数。采用了<a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/AMFG/Deng_Accurate_3D_Face_Reconstruction_With_Weakly-Supervised_Learning_From_Single_Image_CVPRW_2019_paper.pdf">Accurate 3D Face Reconstruction With Weakly-Supervised Learning: From Single Image to Image Set (thecvf.com)</a>的方法，其他3DMM的方法同样适用。</p>
<h2><span id="形状转换shape-transfer">形状转换（Shape Transfer）</span></h2><p>形状传递模块的目的是将重建的3DMM网格传递到游戏网格。我们设计了基于Radial Basis Function（RBF）插值的形状传递模块。</p>
<h2><span id="损失函数">损失函数</span></h2><p>我们设计了损失函数来最小化渲染人脸图像和输入人脸照片之间的距离，以及精细纹理贴图和地面真实纹理贴图之间的距离。在渲染循环中，我们设计了四种类型的损失函数，即像素损失、感知损失、皮肤正则化损失和对抗性损失，以从全局外观和局部细节来衡量面部相似性。 </p>
<p><strong>像素损失（Pixel Loss）</strong></p>
<p>渲染后的图像R和输入图像I，做pixel loss：</p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629153338519.png" alt="image-20220629153338519"></p>
<p>GT的UV贴图G和细化的纹理贴图F，做pixel loss：</p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629154446786.png" alt="image-20220629154446786"></p>
<p><strong>感知损失（Perceptual Loss）</strong></p>
<p>在感知层面减小重建图像的差异，做法遵循[Structure Guided Img Inpaint using Edge Prediction - 知乎 (zhihu.com)](<a href="https://zhuanlan.zhihu.com/p/147654092#:~:text=%E3%80%8AEdgeConnect%3A">https://zhuanlan.zhihu.com/p/147654092#:~:text=《EdgeConnect%3A</a> Structure Guided Image Inpainting using Edge Prediction》,结构信息 中的 边缘信息 来实现图像的修复（类似Free-form的素描信息） 适用于： rectangular masks、irregular masks。)</p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629160619803.png" alt="image-20220629160619803"></p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629160626723.png" alt="image-20220629160626723"></p>
<p><strong>皮肤正则化损失 （Skin Regularization Loss）</strong></p>
<p>为了在整个面部产生恒定的肤色并去除高光和阴影，我们进行了两次损失来调整面部皮肤，即“对称损失”和“标准偏差损失”。与之前将皮肤正则化直接应用于顶点颜色的工作不同，我们对高斯模糊纹理贴图施加惩罚。这是基于一个事实，即一些个性化的细节（例如痣）并不总是对称的，并且与肤色无关。我们将对称损耗定义如下： </p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629160919834.png" alt="image-20220629160919834"></p>
<p>我们将表皮标准偏差损失定义如下： </p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629161001203.png" alt="image-20220629161001203"></p>
<p><strong>对抗损失（Adversarial Loss）</strong></p>
<p>为了进一步提高重建的逼真度，我们还在训练期间使用对抗性损失。我们引入了两个鉴别器，一个用于渲染人脸，另一个用于生成的UV纹理贴图。我们训练鉴别器来判断生成的输出是真是假，同时，我们训练网络的其他部分来愚弄鉴别器。对抗训练的目标功能定义如下： </p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629161825452.png" alt="image-20220629161825452"></p>
<p><strong>总损失定义</strong></p>
<p>**<img src="/2022/06/29/review_9_MeInGame/image-20220629161847549.png" alt="image-20220629161847549">    **        </p>
<p><img src="/2022/06/29/review_9_MeInGame/image-20220629161920596.png" alt="image-20220629161920596"></p>
]]></content>
      <categories>
        <category>paper review</category>
      </categories>
      <tags>
        <tag>3DMM</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo建站相关</title>
    <url>/2022/03/25/tools_1_start/</url>
    <content><![CDATA[<!-- toc -->

<ul>
<li><a href="#%E6%90%AD%E5%BB%BAhexo%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%80%9A%E8%BF%87git%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2%E5%9C%A8%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8">搭建Hexo服务器，通过Git自动部署在阿里云服务器</a><ul>
<li><a href="#%E6%90%AD%E5%BB%BA%E5%8F%8A%E9%83%A8%E7%BD%B2">搭建及部署</a></li>
<li><a href="#%E5%8D%9A%E6%96%87%E5%8F%91%E5%B8%83">博文发布</a></li>
</ul>
</li>
<li><a href="#%E4%B8%BB%E9%A2%98%E5%8F%8A%E5%85%B6%E7%BE%8E%E5%8C%96">主题及其美化</a><ul>
<li><a href="#butterfly">Butterfly</a></li>
<li><a href="#next">Next</a><ul>
<li><a href="#%E7%BE%8E%E5%8C%96">美化</a></li>
<li><a href="#%E6%96%87%E7%AB%A0%E5%90%AF%E7%94%A8tags%E5%92%8Ccategories">文章启用tags和categories</a></li>
<li><a href="#%E8%AE%BE%E7%BD%AE%E9%98%85%E8%AF%BB%E5%85%A8%E6%96%87">设置阅读全文</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98updating">相关问题（Updating…）</a><ul>
<li><a href="#%E6%97%A0%E6%B3%95%E6%AD%A3%E7%A1%AE%E6%B8%B2%E6%9F%93markdown%E6%96%87%E4%BB%B6%E9%97%AE%E9%A2%98">无法正确渲染markdown文件问题</a></li>
<li><a href="#%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87%E9%97%AE%E9%A2%98">插入图片问题</a></li>
<li><a href="#%E7%9B%AE%E5%BD%95%E7%94%9F%E6%88%90%E9%97%AE%E9%A2%98">目录生成问题</a></li>
<li><a href="#%E6%B7%BB%E5%8A%A0%E6%9C%AC%E5%9C%B0%E6%90%9C%E7%B4%A2%E5%8A%9F%E8%83%BD">添加本地搜索功能</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<span id="more"></span>

<h1><span id="搭建hexo服务器通过git自动部署在阿里云服务器">搭建Hexo服务器，通过Git自动部署在阿里云服务器</span></h1><h2><span id="搭建及部署">搭建及部署</span></h2><p>参考博文链接：<a href="https://mp.weixin.qq.com/s/JTTUYJTvtdT6X2fvLUBFZg">Win10下Hexo博客搭建教程，及阿里云服务器部署实战 (qq.com)</a></p>
<h2><span id="博文发布">博文发布</span></h2><p>在本地机器上部署Hexo相关MyHexoBlogs文件夹下，进入MyHexoBlogs&#x2F;myblogs&#x2F;source&#x2F;_posts目录；</p>
<p>在当前页进入git bash，输入hexo clean 清除缓存，hexo g 解析静态文件，hexo d 刷新部署新的资源。</p>
<p>hexo cl &amp;&amp; hexo g &amp;&amp; hexo d</p>
<h1><span id="主题及其美化">主题及其美化</span></h1><h2><span id="butterfly">Butterfly</span></h2><p>Butterfly主题部署参考博文链接：<a href="https://www.jianshu.com/p/50a565adaf15?ivk_sa=1024320u">hexo框架|butterfly主题配置 - 简书 (jianshu.com)</a></p>
<p>Butterfly主题官方文档：<a href="https://www.butterfly1.cn/">Hexo-Butterfly主题(🦋 A Hexo Theme: Butterfly-Official website) (butterfly1.cn)</a></p>
<p>Hexo默认美化项：<a href="https://zhuanlan.zhihu.com/p/369951111">Hexo个性化设置 - 知乎 (zhihu.com)</a></p>
<h2><span id="next">Next</span></h2><h3><span id="美化">美化</span></h3><p><a href="https://blog.csdn.net/qq_34003239/article/details/100883213">(54条消息) Next主题美化_蜗牛非牛的博客-CSDN博客_next主题美化</a></p>
<h3><span id="文章启用tags和categories">文章启用tags和categories</span></h3><p><a href="https://blog.csdn.net/Lancis/article/details/118788205">(54条消息) hexo next主题简单美化_Lancis的博客-CSDN博客_next主题美化</a></p>
<h3><span id="设置阅读全文">设置阅读全文</span></h3><p><a href="https://blog.csdn.net/CHENGXUYUAN09/article/details/103408380">(54条消息) next7.6版本关于设置阅读全文_LIYUANWAISPRING的博客-CSDN博客</a></p>
<h1><span id="相关问题updating">相关问题（Updating…）</span></h1><h2><span id="无法正确渲染markdown文件问题">无法正确渲染markdown文件问题</span></h2><p>hexo对于md文件的解析规则不是标准规则，有时候在复制论文标题时会有换行符无法正确解析；</p>
<p>重新手动输入检查标题格式解决问题</p>
<h2><span id="插入图片问题">插入图片问题</span></h2><p>选择采用hexo官网的解决方式：<a href="https://hexo.io/zh-cn/docs/asset-folders">资源文件夹 | Hexo</a></p>
<h2><span id="目录生成问题">目录生成问题</span></h2><p>安装插件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cnpm install hexo-toc --save</span><br></pre></td></tr></table></figure>

<p>hexo的配置文件中设置格式，添加配置代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">toc:</span><br><span class="line">  maxdepth: 3</span><br></pre></td></tr></table></figure>

<p>在md中使用时</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;!-- toc --&gt;</span><br></pre></td></tr></table></figure>

<h2><span id="添加本地搜索功能">添加本地搜索功能</span></h2><p>与Next官方配置文件中的链接步骤不同的是，还需要修改</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">preload: true</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
